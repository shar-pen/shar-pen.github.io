[{"title":"RAG 上下文压缩 - 基础概念","url":"/2025/05/04/RAG-context-compression-series/1.basic/","content":"前言之前遇到一个问题，公司大模型的长度不长，而且业务需要RAG返回不少的相关内容。通常RAG都是取top-10/15，文档由于长度限制不能太多，加上embedding效果不佳，返回的文档不多也不能保证都相关。\n我们考虑到返回的文档中真正能被用于求解的文本可能只占总文本的一部分，其他无关部分只会影响大模型，而且会占用上下文长度。\n因此我们对RAG文档进行问题相关的总计/query-focused summarization，一方面可以过滤无关内容，另一方面让文本更助于大模型回答问题。此外，同样的top-k设置下，过滤压缩后上下文长度变短了，可以倍数增长top-k，即使总结会有些信息损失，更多的文档可以弥补这方面。\n这个文章只介绍基础的总结技术，代码基于langchain实现。\n文档摘要的核心原则在构建摘要生成器时，一个核心问题是：如何将文档呈现给 LLM 的上下文窗口？  \n主要方法包括：  \n\nStuff（完整输入）：直接将整个文档一次性放入上下文窗口。方法简单，但在处理长文档时受限。  \n\nMap-Reduce（分块合并）：将文档拆分为多个小块，分别对每个部分进行摘要，然后合并各部分摘要得到最终结果。适用于处理大规模数据集。  \n\nRefine（逐步优化）：按顺序处理文档，并在摘要过程中不断融合先前的摘要和新内容，从而逐步优化总结结果。适用于需要更精细摘要的场景。\n\n\nimport osimport jsonfrom langchain_core.documents import Documentdata = []file_path = './data/data_100.json'with open(file_path) as f:    for line in f:        a_record = json.loads(line)        data.append(a_record)print(len(data))data_indice = 0a_query = data[data_indice]['query']a_docs = data[data_indice]['pos']a_docs = [Document(item) for item in a_docs]\n\n100\n\nStuff它将一组文档直接插入提示（prompt），然后将该提示发送给 LLM。该方法适用于文档较小、且每次调用仅需处理少量文档的应用场景。\nfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import PromptTemplate, ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = PromptTemplate(    input_variables=[\"context\", \"query\"],    template=(\t\t\"请清晰简明地总结以下文本以回答问题。\\n\\n\"  \t\t\"在总结时，请注意以下几点：\\n\"  \t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\"- 省略不必要的细节。\\n\\n\"          \"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"\t\t\"[需要总结的文本-开始]：\\n{context}\\n[需要总结的文本-结束]\\n\"  \t\t\"摘要：\"        )    )llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)output_parser = StrOutputParser()map_chain = prompt | llm | output_parser\n\n\nresult = map_chain.invoke(    {        \"query\":a_query,        \"context\":'\\n'.join([item.page_content for item in a_docs])\t})print(a_query)print(20*'=')print(result)\n\n澳大利亚新任外长黄英贤近日访问了哪个国家，他的目的是什么？\n====================\n澳大利亚新任外长黄英贤访问了所罗门群岛，目的是加强澳大利亚与太平洋国家的安全合作，并强调澳大利亚在该地区的存在感。黄英贤与所罗门群岛总理索加瓦雷进行了会谈，强调了澳大利亚警方在该国骚乱后的援助，并表示澳大利亚不会在所罗门群岛建立军事基地。黄英贤的访问正值中国与所罗门群岛签署安全协议引发地区关切之际，澳大利亚、新西兰和美国等国也在该地区采取行动，试图抗衡中国在太平洋地区的影响力。\n\n事实上既然直接 stuff 的话, 不一点需要先总结再回答, 可以直接回答问题。在我看来, stuff 可以算作一个 CoT 环节, 先显示找出相关内容, 再求解问题\nMap-Reduce\nMap-Reduce 摘要是一种有效的长文档压缩技术，主要包含两个阶段：  \n\nMap 阶段：将文档拆分为多个小块，并对每个部分独立生成摘要。  \nReduce 阶段：将各个部分的摘要合并，形成连贯的最终摘要。\n\n该方法在处理超长文档时尤为有用，因为它允许在 Map 阶段对各个块并行处理，从而提高效率。此外，它还能有效规避语言模型的 token 限制，确保每个文本块都能适应模型的上下文窗口。\nMap 阶段在 Map 阶段，通常对每个文本块进行摘要生成。  \n标准方法是对每个块的内容进行总结，但另一种替代方式是提取关键信息。由于 Reduce 阶段最终会将所有输出合并为最终摘要，因此这两种方法通常都能有效完成任务，并且对最终结果的影响较小。  \n在 Map 阶段选择摘要生成还是关键信息提取，可以根据具体任务的目标和需求进行调整。\nimport asynciofrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplate, ChatPromptTemplate# 定义输出格式class MapSummary(BaseModel):    reasoning: str = Field(description=\"关于问题、文本内容、两者之间关联性的思考\")    summary: str = Field(description=\"对文本中与问题相关片段的内容总结\")# 创建解析器map_parser = PydanticOutputParser(pydantic_object=MapSummary)# 创建模板map_prompt = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"你是一名专业的内容提取和总结专家。\"),        (\"human\", (\t\t\t\t\t\"请清晰简明地总结以下文本以回答问题。\\n\\n\"  \t\t\t\t\t\"在总结时，请注意以下几点：\\n\"                     \"- 你的任务是总结问题相关的文本，而不是回答问题。\" \t\t\t\t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\t\t\t\"- 省略不必要的细节。\\n\\n\"  \t\t\t\t\t\"按以下格式要求输出：\\n{format_instructions}\\n\\n\"\t\t\t\t\t\"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"\t\t\t\t\t\"[需要总结的文本-开始]：\\n{context}\\n[需要总结的文本-结束]\\n\"  \t\t\t\t)         ),    ])# 固定输出格式指令map_prompt = map_prompt.partial(format_instructions=map_parser.get_format_instructions())llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)map_chain = map_prompt | llm | map_parsermap_prompt.pretty_print()\n\n================================[1m System Message [0m================================\n\n你是一名专业的内容提取和总结专家。\n\n================================[1m Human Message [0m=================================\n\n请清晰简明地总结以下文本以回答问题。\n\n在总结时，请注意以下几点：\n- 你的任务是总结问题相关的文本，而不是回答问题。- 包含关键事件、重要事实和核心信息。\n- 省略不必要的细节。\n\n按以下格式要求输出：\n[33;1m[1;3m{format_instructions}[0m\n\n[问题-开始]：\n[33;1m[1;3m{query}[0m\n[问题-结束]\n\n[需要总结的文本-开始]：\n[33;1m[1;3m{context}[0m\n[需要总结的文本-结束]\n摘要：\n\n# 并行运行所有任务tasks = [\tmap_chain.ainvoke(\t\t{            \"query\":a_query,        \t\"context\":doc.page_content,\t\t}\t)\tfor doc in a_docs]map_results = await asyncio.gather(*tasks)map_results\n\n\n\n\n[MapSummary(think='文本主要讨论了澳大利亚外长黄英贤访问所罗门群岛的背景和目的，以及美国与马绍尔群岛的经济援助谈判，但未直接回答黄英贤访问的具体国家和目的。', summary='澳大利亚外长黄英贤访问了所罗门群岛，强调了澳大利亚对所罗门群岛的援助，并与总理讨论了安全问题。美国与马绍尔群岛就经济援助进行谈判，以应对中国在太平洋地区的影响力增强。'),\n MapSummary(think='文本主要描述了澳大利亚新任外长黄英贤访问萨摩亚和汤加的目的，以及美日等国在南太平洋的动作，意图抗衡中国的影响力。', summary='澳大利亚新任外长黄英贤访问萨摩亚和汤加，旨在加强与太平洋国家的安全合作。此前，美日等国也在南太平洋频繁活动，意图抗衡中国在该地区的影响力。'),\n MapSummary(think='文本主要报道了澳大利亚外长黄英贤访问中国的背景、目的以及外界对此的评价。', summary='澳大利亚外长黄英贤于12月20日至21日对中国进行访问，这是澳中建交50周年之际，黄英贤四年多来首次访华。多家外媒认为此访标志着中澳关系迈出重要一步，有助于推进共同利益和管控分歧。'),\n MapSummary(think='文本主要讲述了澳大利亚新任外长黄英贤访问中国的目的和背景，以及专家对此的解读。', summary='澳大利亚外长黄英贤访问中国，旨在推动贸易限制措施的取消，并寻求与中国的稳定平等伙伴关系。澳专家认为，这是澳大利亚抓住与中国关系解冻的机会，但中澳关系回暖仍面临美国的影响。')]\n\nReduce 阶段在 Reduce Chain 中，对 Map 阶段生成的结果进行进一步处理，以合并和优化内容，最终生成连贯的摘要。  \nclass ReduceSummary(BaseModel):    reasoning: str = Field(description=\"关于问题与局部内容总结的思考\")    summary: str = Field(description=\"整合局部内容总结的全局总结\")reduce_parser = PydanticOutputParser(pydantic_object=ReduceSummary)reduce_prompt = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"你是一名专业的摘要专家。你将收到一组文档摘要，并需要将其整合为一个完整的摘要。\"),        (\"human\", (\t\t\t\t\t\"请将以下局部的总结内容整合，形成一个完整的内容摘要，以作为参考材料回答问题。\\n\\n\"  \t\t\t\t\t\"在总结时，请注意以下几点：\\n\"                     \"- 你的任务是总结问题相关的文本，而不是回答问题。\" \t\t\t\t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\t\t\t\"- 省略不必要的细节。\\n\"  \t\t\t\t\t\"- 去除重复冗余内容，使语言更加简洁和凝练。\\n\\n\"\t\t\t\t\t\"按以下格式要求输出：\\n{format_instructions}\\n\\n\"\t\t\t\t\t# \"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"\t\t\t\t\t\"[局部内容总结-开始]：\\n{map_summary}\\n[局部内容总结-结束]\\n\"  \t\t\t\t)         ),    ])reduce_prompt = reduce_prompt.partial(format_instructions=reduce_parser.get_format_instructions())llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)reduce_chain = reduce_prompt | llm | reduce_parserreduce_prompt.pretty_print()\n\n\nreduce_result = reduce_chain.invoke(    {        \"query\":a_query,        \"map_summary\": '\\n'.join([f'- {item.summary}' for item in map_results])\t})\n\n\nprint(reduce_result.summary)\n\n澳大利亚外长黄英贤频繁访问太平洋国家，强调安全合作并应对中国影响力。同时，黄英贤访问中国，推动贸易限制措施取消，寻求稳定平等的伙伴关系，但美国的影响仍存。\n\nprint(reduce_prompt.invoke(    {        \"query\":a_query,        \"map_summary\": '\\n'.join([f'- {item.summary}' for item in map_results])\t}).to_messages()[-1].content)\n\n请将以下局部的总结内容整合，形成一个完整的内容摘要，以作为参考材料回答问题。\n\n在总结时，请注意以下几点：\n- 你的任务是总结问题相关的文本，而不是回答问题。- 包含关键事件、重要事实和核心信息。\n- 省略不必要的细节。\n- 去除重复冗余内容，使语言更加简洁和凝练。\n\n按以下格式要求输出：\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n{\"properties\": {\"think\": {\"description\": \"关于问题与局部内容总结的思考\", \"title\": \"Think\", \"type\": \"string\"}, \"summary\": {\"description\": \"整合局部内容总结的全局总结\", \"title\": \"Summary\", \"type\": \"string\"}}, \"required\": [\"think\", \"summary\"]}\n\n[局部内容总结-开始]：\n- 澳大利亚外长黄英贤访问了所罗门群岛，强调了澳大利亚对所罗门群岛的援助，并与总理讨论了安全问题。美国与马绍尔群岛就经济援助进行谈判，以应对中国在太平洋地区的影响力增强。\n- 澳大利亚新任外长黄英贤访问萨摩亚和汤加，旨在加强与太平洋国家的安全合作。此前，美日等国也在南太平洋频繁活动，意图抗衡中国在该地区的影响力。\n- 澳大利亚外长黄英贤于12月20日至21日对中国进行访问，这是澳中建交50周年之际，黄英贤四年多来首次访华。多家外媒认为此访标志着中澳关系迈出重要一步，有助于推进共同利益和管控分歧。\n- 澳大利亚外长黄英贤访问中国，旨在推动贸易限制措施的取消，并寻求与中国的稳定平等伙伴关系。澳专家认为，这是澳大利亚抓住与中国关系解冻的机会，但中澳关系回暖仍面临美国的影响。\n[局部内容总结-结束]\n摘要：\n\n关于是否在reduce阶段附加query的思考\n在这个阶段里, prompt里可以不附加query, 因为map阶段已经过滤内容了, 此阶段只需要整合。但最好附上query, 不然可能会出现把需要的内容给整合漏了的情况。以下是一个例子，map阶段内容还很具体，但reduce过于简洁，以致于如果换了具体的问题，就可能无法求解了。\nquery\n澳大利亚新任外长黄英贤近日访问了哪个国家，他的目的是什么？\n\nmap结果\n- 澳大利亚外长黄英贤访问了所罗门群岛，强调了澳大利亚对所罗门群岛的援助，并与总理讨论了安全问题。美国与马绍尔群岛就经济援助进行谈判，以应对中国在太平洋地区的影响力增强。- 澳大利亚新任外长黄英贤访问萨摩亚和汤加，旨在加强与太平洋国家的安全合作。此前，美日等国也在南太平洋频繁活动，意图抗衡中国在该地区的影响力。- 澳大利亚外长黄英贤于12月20日至21日对中国进行访问，这是澳中建交50周年之际，黄英贤四年多来首次访华。多家外媒认为此访标志着中澳关系迈出重要一步，有助于推进共同利益和管控分歧。- 澳大利亚外长黄英贤访问中国，旨在推动贸易限制措施的取消，并寻求与中国的稳定平等伙伴关系。澳专家认为，这是澳大利亚抓住与中国关系解冻的机会，但中澳关系回暖仍面临美国的影响。\n\nreduce结果\n澳大利亚外长黄英贤频繁访问太平洋国家，强调安全合作并应对中国影响力。同时，黄英贤访问中国，推动贸易限制措施取消，寻求稳定平等的伙伴关系，但美国的影响仍存。\n\nMap-Reduce Chain 的完整流程import asynciofrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplate, ChatPromptTemplate# ===== map =====class MapSummary(BaseModel):    reasoning: str = Field(description=\"关于问题、文本内容、两者之间关联性的思考\")    summary: str = Field(description=\"对文本中与问题相关片段的内容总结\")map_parser = PydanticOutputParser(pydantic_object=MapSummary)map_prompt = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"你是一名专业的内容提取和总结专家。\"),        (\"human\", (\t\t\t\t\t\"请清晰简明地总结以下文本以回答问题。\\n\\n\"  \t\t\t\t\t\"在总结时，请注意以下几点：\\n\"                     \"- 你的任务是总结问题相关的文本，而不是回答问题。\" \t\t\t\t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\t\t\t\"- 省略不必要的细节。\\n\\n\"  \t\t\t\t\t\"按以下格式要求输出：\\n{format_instructions}\\n\\n\"\t\t\t\t\t\"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"\t\t\t\t\t\"[需要总结的文本-开始]：\\n{context}\\n[需要总结的文本-结束]\\n\" \t\t\t\t)         ),    ])map_prompt = map_prompt.partial(format_instructions=map_parser.get_format_instructions())llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)map_chain = map_prompt | llm | map_parser# ===== reduce =====class ReduceSummary(BaseModel):    reasoning: str = Field(description=\"关于问题与局部内容总结的思考\")    summary: str = Field(description=\"整合局部内容总结的全局总结\")reduce_parser = PydanticOutputParser(pydantic_object=ReduceSummary)reduce_prompt = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"你是一名专业的摘要专家。你将收到一组文档摘要，并需要将其整合为一个完整的摘要。\"),        (\"human\", (\t\t\t\t\t\"请将以下局部的总结内容整合，形成一个完整的内容摘要，以作为参考材料回答问题。\\n\\n\"  \t\t\t\t\t\"在总结时，请注意以下几点：\\n\"                     \"- 你的任务是总结问题相关的文本，而不是回答问题。\" \t\t\t\t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\t\t\t\"- 省略不必要的细节。\\n\"  \t\t\t\t\t\"- 去除重复冗余内容，使语言更加简洁和凝练。\\n\\n\"\t\t\t\t\t\"按以下格式要求输出：\\n{format_instructions}\\n\\n\"\t\t\t\t\t# \"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"\t\t\t\t\t\"[局部内容总结-开始]：\\n{map_summary}\\n[局部内容总结-结束]\\n\" \t\t\t\t)         ),    ])reduce_prompt = reduce_prompt.partial(format_instructions=reduce_parser.get_format_instructions())llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)reduce_chain = reduce_prompt | llm | reduce_parserasync def map_reduce_chain(query, docs):\ttasks = [\t\tmap_chain.ainvoke(\t\t\t{\t\t\t\t\"query\":a_query,\t\t\t\t\"context\":doc.page_content,\t\t\t}\t\t)\t\tfor doc in a_docs\t]    \tmap_results = await asyncio.gather(*tasks)\treduce_result = reduce_chain.invoke(\t\t{\t\t\t\"query\":a_query,\t\t\t\"map_summary\": '\\n'.join([f'- {item.summary}' for item in map_results])\t\t}\t)\treturn reduce_resultresult = await map_reduce_chain(a_query, a_docs)\n\n\nprint(result.summary)\n\n澳大利亚外长黄英贤访问所罗门群岛、萨摩亚、汤加及中国，旨在加强教育、医疗援助、安全合作，并推动贸易限制措施的取消，重建与中国的友好关系。此访问具有重大象征意义，标志着澳中关系改善的积极信号，但美国可能成为关系改善的障碍。\n\nMap-Refine\nMap-Refine 方法是一种文档摘要处理方式，类似于 Map-Reduce，但在摘要的处理和合并方式上有所不同。  \n\nMap 阶段\n\n\n将文档拆分为多个小块。  \n独立对每个块进行摘要。\n\n\nRefine 阶段\n\n\n生成的摘要按顺序进行处理。  \n每次迭代时，将上一次的摘要与下一个文本块的信息结合，对摘要进行更新和优化。\n\n\n迭代过程\n\n\nRefine 阶段会持续迭代，直到所有文本块都处理完毕。  \n每次迭代都会在保留已有信息的基础上，进一步完善摘要。\n\n\n最终摘要\n\n\n所有文本块处理完成后，最终摘要将在最后一次优化步骤后生成。\n\n\n主要优势: \n\n保持文档顺序：适用于需要保持原始内容顺序的情况，如叙述性或结构化文档。  \n上下文优化：摘要在每个步骤都会逐步完善，适用于需要递进式构建背景信息的内容。\n\n局限性:\n\n顺序处理：Refine 阶段需要按顺序执行，难以并行化。  \n时间成本较高：相比 Map-Reduce，由于无法并行处理，大规模数据处理可能较慢。\n\nMap 阶段和 Map-Reduce 同样\nimport asynciofrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplate, ChatPromptTemplate# 定义输出格式class MapSummary(BaseModel):    reasoning: str = Field(description=\"关于问题、文本内容、两者之间关联性的思考\")    summary: str = Field(description=\"对文本中与问题相关片段的内容总结\")# 创建解析器map_parser = PydanticOutputParser(pydantic_object=MapSummary)# 创建模板map_prompt = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"你是一名专业的内容提取和总结专家。\"),        (\"human\", (\t\t\t\t\t\"请清晰简明地总结以下文本以回答问题。\\n\\n\"  \t\t\t\t\t\"在总结时，请注意以下几点：\\n\"                     \"- 你的任务是总结问题相关的文本，而不是回答问题。\" \t\t\t\t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\t\t\t\"- 省略不必要的细节。\\n\\n\"  \t\t\t\t\t\"按以下格式要求输出：\\n{format_instructions}\\n\\n\"\t\t\t\t\t\"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"\t\t\t\t\t\"[需要总结的文本-开始]：\\n{context}\\n[需要总结的文本-结束]\\n\"  \t\t\t\t)         ),    ])# 固定输出格式指令map_prompt = map_prompt.partial(format_instructions=map_parser.get_format_instructions())llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)map_chain = map_prompt | llm | map_parsermap_prompt.pretty_print()\n\n================================[1m System Message [0m================================\n\n你是一名专业的内容提取和总结专家。\n\n================================[1m Human Message [0m=================================\n\n请清晰简明地总结以下文本以回答问题。\n\n在总结时，请注意以下几点：\n- 你的任务是总结问题相关的文本，而不是回答问题。- 包含关键事件、重要事实和核心信息。\n- 省略不必要的细节。\n\n按以下格式要求输出：\n[33;1m[1;3m{format_instructions}[0m\n\n[问题-开始]：\n[33;1m[1;3m{query}[0m\n[问题-结束]\n\n[需要总结的文本-开始]：\n[33;1m[1;3m{context}[0m\n[需要总结的文本-结束]\n\n​    \n# 并行运行所有任务tasks = [\tmap_chain.ainvoke(\t\t{            \"query\":a_query,        \t\"context\":doc.page_content,\t\t}\t)\tfor doc in a_docs]map_results = await asyncio.gather(*tasks)map_results\n\n\n\n\n[MapSummary(reasoning='文本主要描述了澳大利亚外长黄英贤访问所罗门群岛的情况，以及访问的目的和背景。虽然文本中提到了澳大利亚对所罗门群岛的援助，但未明确提及黄英贤访问的具体目的。', summary='澳大利亚外长黄英贤访问了所罗门群岛，强调了澳大利亚对所罗门群岛的教育、医疗援助以及当地骚乱后的恢复工作。访问期间，黄英贤与所罗门群岛总理索加瓦雷进行了会谈，讨论了安全问题。'),\n MapSummary(reasoning='文本主要描述了澳大利亚新任外长黄英贤访问萨摩亚和汤加的目的，以及澳大利亚与其他国家在南太平洋地区的活动，旨在抗衡中国在该地区的影响力。', summary='澳大利亚新任外长黄英贤访问萨摩亚和汤加，目的是加强澳大利亚与太平洋国家的安全合作，抗衡中国在南太平洋的影响力。'),\n MapSummary(reasoning='文本主要报道了澳大利亚外长黄英贤访问中国的事件，包括访问的目的和背景，以及多家媒体对此的报道和评论。', summary='澳大利亚外长黄英贤于12月20日至21日对中国进行访问，这是澳中建交50周年之际，四年多来首次访华。多家媒体认为此访标志着中澳关系迈出重要一步，有助于推进共同利益和管控分歧。'),\n MapSummary(reasoning='文本主要介绍了澳大利亚外长黄英贤访华的目的和背景，以及此次访问的意义。文章提到了澳中关系的现状和未来可能的发展方向。', summary='澳大利亚外长黄英贤访问中国，旨在推动贸易限制措施的取消，并寻求与中国的稳定平等伙伴关系。此次访问被视为中澳关系解冻的重要时刻，但双方仍面临一些棘手问题，未来关系回暖仍面临美国的影响。')]\n\nRefine 阶段在 Refine 阶段，Map 阶段生成的文本块会按顺序依次处理，每次迭代都会逐步优化最终摘要。摘要的更新方式是将上一轮的摘要与下一个文本块的信息结合，从而确保最终摘要更加全面且符合上下文逻辑。\nclass RefineSummary(BaseModel):    reasoning: str = Field(description=\"关于问题与局部内容总结的思考\")    summary: str = Field(description=\"整合局部内容总结的全局总结\")refine_parser = PydanticOutputParser(pydantic_object=RefineSummary)refine_prompt = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"你是一名专业的摘要专家。你的任务是生成一个最终的内容总结。\"),        (\"human\", (                    \"我提供你一份当前的总结，和一份新文本，你需要结合两者，精炼出一份新的总结，以作为参考材料回答问题。\\n\\n\"\t\t\t\t\t\"在总结时，请注意以下几点：\\n\"                     \"- 你的任务是总结问题相关的文本，而不是回答问题。\" \t\t\t\t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\t\t\t\"- 省略不必要的细节。\\n\"  \t\t\t\t\t\"- 去除重复冗余内容，使语言更加简洁和凝练。\\n\\n\"\t\t\t\t\t\"按以下格式要求输出：\\n{format_instructions}\\n\\n\"\t\t\t\t\t\"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"                    \"[当前内容总结-开始]：\\n{previous_summary}\\n[当前内容总结-结束]\\n\\n\"\t\t\t\t\t\"[新文本-开始]：\\n{current_summary}\\n[新文本-结束]\\n\"  \t\t\t\t)         ),    ])refine_prompt = refine_prompt.partial(format_instructions=refine_parser.get_format_instructions())llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)refine_chain = refine_prompt | llm | refine_parserrefine_prompt.pretty_print()\n\n================================[1m System Message [0m================================\n\n你是一名专业的摘要专家。你的任务是生成一个最终的内容总结。\n\n================================[1m Human Message [0m=================================\n\n我提供你一份当前的总结，和一份新文本，你需要结合两者，精炼出一份新的总结，以作为参考材料回答问题。\n\n在总结时，请注意以下几点：\n- 你的任务是总结问题相关的文本，而不是回答问题。- 包含关键事件、重要事实和核心信息。\n- 省略不必要的细节。\n- 去除重复冗余内容，使语言更加简洁和凝练。\n\n按以下格式要求输出：\n[33;1m[1;3m{format_instructions}[0m\n\n[问题-开始]：\n[33;1m[1;3m{query}[0m\n[问题-结束]\n\n[当前内容总结-开始]：\n[33;1m[1;3m{previous_summary}[0m\n[当前内容总结-结束]\n\n[新文本-开始]：\n[33;1m[1;3m{current_summary}[0m\n[新文本-结束]\n\n​    \nprevious_summary = map_results[0].summaryfor item in map_results[1:]:    refine_result = refine_chain.invoke(        {            \"query\":a_query,            \"previous_summary\":previous_summary,            \"current_summary\":item.summary,\t\t}\t)    previous_summary = refine_result.summary    print('='*20)    print(previous_summary)\n\n====================\n澳大利亚新任外长黄英贤访问萨摩亚和汤加，旨在加强与太平洋国家的安全合作，抗衡中国在南太平洋的影响力。访问期间，黄英贤还与所罗门群岛总理讨论了安全问题并提供了援助。\n====================\n澳大利亚新任外长黄英贤分别访问了萨摩亚、汤加和中国。他对萨摩亚和汤加的访问旨在加强与太平洋国家的安全合作，抗衡中国在南太平洋的影响力。同时，黄英贤对中国进行访问，这是澳中建交50周年之际，四年多来首次访华，标志着中澳关系迈出重要一步。\n====================\n澳大利亚新任外长黄英贤访问了萨摩亚、汤加和中国。他对萨摩亚和汤加的访问旨在加强与太平洋国家的安全合作，抗衡中国在南太平洋的影响力。对中国访问则旨在推动贸易限制措施的取消，并寻求与中国的稳定平等伙伴关系。此次访问被视为中澳关系解冻的重要时刻，但未来关系回暖仍面临美国的影响。\n\n具体 refine 的粒度还需要自己通过 prompt 调整, 但大多数下大模型都会简化内容, 因为”总结”就是简化。\n","categories":["RAG 上下文压缩"]},{"title":"RAG 上下文压缩 - 带早停机制的 map-refine","url":"/2025/05/04/RAG-context-compression-series/2.map_refine-early_stop/","content":"我试过用map-refine方法来精炼上下文，由于它是线性的，运行时间随着文档数量线性增长。所以可以考虑通过判断上下文是否可以满足QA来提前结束过程。\nimport osimport jsonfrom langchain_core.documents import Documentdata = []file_path = './data/data_&gt;=10.json'with open(file_path) as f:    for line in f:        a_record = json.loads(line)        data.append(a_record)print(len(data))data_indice = 1a_query = data[data_indice]['query']a_docs = data[data_indice]['pos']a_docs = [Document(item) for item in a_docs]\n\n50\n\nMap-Refine 附加早停机制Map 阶段import asynciofrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplate, ChatPromptTemplatefrom langchain.output_parsers import OutputFixingParser# ===== map =====class MapSummary(BaseModel):    reasoning: str = Field(description=\"关于问题和本内容之间关联性的思考\")    summary: str = Field(description=\"对文本中与问题相关片段的提取，直接输出为str\")map_parser = PydanticOutputParser(pydantic_object=MapSummary)map_prompt = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"你是一名专业的内容提取和总结专家。\"),        (\"human\", (\t\t\t\t\t\"请清晰简明地总结以下文本以回答问题。\\n\\n\"  \t\t\t\t\t\"在总结时，请注意以下几点：\\n\"                     \"- 你的任务是总结问题相关的文本，而不是回答问题。\" \t\t\t\t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\t\t\t\"- 省略不必要的细节。\\n\\n\"  \t\t\t\t\t\"按以下格式要求输出：\\n{format_instructions}\\n\\n\"\t\t\t\t\t\"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"\t\t\t\t\t\"[需要总结的文本-开始]：\\n{context}\\n[需要总结的文本-结束]\\n\" \t\t\t\t)         ),    ])map_prompt = map_prompt.partial(format_instructions=map_parser.get_format_instructions())llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.5,)map_chain = map_prompt | llm | map_parser\n\n\ntasks = [\tmap_chain.ainvoke(\t\t{            \"query\":a_query,        \t\"context\":doc.page_content,\t\t}\t)\tfor doc in a_docs]map_results = await asyncio.gather(*tasks)map_results\n\n\n\n\n[MapSummary(reasoning='文本主要介绍了90版本阿修罗武器的排行，提到了四把武器的特点和排名依据。', summary='荒古排名第一，理由是其技能攻击力和魔能提升效果；妖刀村正排名第二，因其无视和额外黄字属性；暗影蔽日排名第三，虽然所有攻击力亮眼但需注意堆属性的搭配；圣剑排名第四，适合当前版本的天域套装备。'),\n MapSummary(reasoning='文本主要讨论了90版本阿修罗的武器排行，提到了支点、别云和天丛云这三把武器的相关信息。', summary='支点、别云和天丛云是90版本阿修罗的优秀武器，支点适合光强修罗，别云有高黄字但存在黄字冲突问题，天丛云则有27白字且适合一觉cd换装。'),\n MapSummary(reasoning='文本主要描述了90版本阿修罗武器的排行情况，但并未直接提及名刀32和90版本的具体排行情况，因此需要进一步筛选相关信息。', summary='名刀32和七支刀在描述中被提及，但具体排名信息未给出。'),\n MapSummary(reasoning='文本主要讨论了90版本阿修罗武器的排行，提到了三把武器的特点和优势，与问题相关性较强，可以直接提取关键信息作为总结。', summary='90版本阿修罗武器排行：1.荒古太（未升级和升级后的技能攻击力及获取方式）；2.妖刀村正（90版本新武器，无视和额外黄字优势）；3.暗影蔽日（所有攻击力高，搭配需注意）。'),\n MapSummary(reasoning='文本主要讨论了90版本阿修罗的主流武器测试排名，特别是针对吞噬魔和破锁血马蹄卡的测试结果。', summary='90版本阿修罗主流武器测试排名：吞噬魔-支点&gt;开魔能荒古&gt;妖刀传奇；破锁血马蹄卡-妖刀不适合作为破锁血武器。'),\n MapSummary(reasoning='文本主要讨论了90版本阿修罗武器在20人本的表现，提到了妖刀、开魔能荒古、圣剑等武器的排名情况，以及影响排名的因素。', summary='20人本妖刀&gt;开魔能荒古&gt;圣剑=支点=避日，圣剑攻击力受自身属强影响，荒古属性攻击选最高值。'),\n MapSummary(reasoning='文本内容与90版本阿修罗武器排行无关，为避免误导，应排除。', summary=''),\n MapSummary(reasoning='文本主要讨论了90版本DNF游戏中阿修罗武器的排行，特别是前10名的排名情况，与问题相关度高。', summary='90版本DNF修罗武器排行榜：10.无影剑，荒古太刀排名第一。'),\n MapSummary(reasoning='文本主要介绍了90版本阿修罗武器的排行及特点，与问题相关性较强，但未直接提到排行结果。', summary='文本介绍了90版本阿修罗武器的排行及特点，如七支刀、名刀、天丛云等，但未直接给出具体排行结果。'),\n MapSummary(reasoning='文本主要介绍了90版本阿修罗武器的排行，提到了别云、支点和圣剑这三种武器的特点和适用情况。问题询问90版本阿修罗武器排行，因此这些信息与问题直接相关。', summary='别云武器适合搭配50黄字装备，支点适合幽魂套和光强修罗，圣剑100属强适合全属强套装，但释放速度慢影响手感。'),\n MapSummary(reasoning='文本主要讨论了90版本阿修罗武器的排行，提到了暗影蔽日和妖刀村正的优缺点。', summary='90版本阿修罗武器排行，暗影蔽日和妖刀村正表现突出，分别适用于幽魂流光和未升级的荒古。')]\n\nRefine 阶段一开始的早停只有两个选项：内容不完整（继续）和内容完整（早停）。但有问题，例如枚举类问题，你可以拿部分文档来回答问题，也可以那更多文档来提高QA效果，所以增加第三选项，可以继续完善。这个选项和内容不完整没有什么根本的不同，都是某种意义上的内容不完整。\nclass RefineSummary(BaseModel):    query_context_reasoning: str = Field(description=\"关于问题、先前内容总结、新内容之间联系的思考\")    refined_summary: str = Field(description=\"整合局部内容总结的全局总结\")    summary_sufficiency_reasoning: str = Field(description=\"关于当前总结是否足够回答问题（是否提供了所有必要的细节）的思考，此外也需要判断是否可以再扩充新文本以获得更好的回答效果\")    summary_sufficiency_score: int = Field(description=\"用100分制表示利用当前总结回答问题的效果预期分数\")    next_action: str = Field(description=\"决定下一个动作，从以下选项中选一个：内容不完整, 内容完整, 内容可继续完善\")refine_parser = PydanticOutputParser(pydantic_object=RefineSummary)refine_prompt = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"你是一名专业的摘要专家。你的任务是生成一个最终的内容总结。\"),        (\"human\", (                    \"我提供你一份当前的总结，和一份新文本，你需要结合两者，精炼出一份新的总结，以作为参考材料回答问题。\\n\\n\"\t\t\t\t\t\"在总结时，请注意以下几点：\\n\"                     \"- 你的任务是总结问题相关的文本，而不是回答问题。\" \t\t\t\t\t\"- 包含关键事件、重要事实和核心信息。\\n\"  \t\t\t\t\t\"- 省略不必要的细节。\\n\"  \t\t\t\t\t\"- 去除重复冗余内容，使语言更加简洁和凝练。\\n\\n\"\t\t\t\t\t\"按以下格式要求输出：\\n{format_instructions}\\n\\n\"\t\t\t\t\t\"[问题-开始]：\\n{query}\\n[问题-结束]\\n\\n\"                    \"[当前内容总结-开始]：\\n{previous_summary}\\n[当前内容总结-结束]\\n\\n\"\t\t\t\t\t\"[新文本-开始]：\\n{current_summary}\\n[新文本-结束]\\n\"  \t\t\t\t)         ),    ])refine_prompt = refine_prompt.partial(format_instructions=refine_parser.get_format_instructions())llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)refine_chain = refine_prompt | llm | refine_parserrefine_prompt.pretty_print()\n\n================================[1m System Message [0m================================\n\n你是一名专业的摘要专家。你的任务是生成一个最终的内容总结。\n\n================================[1m Human Message [0m=================================\n\n我提供你一份当前的总结，和一份新文本，你需要结合两者，精炼出一份新的总结，以作为参考材料回答问题。\n\n在总结时，请注意以下几点：\n- 你的任务是总结问题相关的文本，而不是回答问题。- 包含关键事件、重要事实和核心信息。\n- 省略不必要的细节。\n- 去除重复冗余内容，使语言更加简洁和凝练。\n\n按以下格式要求输出：\n[33;1m[1;3m{format_instructions}[0m\n\n[问题-开始]：\n[33;1m[1;3m{query}[0m\n[问题-结束]\n\n[当前内容总结-开始]：\n[33;1m[1;3m{previous_summary}[0m\n[当前内容总结-结束]\n\n[新文本-开始]：\n[33;1m[1;3m{current_summary}[0m\n[新文本-结束]\n\nrefine_result_saves = []for indice, item in enumerate(map_results):\t\t\tif indice == 0:\t\tprevious_summary = item.summary\telse:\t\trefine_result = refine_chain.invoke(\t\t\t{\t\t\t\t\"query\":a_query,\t\t\t\t\"previous_summary\":previous_summary,\t\t\t\t\"current_summary\":item.summary,\t\t\t}\t\t)\t\trefine_result_saves.append(refine_result)\t\tprevious_summary = refine_result.refined_summary\t\tprint(refine_result)\tprint('='*20)\t\n\n====================\nquery_context_reasoning='新文本提供了90版本阿修罗的另外三种优秀武器，分别是支点、别云和天丛云，补充了当前总结中未提及的武器信息。' refined_summary='90版本阿修罗的优秀武器包括荒古（排名第一）、妖刀村正（排名第二）、暗影蔽日（排名第三）、圣剑（排名第四）、支点、别云和天丛云。其中，支点适合光强修罗，别云有高黄字但存在黄字冲突问题，天丛云则有27白字且适合一觉cd换装。' summary_sufficiency_reasoning='当前总结已经涵盖了90版本阿修罗的大部分优秀武器，但新文本提供了更多细节，特别是支点、别云和天丛云的具体适用情况，这些信息对于回答问题是有帮助的。' summary_sufficiency_score=85 next_action='内容可继续完善'\n====================\nquery_context_reasoning='新文本补充了90版本阿修罗的优秀武器中未提及的名刀32和七支刀，但未给出具体排名。当前总结中已经包含了其他排名较高的武器，因此需要更新总结以包含新文本中的信息。' refined_summary='90版本阿修罗的优秀武器包括荒古（排名第一）、妖刀村正（排名第二）、暗影蔽日（排名第三）、圣剑（排名第四）、支点、别云、天丛云、名刀32和七支刀。其中，支点适合光强修罗，别云有高黄字但存在黄字冲突问题，天丛云则有27白字且适合一觉cd换装。' summary_sufficiency_reasoning='当前总结已经包含了大部分排名较高的90版本阿修罗武器，但未提及名刀32和七支刀的具体排名。新文本提供了这两款武器的信息，因此需要更新总结。' summary_sufficiency_score=85 next_action='内容可继续完善'\n====================\nquery_context_reasoning='新文本提供了荒古太的具体信息和妖刀村正的详细描述，补充了当前总结中未提及的内容。' refined_summary='90版本阿修罗的优秀武器排名为：1. 荒古太（未升级和升级后的技能攻击力及获取方式）；2. 妖刀村正（90版本新武器，无视和额外黄字优势）；3. 暗影蔽日（所有攻击力高，搭配需注意）。支点适合光强修罗，别云有高黄字但存在黄字冲突问题，天丛云则有27白字且适合一觉cd换装。' summary_sufficiency_reasoning='当前总结已经涵盖了90版本阿修罗的主要武器排行，但新文本提供了更详细的武器信息，特别是荒古太和妖刀村正的具体描述，可以进一步丰富总结内容。' summary_sufficiency_score=85 next_action='内容可继续完善'\n====================\nquery_context_reasoning='新文本提供了90版本阿修罗武器的主流测试排名，补充了当前总结中未提及的武器排名信息。' refined_summary='90版本阿修罗的优秀武器排名为：1. 吞噬魔-支点；2. 开魔能荒古；3. 妖刀村正；4. 暗影蔽日；5. 天丛云。支点适合光强修罗，别云有高黄字但存在黄字冲突问题，妖刀村正适合破锁血武器。' summary_sufficiency_reasoning='当前总结包含了90版本阿修罗的主要武器排名和部分武器的详细信息，但未提及妖刀村正适合破锁血武器的信息，需要补充。' summary_sufficiency_score=85 next_action='内容可继续完善'\n====================\nquery_context_reasoning='新文本提供了20人本环境下妖刀村正的排名和属性攻击规则，需要结合当前内容总结进行整合。' refined_summary='90版本阿修罗的优秀武器排名为：1. 吞噬魔-支点；2. 开魔能荒古；3. 妖刀村正；4. 暗影蔽日；5. 天丛云。支点适合光强修罗，别云有高黄字但存在黄字冲突问题，妖刀村正适合破锁血武器。20人本环境下，妖刀村正&gt;开魔能荒古&gt;圣剑=支点=避日，圣剑攻击力受自身属强影响，荒古属性攻击选最高值。' summary_sufficiency_reasoning='当前总结已经涵盖了90版本阿修罗的优秀武器排名和适用情况，但未完全包含20人本环境下的具体排名和属性规则，需要进一步完善。' summary_sufficiency_score=85 next_action='内容可继续完善'\n====================\nquery_context_reasoning='新文本为空，无需补充。当前总结已经涵盖了90版本阿修罗武器的排名和适用情况。' refined_summary='90版本阿修罗的优秀武器排名为：1. 吞噬魔-支点；2. 开魔能荒古；3. 妖刀村正；4. 暗影蔽日；5. 天丛云。支点适合光强修罗，妖刀村正适合破锁血武器。20人本环境下，妖刀村正&gt;开魔能荒古&gt;圣剑=支点=避日，圣剑攻击力受自身属强影响，荒古属性攻击选最高值。' summary_sufficiency_reasoning='当前总结已经涵盖了90版本阿修罗武器的排名和适用情况，信息较为全面。' summary_sufficiency_score=95 next_action='内容完整'\n====================\nquery_context_reasoning='新文本提供了90版本DNF修罗武器排行榜的最新信息，但与当前内容总结中的具体排名和详细分析有所差异，需要结合两者进行整合。' refined_summary='90版本阿修罗的优秀武器排名为：1. 吞噬魔-支点；2. 开魔能荒古；3. 无影剑；4. 妖刀村正；5. 暗影蔽日；6. 天丛云。支点适合光强修罗，妖刀村正适合破锁血武器。20人本环境下，妖刀村正&gt;开魔能荒古&gt;圣剑=支点=避日，圣剑攻击力受自身属强影响，荒古属性攻击选最高值。' summary_sufficiency_reasoning='当前总结提供了详细的武器排名和适用情况，但未提及无影剑的具体排名，需要补充。' summary_sufficiency_score=85 next_action='内容可继续完善'\n====================\nquery_context_reasoning='新文本补充了90版本阿修罗武器排行的相关信息，但未直接给出具体排行结果。当前内容总结已经包含了详细的排行结果，因此需要结合新文本进一步完善。' refined_summary='90版本阿修罗的优秀武器排名为：1. 吞噬魔-支点；2. 开魔能荒古；3. 无影剑；4. 妖刀村正；5. 暗影蔽日；6. 天丛云。支点适合光强修罗，妖刀村正适合破锁血武器。20人本环境下，妖刀村正&gt;开魔能荒古&gt;圣剑=支点=避日，圣剑攻击力受自身属强影响，荒古属性攻击选最高值。' summary_sufficiency_reasoning='当前总结已经包含了详细的排行结果和武器特点，但新文本提供了更多武器的信息，可以进一步完善总结。' summary_sufficiency_score=85 next_action='内容可继续完善'\n====================\nquery_context_reasoning='新文本提供了关于别云武器和支点的额外信息，补充了当前内容总结中未提及的装备搭配和属性影响。' refined_summary='90版本阿修罗的优秀武器排名为：1. 吞噬魔-支点；2. 开魔能荒古；3. 无影剑；4. 妖刀村正；5. 暗影蔽日；6. 天丛云。支点适合光强修罗，妖刀村正适合破锁血武器。20人本环境下，妖刀村正&gt;开魔能荒古&gt;圣剑=支点=避日，圣剑攻击力受自身属强影响，荒古属性攻击选最高值。别云武器适合搭配50黄字装备，支点适合幽魂套和光强修罗，圣剑100属强适合全属强套装，但释放速度慢影响手感。' summary_sufficiency_reasoning='当前总结已经涵盖了主要的武器排名和属性，但补充了关于别云武器和支点的额外信息，使得总结更加全面。' summary_sufficiency_score=95 next_action='内容完整'\n====================\nquery_context_reasoning='新文本补充了90版本阿修罗武器排行中的两个重要武器，暗影蔽日和妖刀村正，并指出了它们的适用流派。' refined_summary='90版本阿修罗的优秀武器排名为：1. 吞噬魔-支点；2. 开魔能荒古；3. 无影剑；4. 妖刀村正；5. 暗影蔽日；6. 天丛云。支点适合光强修罗，妖刀村正适合破锁血武器。20人本环境下，妖刀村正&gt;开魔能荒古&gt;圣剑=支点=避日。暗影蔽日适用于幽魂流光。' summary_sufficiency_reasoning='当前总结涵盖了90版本阿修罗的主要武器排名及其适用性，但未提及所有武器的详细对比，可以进一步完善。' summary_sufficiency_score=85 next_action='内容可继续完善'\n====================\n\nscore_threshold = 95refine_result_saves = []for indice, item in enumerate(map_results):\t\t\tif indice == 0:\t\tprevious_summary = item.summary\telse:\t\trefine_result = refine_chain.invoke(\t\t\t{\t\t\t\t\"query\":a_query,\t\t\t\t\"previous_summary\":previous_summary,\t\t\t\t\"current_summary\":item.summary,\t\t\t}\t\t)\t\trefine_result_saves.append(refine_result)\t\tprevious_summary = refine_result.refined_summary\t\t\t\t# 根据分数早停\t\tif refine_result.summary_sufficiency_score &gt; score_threshold:\t\t\tbreak\t\t# 根据大模型的判断早停\t\tif refine_result.next_action == '内容完整':\t\t\tbreak\t\tprint(refine_result)\tprint('='*20)\t","categories":["RAG 上下文压缩"]},{"title":"RAG 上下文压缩 - 优化map-reduce(reranker过滤+社区聚类)","url":"/2025/05/04/RAG-context-compression-series/3.map_reduce-filter_and_compress/","content":"我遇到的业务问题实际上是RAG需要处理很多同一对象的日常报告，不像常识类问题，它的相关Document更多而且更分散，日常报告代表数据库里有很多它的内容，而且对象可能只在段落中的几句话提及到。top-k数量受限于大模型长度，目前是top-10/15，明显一个月的情况都没法正常枚举，而且上下文中也有很多无关内容，既会干扰大模型又会占着token，所以必需对上下文进行过滤，到这一步后再压缩下文本。\n\n我的方法主要灵感来自于map-reduce总结方法，主要由于它可以并行处理的优点，利用小模型做简单任务。\n\nInner-chunk 过滤\n\n将块拆分为更小的单元，通过语义分块，\n使用重排序器过滤掉不相关的部分，\n将剩余的句子合并为较小的块或片段。\n\n\nOuter-chunk 重合成\n\n对片段执行社区检测，并将同一聚类中的片段在块长度限制内合并成新的块。社区检测的主要目的是将语义上相近或重复的内容保持在同一块中，以便我们能够很好地合并它们或去除冗余。\n合成上下文内容，确保保留相关和有用的信息，以便为后续的 QA 阶段提供合理的内容，同时防止丢失潜在有价值的上下文。\n\n\n\nimport osimport jsonfrom langchain_core.documents import Documentdata = []file_path = './data/data_100.json'with open(file_path) as f:\tfor line in f:\t\ta_record = json.loads(line)\t\tdata.append(a_record)print(len(data))data_indice = 0a_query = data[data_indice]['query']a_docs = data[data_indice]['pos']a_docs = [Document(item) for item in a_docs]\n\n100\n\nimport asyncioimport nest_asyncio# 应用 nest_asyncio 以支持 Jupyter 笔记本中的异步操作nest_asyncio.apply()\n\nInner-chunk 过滤原始代码from meta_chunking import meta_chunking_, sentences_doc = meta_chunking(a_docs[0].page_content, 'PPL Chunking', 'zh', 0.5, 256)print(*sentences_doc, sep='\\n====\\n')\n\n。据路透社6月17日报道,在华盛顿对中国在太平洋地区不断扩大的影响力感到担忧之际,路透社记者17日看到的一份声明显示,美国和马绍尔群岛同意努力在今年就美国对后者的经济援助达成协议。报道称,美国《自由联系协定》谈判特使尹汝尚本周前往马绍尔群岛,与这个具有战略意义的太平洋岛国的相关部门举行会谈。根据一份联合声明,双方希望在9月前签署一项谅解备忘录,“目的是在秋末或初冬之前完成《自由联系协定》谈判”。声明称,双方阐明了在2023年《自由联系协定》到期后美国继续向马绍尔群岛提供经济援助的重要性。\n====\n报道还说,长期以来,华盛顿与马绍尔群岛、帕劳和密克罗尼西亚联邦保持着特殊关系,这使得美军能够进入太平洋地区的广阔战略区域。但这三个太平洋岛国抱怨说,美方的援助没有跟上。尹汝尚还负责与密克罗尼西亚联邦和帕劳就延长《自由联系协定》进行谈判。\n====\n报道说,中国加强了与太平洋岛国的经济、军事和警务联系,并在商业和旅游方面向马绍尔群岛、帕劳和密克罗尼西亚联邦示好。前不久,中国与所罗门群岛签署安全协议,凸显了北京在该地区日益增强的影响力。此举引发了澳大利亚、新西兰和美国的关切。战略与国际问题研究中心的专家哈里森·普雷拉表示:“该地区国家更感兴趣的可能是轮流吸引中国和西方的投资,而不是作出加入北京阵营的承诺。”声明还说,尹汝尚和马绍尔群岛外长基特兰·卡布阿还讨论了美国在马绍尔群岛进行核试验的遗留问题。\n====\n在尹汝尚今年3月上任之前,这个问题一直是双方关系的症结所在。报道写道,1946年至1958年期间,美国在马绍尔群岛进行了67次核试验——其中包括1954年在比基尼环礁进行的“喝彩城堡”氢弹试验。这些核试验对健康和环境造成的影响迄今仍然令岛国居民感到困扰。另据路透社6月17日报道,在所罗门群岛与中国的安全协议引发地区关切之际,澳大利亚外长黄英贤周五造访所罗门群岛一个骚乱频发的社区,以强调澳大利亚警方的能力。黄英贤表示,在访问期间,她与所罗门群岛总理索加瓦雷举行了“建设性”会谈。\n====\n报道称,黄英贤的访问强调了澳大利亚对所罗门群岛的教育和医疗援助,以及在当地去年发生骚乱后澳大利亚警方为恢复稳定作出的努力。黄英贤对媒体称,索加瓦雷重申了他的公开保证,即根据与中方的协议,所罗门群岛不会有军事基地,也不会有持续的外国驻军。黄英贤周五说:“澳大利亚仍然认为,应该由太平洋大家庭来负责我们的安全问题。”黄英贤访问了所罗门群岛首都霍尼亚拉以东的伯恩斯克里克定居点,那里的房屋在去年11月的骚乱中被烧毁。澳大利亚、斐济和新西兰的警察以及当地年轻人参与了重建工作,在那里修建了一个诊所。\n\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder# Initialize the modelreranker = HuggingFaceCrossEncoder(\tmodel_name=\"../../DataCollection/officials/bge-reranker-v2-m3\",\tmodel_kwargs = {'device': 'cuda:6'}\t)reranker.score([('今天是星期几？', '今天是星期二。')])\n\n\n\n\narray([0.8927492], dtype=float32)\n\nfragment_maxlen = 256reranker_score_threshold = 0.1_, doc_fragments = meta_chunking(a_docs[0].page_content, 'PPL Chunking', 'zh', 0.5, fragment_maxlen)fragment_scores = reranker.score([(a_query, frag) for frag in doc_fragments])filtered_fragments = [\tfrag\tfor frag, score in zip(doc_fragments, fragment_scores)\tif score &gt; reranker_score_threshold]new_chunk = Document(''.join(filtered_fragments), metadata=a_docs[0].metadata)print(f'过滤前文本长度: {len(a_docs[0].page_content)}')print(f'过滤后文本长度: {len(new_chunk.page_content)}')\n\n过滤前文本长度: 1077\n过滤后文本长度: 485\n\n至于为什么要组合回去:保持文档逻辑结构原始文档被分割成多个chunk，每个chunk是根据文档内的逻辑顺序切分的。每个chunk内部的内容具有较强的上下文相关性，通常是围绕某个主题或段落展开。因此，组合回去可以最大限度地保留文档内部的逻辑顺序和连贯性。\n封装为 Runnableimport asynciofrom typing import List, Dictfrom langchain.schema import Documentfrom langchain.schema.runnable import Runnablefrom langchain_community.cross_encoders import HuggingFaceCrossEncoderclass Fragment_Filter(Runnable):\tdef __init__(self, reranker, fragment_maxlen=256, reranker_score_threshold=0.1):\t\t\"\"\"\t\t继承 Runnable，实现文档切片与筛选逻辑。\t\t:param model: 评分模型\t\t:param fragment_maxlen: 片段最大长度\t\t:param reranker_score_threshold: 重新排序得分阈值\t\t\"\"\"\t\tself.model = reranker\t\tself.fragment_maxlen = fragment_maxlen\t\tself.reranker_score_threshold = reranker_score_threshold\tasync def process_document(self, a_query: str, a_doc: Document) -&gt; Document:\t\t\"\"\"\t\t异步处理单个文档的 fragment，并根据得分筛选。\t\t:param a_query: 查询字符串\t\t:param a_doc: Document 对象\t\t:return: 处理后的 Document 对象\t\t\"\"\"\t\t# 文档切片\t\t_, doc_fragments = meta_chunking(\t\t\ta_doc.page_content, 'PPL Chunking', 'zh', 0.5, self.fragment_maxlen\t\t)\t\t# 计算 fragment 的得分\t\tfragment_scores = self.model.score([(a_query, frag) for frag in doc_fragments])\t\t# 过滤符合条件的 fragments\t\tfiltered_fragments = [\t\t\tfrag for frag, score in zip(doc_fragments, fragment_scores)\t\t\tif score &gt; self.reranker_score_threshold\t\t]\t\t# 返回新的 Document\t\treturn Document(''.join(filtered_fragments), metadata=a_doc.metadata)\tdef invoke(self, inputs: Dict[str, any]) -&gt; List[Document]:\t\t\"\"\"\t\t同步调用，并通过 asyncio.run() 调用异步任务。\t\t:param inputs: 包含 `query` 和 `documents` 的字典\t\t:return: 过滤后的 List[Document]\t\t\"\"\"\t\ta_query = inputs[\"query\"]\t\tdocuments = inputs[\"documents\"]\t\t# 使用 asyncio.run 执行异步的并行任务\t\treturn asyncio.run(self.ainvoke(a_query, documents))\tasync def ainvoke(self, a_query: str, documents: List[Document]) -&gt; List[Document]:\t\t\"\"\"\t\t异步处理多个文档的 fragment，并根据得分筛选。\t\t:param a_query: 查询字符串\t\t:param documents: 要处理的文档列表\t\t:return: 过滤后的 List[Document]\t\t\"\"\"\t\t# 使用 asyncio.gather 并行处理每个文档\t\ttasks = [self.process_document(a_query, a_doc) for a_doc in documents]\t\treturn await asyncio.gather(*tasks)fragment_filter = Fragment_Filter(reranker=reranker)\n\n\n# 示例用法result_doc = fragment_filter.invoke({\"query\": a_query, \"documents\": a_docs})print(f'过滤前文本长度: {sum([len(item.page_content) for item in a_docs])}')print(f'过滤后文本长度: {sum([len(item.page_content) for item in result_doc])}')\n\nOuter-chunk 重组织原始代码在做 query-focused summarization 时，出现了一个问题，summarization 始终会造成信息损失，在二次总结时最明显。\n例如问题是泰坦尼克号是如何沉没的，原始的总结只会返回泰坦尼克号沉没的信息，但有其他信息虽然与问题不直接相关，但确实可以作为上下文来优化问答效果，例如泰坦尼克号的背景、远航的原因和宣传、航线路线、沉没的后续救援行动、对行业的影响等等。这些辅助信息都因为不直接下相关被去除。\n我认为应该是任务定义错误，summarization一定是减少信息，因为他的定义本身就是总结。所以现在改为”上下文重新组织”，强调必须保留辅助性的细节。\n当然这估计只能缓解，所以我后期会再与query改写相结合，不管是改写为多轮query，还是单独改写为更丰富的query。\nfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import PromptTemplate, ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserreorganzie_prompt = PromptTemplate(\ttemplate=(\t\t\"Given the following set of document fragments and the query, filter out irrelevant content and reorganize the remaining fragments into a single, cohesive chunk.\\n\\n\"  \t\t\"The goal is to synthesize contextual content that will support a detailed and nuanced QA response, ensuring diverse perspectives are preserved while reducing redundancy. \"\t\t\"Retain important background information and supplementary details, even if they are less directly relevant to the query. \"\t\t\"Present the reorganized content in {language} language. \\n\"        \"[query-START]:\\n{query}\\n[query-END]\\n\"        \"[fragments-START]:\\n{fragments}\\n[fragments-END]\\n\"\t),\tpartial_variables={\t\t\"language\":\"Chinese\" # \"English\" or \"Chinese\"\t}    )llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-14B-Instruct',\ttemperature=0.2,)output_parser = StrOutputParser()reorganzie_chain = reorganzie_prompt | llm | output_parser\n\n\nquery = a_queryfragments = a_docs[0].page_contentresult = reorganzie_chain.invoke(    {        \"query\":query,        \"fragments\":fragments    })print(query, fragments, result, sep='\\n========\\n')\n\n澳大利亚新任外长黄英贤近日访问了哪个国家，他的目的是什么？\n========\n 。据路透社6月17日报道,在华盛顿对中国在太平洋地区不断扩大的影响力感到担忧之际,路透社记者17日看到的一份声明显示,美国和马绍尔群岛同意努力在今年就美国对后者的经济援助达成协议。报道称,美国《自由联系协定》谈判特使尹汝尚本周前往马绍尔群岛,与这个具有战略意义的太平洋岛国的相关部门举行会谈。根据一份联合声明,双方希望在9月前签署一项谅解备忘录,“目的是在秋末或初冬之前完成《自由联系协定》谈判”。声明称,双方阐明了在2023年《自由联系协定》到期后美国继续向马绍尔群岛提供经济援助的重要性。报道还说,长期以来,华盛顿与马绍尔群岛、帕劳和密克罗尼西亚联邦保持着特殊关系,这使得美军能够进入太平洋地区的广阔战略区域。但这三个太平洋岛国抱怨说,美方的援助没有跟上。尹汝尚还负责与密克罗尼西亚联邦和帕劳就延长《自由联系协定》进行谈判。报道说,中国加强了与太平洋岛国的经济、军事和警务联系,并在商业和旅游方面向马绍尔群岛、帕劳和密克罗尼西亚联邦示好。前不久,中国与所罗门群岛签署安全协议,凸显了北京在该地区日益增强的影响力。此举引发了澳大利亚、新西兰和美国的关切。战略与国际问题研究中心的专家哈里森·普雷拉表示:“该地区国家更感兴趣的可能是轮流吸引中国和西方的投资,而不是作出加入北京阵营的承诺。”声明还说,尹汝尚和马绍尔群岛外长基特兰·卡布阿还讨论了美国在马绍尔群岛进行核试验的遗留问题。在尹汝尚今年3月上任之前,这个问题一直是双方关系的症结所在。报道写道,1946年至1958年期间,美国在马绍尔群岛进行了67次核试验——其中包括1954年在比基尼环礁进行的“喝彩城堡”氢弹试验。这些核试验对健康和环境造成的影响迄今仍然令岛国居民感到困扰。另据路透社6月17日报道,在所罗门群岛与中国的安全协议引发地区关切之际,澳大利亚外长黄英贤周五造访所罗门群岛一个骚乱频发的社区,以强调澳大利亚警方的能力。黄英贤表示,在访问期间,她与所罗门群岛总理索加瓦雷举行了“建设性”会谈。报道称,黄英贤的访问强调了澳大利亚对所罗门群岛的教育和医疗援助,以及在当地去年发生骚乱后澳大利亚警方为恢复稳定作出的努力。黄英贤对媒体称,索加瓦雷重申了他的公开保证,即根据与中方的协议,所罗门群岛不会有军事基地,也不会有持续的外国驻军。黄英贤周五说:“澳大利亚仍然认为,应该由太平洋大家庭来负责我们的安全问题。”黄英贤访问了所罗门群岛首都霍尼亚拉以东的伯恩斯克里克定居点,那里的房屋在去年11月的骚乱中被烧毁。澳大利亚、斐济和新西兰的警察以及当地年轻人参与了重建工作,在那里修建了一个诊所。\n========\n澳大利亚新任外长黄英贤近日访问了所罗门群岛。据报道，黄英贤在访问期间与所罗门群岛总理索加瓦雷进行了“建设性”的会谈。黄英贤强调了澳大利亚对所罗门群岛的教育和医疗援助，以及在去年发生骚乱后澳大利亚警方为恢复稳定所作出的努力。黄英贤表示，索加瓦雷重申了他不会在所罗门群岛设立军事基地或持续驻扎外国军队的公开保证。黄英贤还访问了所罗门群岛首都霍尼亚拉以东的伯恩斯克里克定居点，该定居点在去年11月的骚乱中遭受了严重破坏。澳大利亚、斐济和新西兰的警察以及当地年轻人参与了重建工作，其中包括修建了一个诊所。黄英贤的访问正值中国与所罗门群岛签署安全协议，引发澳大利亚、新西兰和美国的关切之际。\n\n封装import asyncioimport numpy as npfrom typing import List, Dictfrom langchain_openai import ChatOpenAIfrom langchain.schema.runnable import Runnablefrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom sklearn.cluster import AgglomerativeClusteringfrom sklearn.metrics.pairwise import cosine_similarityclass Content_Reorganizer(Runnable):\t\tdef __init__(self, llm, embedding_model, threshold:float=0.5, chunk_size:int=512):\t\tself.llm = llm\t\tself.embedding_model = embedding_model\t\tself.threshold = threshold\t\tself.chunk_size = chunk_size\t\tself.reorganzie_prompt = PromptTemplate(\t\t\ttemplate=(\t\t\t\t\"Given the following set of document fragments and the query, filter out irrelevant content and reorganize the remaining fragments into a single, cohesive chunk.\\n\\n\"  \t\t\t\t\"The goal is to synthesize contextual content that will support a detailed and nuanced QA response, ensuring diverse perspectives are preserved while reducing redundancy. \"\t\t\t\t\"Retain important background information and supplementary details, even if they are less directly relevant to the query. \"\t\t\t\t\"Present the reorganized content in {language} language. \\n\"\t\t\t\t\"[query-START]:\\n{query}\\n[query-END]\\n\"\t\t\t\t\"[fragments-START]:\\n{fragments}\\n[fragments-END]\\n\"\t\t\t),\t\t\tpartial_variables={\t\t\t\t\"language\":\"Chinese\" # \"English\" or \"Chinese\"\t\t\t}    \t\t)\t\toutput_parser = StrOutputParser()\t\tself.reorganzie_chain = self.reorganzie_prompt | llm | output_parser\t\tdef compute_similarity_matrix(self, texts):\t\tembeddings = self.embedding_model.embed_documents(texts)\t\tembeddings = np.array(embeddings)\t\tsimilarity_matrix = cosine_similarity(embeddings)\t\treturn similarity_matrix\t\tdef hierarchical_community_detection(self, texts):\t\t\t\t# 计算相似度矩阵\t\tsimilarity_matrix = self.compute_similarity_matrix(texts)\t\tdistance_matrix = 1 - similarity_matrix\t\t\t\t# 以相似度为阈值来控制聚类\t\tclustering = AgglomerativeClustering(\t\t\tmetric='precomputed',\t\t\tlinkage='average',\t\t\tdistance_threshold=self.threshold,\t\t\tn_clusters=None\t\t)\t\t\t\t# 聚类结果\t\tlabels = clustering.fit_predict(distance_matrix)\t\t\t\t# 输出每个社区的文本\t\tcommunities = {}\t\tfor idx, label in enumerate(labels):\t\t\tif label not in communities:\t\t\t\tcommunities[label] = []\t\t\tcommunities[label].append(texts[idx])\t\treturn communities\t\tdef combine_texts_into_chunks(self, communities):\t\tchunks = []\t\t\tfor community, texts_in_community in communities.items():\t\t\tcurrent_chunk = []\t\t\tcurrent_chunk_size = 0\t\t\t\t\t\tfor text in texts_in_community:\t\t\t\ttext_size = len(text.split())  # 计算文本大小（按单词数）\t\t\t\t\t\t\t\t# 如果当前文本大小超过chunk_size，则直接单独放入一个块\t\t\t\tif text_size &gt; self.chunk_size:\t\t\t\t\tif current_chunk:  # 当前块不为空，则保存并开始新块\t\t\t\t\t\tchunks.append(\" \".join(current_chunk))\t\t\t\t\t\tcurrent_chunk = []\t\t\t\t\t\tcurrent_chunk_size = 0\t\t\t\t\tchunks.append(text)  # 该文本单独作为一个块\t\t\t\telse:\t\t\t\t\t# 当前块 + 文本的大小是否超过chunk_size\t\t\t\t\tif current_chunk_size + text_size &lt;= self.chunk_size:\t\t\t\t\t\tcurrent_chunk.append(text)\t\t\t\t\t\tcurrent_chunk_size += text_size\t\t\t\t\telse:\t\t\t\t\t\t# 当前块满了，保存并开始新块\t\t\t\t\t\tchunks.append(\" \".join(current_chunk))\t\t\t\t\t\tcurrent_chunk = [text]\t\t\t\t\t\tcurrent_chunk_size = text_size\t\t\t\t\t\t# 如果还有剩余的文本块，添加到chunks中\t\t\tif current_chunk:\t\t\t\tchunks.append(\" \".join(current_chunk))\t\t\t\treturn chunks\t\tdef invoke(self, inputs: Dict[str, any]):\t\treturn asyncio.run(self.ainvoke(inputs))\tasync def ainvoke(self, inputs: Dict[str, any]):\t\t\t\tquery = inputs[\"query\"]\t\tdocuments = inputs[\"documents\"]\t\ttexts = [doc.page_content for doc in documents]\t\tcommunities = self.hierarchical_community_detection(texts)\t\tchunks = self.combine_texts_into_chunks(communities)\t\ttasks = [self.process_document(query, content) for content in chunks]\t\treturn await asyncio.gather(*tasks)reorganizer = Content_Reorganizer()\n\n\nimport asyncioimport numpy as npfrom typing import List, Dictfrom langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIEmbeddingsfrom langchain.schema.runnable import Runnablefrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom sklearn.cluster import AgglomerativeClusteringfrom sklearn.metrics.pairwise import cosine_similarityclass Content_Reorganizer(Runnable):        def __init__(self, llm, embedding_model, threshold: float = 0.5, chunk_size: int = 512):        \"\"\"        初始化 ContentReorganizer 类，接受大模型、嵌入模型、阈值和块大小等参数。                :param llm: 用于生成重组内容的语言模型        :param embedding_model: 用于生成文本嵌入的模型        :param threshold: 社区检测时相似度的阈值        :param chunk_size: 生成的每个块的最大大小（以单词为单位）        \"\"\"        self.llm = llm        self.embedding_model = embedding_model        self.threshold = threshold        self.chunk_size = chunk_size        # 用于内容重组的提示模板        self.reorganzie_prompt = PromptTemplate(            template=(                \"Given the following set of document fragments and the query, filter out irrelevant content and reorganize the remaining fragments into a single, cohesive chunk.\\n\\n\"                  \"The goal is to synthesize contextual content that will support a detailed and nuanced QA response, ensuring diverse perspectives are preserved while reducing redundancy. \"                \"Retain important background information and supplementary details, even if they are less directly relevant to the query. \"                \"Present the reorganized content in {language} language. \\n\"                \"[query-START]:\\n{query}\\n[query-END]\\n\"                \"[fragments-START]:\\n{fragments}\\n[fragments-END]\\n\"            ),            partial_variables={                \"language\": \"Chinese\"  # \"English\" 或 \"Chinese\"            }        )        # 输出解析器，将最终生成的文本从模型输出中提取出来        output_parser = StrOutputParser()        self.reorganzie_chain = self.reorganzie_prompt | llm | output_parser    def compute_similarity_matrix(self, texts: List[str]) -&gt; np.ndarray:        \"\"\"        计算文本之间的相似度矩阵。        :param texts: 输入的文本列表        :return: 文本之间的相似度矩阵        \"\"\"        # 获取文本的嵌入表示        embeddings = self.embedding_model.embed_documents(texts)        embeddings = np.array(embeddings)                # 计算余弦相似度矩阵        similarity_matrix = cosine_similarity(embeddings)        return similarity_matrix        def hierarchical_community_detection(self, texts: List[str]) -&gt; Dict[int, List[str]]:        \"\"\"        基于文本的相似度矩阵进行层次社区检测。        :param texts: 输入的文本列表        :return: 返回每个社区的文本列表        \"\"\"        # 计算相似度矩阵并转换为距离矩阵        similarity_matrix = self.compute_similarity_matrix(texts)        distance_matrix = 1 - similarity_matrix                # 层次聚类        clustering = AgglomerativeClustering(            metric='precomputed',  # 使用预计算的距离矩阵            linkage='average',     # 使用平均链接法            distance_threshold=self.threshold,            n_clusters=None        )                # 获取聚类结果        labels = clustering.fit_predict(distance_matrix)                # 输出每个社区的文本        communities = {}        for idx, label in enumerate(labels):            if label not in communities:                communities[label] = []            communities[label].append(texts[idx])        return communities        def combine_texts_into_chunks(self, communities: Dict[int, List[str]]) -&gt; List[str]:        \"\"\"        将社区内的文本重新组合成大小不超过 `chunk_size` 的块。        :param communities: 每个社区及其对应文本的字典        :return: 组合后的文本块列表        \"\"\"        chunks = []                # 遍历每个社区的文本        for community, texts_in_community in communities.items():            current_chunk = []            current_chunk_size = 0                        for text in texts_in_community:                text_size = len(text.split())  # 计算文本的大小（按单词数）                                # 如果当前文本超过 chunk_size，直接单独作为一个块                if text_size &gt; self.chunk_size:                    if current_chunk:  # 当前块不为空，将其保存                        chunks.append(\" \".join(current_chunk))                        current_chunk = []                        current_chunk_size = 0                    chunks.append(text)  # 单个文本单独作为一个块                else:                    # 判断当前块 + 文本是否超过 chunk_size                    if current_chunk_size + text_size &lt;= self.chunk_size:                        current_chunk.append(text)                        current_chunk_size += text_size                    else:                        # 当前块满了，保存并开始新的块                        chunks.append(\" \".join(current_chunk))                        current_chunk = [text]                        current_chunk_size = text_size                        # 如果最后还有剩余的文本块，添加到 chunks 中            if current_chunk:                chunks.append(\" \".join(current_chunk))                return chunks        async def process_document(self, query: str, fragment: str) -&gt; str:        \"\"\"        使用 LLM 进行文档内容的重组。        :param query: 输入查询        :param fragment: 文本片段        :return: 生成的重组文本        \"\"\"        # 组织提示并调用 LLM 进行内容重组        result = await self.reorganzie_chain.ainvoke({\"query\": query, \"fragments\": fragment})        return result        async def ainvoke(self, inputs: Dict[str, any]) -&gt; List[str]:        \"\"\"        异步方法，执行内容重组过程。        :param inputs: 输入参数，包括查询和文档列表        :return: 返回重组后的文本列表        \"\"\"        query = inputs[\"query\"]        documents = inputs[\"documents\"]                # 获取文档的文本内容        texts = [doc.page_content for doc in documents]                # 进行社区检测        communities = self.hierarchical_community_detection(texts)                # 将社区内部文本重新组合成块        chunks = self.combine_texts_into_chunks(communities)                # 并行处理每个块，调用 LLM 进行重组        tasks = [self.process_document(query, content) for content in chunks]        return await asyncio.gather(*tasks)        def invoke(self, inputs: Dict[str, any]) -&gt; List[str]:        \"\"\"        同步方法，调用异步的 `ainvoke` 方法。        :param inputs: 输入参数，包括查询和文档列表        :return: 返回重组后的文本列表        \"\"\"        return asyncio.run(self.ainvoke(inputs))openai_embedding = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='Empty',\t# dimensions=1024,)llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-14B-Instruct',\ttemperature=0.2,)reorganizer = Content_Reorganizer(llm, openai_embedding)results = reorganizer.invoke(    {        \"query\":a_query,        \"documents\":result_doc    })\n\n\nprint(*results, sep='\\n====\\n')\n\n澳大利亚新任外长黄英贤近日访问了所罗门群岛。据报道，黄英贤在6月17日访问了所罗门群岛一个骚乱频发的社区，以强调澳大利亚警方的能力。她与所罗门群岛总理索加瓦雷进行了“建设性”的会谈，并表示澳大利亚仍然认为，应该由太平洋大家庭来负责安全问题。黄英贤的访问强调了澳大利亚对所罗门群岛的教育和医疗援助，以及在去年发生骚乱后澳大利亚警方为恢复稳定所作的努力。此外，索加瓦雷重申了所罗门群岛不会根据与中国的安全协议设立军事基地或持续的外国驻军的公开保证。黄英贤还访问了所罗门群岛首都霍尼亚拉以东的伯恩斯克里克定居点，该定居点在去年11月的骚乱中遭受了破坏。澳大利亚、斐济和新西兰的警察以及当地年轻人参与了重建工作，修建了一个诊所。\n====\n澳大利亚新任外长黄英贤近日访问了萨摩亚和汤加这两个太平洋岛国。她表示，澳大利亚将加强与太平洋国家的安全合作，并强调与太平洋岛国之间的联系。黄英贤上任后尤其重视加强澳大利亚与太平洋岛国之间的关系，她认为澳大利亚需要表明自己是该地区国家可靠和值得信赖的伙伴，并且“决心弥补”在气候行动方面“失去的十年”。此外，黄英贤还于12月20日至21日对中国进行了访问，这是四年多来澳大利亚外长首次访华，标志着中澳关系迈出重要一步。黄英贤表示，她将在同王毅的会晤中推动贸易限制措施的取消，并称两国关系中有许多棘手问题，需要时间根据双方各自利益去解决。黄英贤此次访问也引起了媒体的广泛关注，被认为是中澳关系正在慢慢解冻的信号。\n\n","categories":["RAG 上下文压缩"]},{"title":"RAGas 忠实度 Faithfulness","url":"/2025/05/04/RAGas-langchain-series/1.Faithfulness/","content":"忠实度是指答案应基于给定的上下文。这对于避免幻觉至关重要，同时也能确保检索到的上下文可作为生成答案的依据。实际上，检索增强生成（RAG）系统常用于对生成文本与事实来源的一致性要求很高的应用场景，例如在信息不断更新的法律等领域。\n如果答案中的主张能够从上下文中推断出来，我们就认为该答案忠实于上下文。为了评估忠实度，我们首先使用大语言模型（LLM）提取一组陈述 。这一步的目的是将较长的句子分解为更简短、更聚焦的断言/陈述 statement。我们在这一步使用以下提示词：\n给定一个问题和答案，从给定答案的每个句子中创建一个或多个陈述。问题：[问题]答案：[答案]\n其中，[问题]和[答案]指的是给定的问题和答案。对于集合中的每个陈述，大语言模型会使用验证函数来判断是否能从中推断出来。这个验证步骤使用以下提示词：\n考虑给定的上下文和以下陈述，然后判断这些陈述是否能从上下文中的信息得到支持。在得出结论（是/否）之前，对每个陈述进行简要解释。最后按照给定的格式依次给出每个陈述的最终结论。不要偏离指定的格式。陈述：[陈述1]……陈述：[陈述n]\n最终的忠实度得分的计算方式为，其中是根据大语言模型判断得到支持的陈述数量，是陈述的总数。 \n这是测试数据\nanswer = \"美国最高法院关于堕胎的裁决具有重要的全球影响。该裁决导致在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。那些州的母婴健康支持也较弱，母亲死亡率较高，儿童贫困率也较高。此外，裁决的影响超出了国界，由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。裁决还妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。此外，该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。\"docs = [\t\"- 2022年，美国最高法院作出裁决，推翻了50年的判例法，取消了宪法堕胎权。\\n- 这一裁决产生了巨大影响：三分之一的生育年龄女性和女孩现在生活在堕胎服务几乎完全无法获得的州。\\n- 这些堕胎法律最为严格的州，母婴健康支持最为薄弱，母亲死亡率较高，儿童贫困率较高。\\n- 美国最高法院的裁决还通过美国在全球的地缘政治和文化影响力，超越国界产生了影响。\\n- 全球的SRR组织和活动家对这一裁决可能为其他国家的反堕胎立法和政策攻击铺路表示担忧。\\n- 观察者还注意到该裁决对某些非洲国家的进步法律改革产生了影响，导致堕胎指导方针的 adoption 和执行停滞不前。\\n- 该裁决在国际政策领域产生了寒蝉效应，助长了反堕胎的国家和非国家行为体破坏人权保护的势头。\",\t\"美国最高法院的堕胎裁决不仅在国内引发了激烈的辩论和讨论，也在全球范围内引发了广泛关注。许多国家将美国视为法律和社会问题的领导者，因此这一裁决可能会影响其他国家对堕胎的政策和态度。\",\t\"这一裁决还可能对国际组织和非政府组织（NGO）产生影响，尤其是那些致力于生育权和妇女健康问题的团体。根据裁决的结果，可能会出现资金、倡导工作和与美国同行的合作发生变化，进而在全球范围内引发生育正义斗争的连锁反应。\"]\n\n实现介绍指标原理:忠实度指标用于评估生成回答是否忠于提供的上下文信息。其核心思想是：\n\n从回答中提取陈述：将回答拆解为多个可验证的原子化陈述。\n在上下文中验证陈述：检查这些陈述是否可以从上下文信息推导得出。\n计算忠实度得分：忠实度得分是可验证陈述的比例。\n\n背景原因:生成式模型可能会生成幻觉，即回答中包含超出提供上下文的信息。忠实度指标可以帮助检测这种情况，确保模型回答与上下文一致，提高模型的可靠性。\n实现流程:  \n\n解析回答：使用 LLM 提取回答中的关键陈述。\n验证陈述：使用 LLM 判断每个陈述是否可以在上下文中得到支持。\n计算评分：统计可验证陈述的比例，作为最终得分。\n\n解析回答 create statementsfrom typing import Listfrom langchain_core.documents import Documentfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Field'''输出格式形如：{\t\"sentence_and_statements\": \t\t[\t\t\t# 这是第一个句子和它可拆分的内容陈述\t\t\t{\t\t\t\t\"origin_sentence\": 回答中某个句子的原始内容，为了保证连贯性让模型输出,\t\t\t\t\"statements\":[\t\t\t\t\t{\t\t\t\t\t\t\"description\":这是内容陈述1的具体内容。\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t\t\"description\":这是内容陈述2的具体内容。\t\t\t\t\t},\t\t\t\t\t...\t\t\t\t]\t\t\t},\t\t\t...\t\t]}'''# 用以下 pydantic 类来实现上述的输出结构, 解析时可以顺便做下解析验证class Statement(BaseModel):\tdescription: str = Field(description=\"这是内容陈述的具体内容。\")class Sentence_And_Statement(BaseModel):\torigin_sentence: str = Field(description=\"这是回答的某个句子。\")\tstatements: List[Statement] = Field(description=\"这是由上述句子中提取的一组内容陈述。\")class Sentence_And_Statement_List(BaseModel):\tsentence_and_statements: List[Sentence_And_Statement] = Field(description=\"这是由文本中提取的每组句子和对应的内容陈述。\")statement_output_parser = PydanticOutputParser(pydantic_object=Sentence_And_Statement_List)llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.5,)create_statement_prompt = (\t\"给定一个包含若干句子的文本，分析每个句子并将每个句子分解为一个或多个原子化的完全可理解的内容陈述，同时确保每个语句中不使用代词，且每个对象都需要具体，在其所在陈述中可理解。\\n\"\t\"输出格式：\\n{output_format_instructions}\\n\\n\"\t\"[文本-开始]：\\n{context}\\n[文本-结束]\\n\")create_statement_prompt = PromptTemplate(\ttemplate=create_statement_prompt, \tpartial_variables={\t\t\"output_format_instructions\":statement_output_parser.get_format_instructions()\t})create_statement_chain = create_statement_prompt | llm | statement_output_parser\n\n\ncreate_statement_prompt.pretty_print()\n\n给定一个包含若干句子的文本，分析每个句子并将每个句子分解为一个或多个原子化的完全可理解的内容陈述，同时确保每个语句中不使用代词，且每个对象都需要具体，在其所在陈述中可理解。\n输出格式：\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n{\"$defs\": {\"Sentence_And_Statement\": {\"properties\": {\"origin_sentence\": {\"description\": \"这是回答的某个句子。\", \"title\": \"Origin Sentence\", \"type\": \"string\"}, \"statements\": {\"description\": \"这是由上述句子中提取的一组内容陈述。\", \"items\": {\"$ref\": \"#/$defs/Statement\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"origin_sentence\", \"statements\"], \"title\": \"Sentence_And_Statement\", \"type\": \"object\"}, \"Statement\": {\"properties\": {\"description\": {\"description\": \"这是内容陈述的具体内容。\", \"title\": \"Description\", \"type\": \"string\"}}, \"required\": [\"description\"], \"title\": \"Statement\", \"type\": \"object\"}}, \"properties\": {\"sentence_and_statements\": {\"description\": \"这是由文本中提取的每组句子和对应的内容陈述。\", \"items\": {\"$ref\": \"#/$defs/Sentence_And_Statement\"}, \"title\": \"Sentence And Statements\", \"type\": \"array\"}}, \"required\": [\"sentence_and_statements\"]}\n\n[文本-开始]：\n[33;1m[1;3m{context}[0m\n[文本-结束]\n\n​    \nstatement_list = create_statement_chain.invoke({\"context\":answer})\n\n\nstatements = sum([item.statements for item in statement_list.sentence_and_statements], [])\n\n\nstatements\n\n\n\n\n[Statement(description='美国最高法院关于堕胎的裁决具有重要的全球影响。'),\n Statement(description='在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。'),\n Statement(description='那些州的母婴健康支持较弱。'),\n Statement(description='那些州的母亲死亡率较高。'),\n Statement(description='那些州的儿童贫困率较高。'),\n Statement(description='裁决的影响超出了国界。'),\n Statement(description='由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。'),\n Statement(description='全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。'),\n Statement(description='裁决妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。'),\n Statement(description='该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。')]\n\n这是测试数据的提取结果\n[Statement(description='美国最高法院关于堕胎的裁决具有重要的全球影响。'), Statement(description='在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。'), Statement(description='那些州的母婴健康支持较弱。'), Statement(description='那些州的母亲死亡率较高。'), Statement(description='那些州的儿童贫困率较高。'), Statement(description='裁决的影响超出了国界。'), Statement(description='由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。'), Statement(description='全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。'), Statement(description='裁决妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。'), Statement(description='该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。')]\n\n验证陈述 statement verdict'''输出格式形如：{\t\"statement_verdications\":\t\t[\t\t\t# 这是第一个内容陈述和它的验证结果\t\t\t{\t\t\t\t\"statement\": 某个内容陈述，为了保证模型不会中途乱输出,\t\t\t\t\"verdication_reasoning\":这是判断陈述是否可以由上下文推断时的思考内容\t\t\t\t\"verdication_result\":陈述可信度，如果可以根据上下文直接推断该出陈述，则为1，否则为0。\t\t\t},\t\t\t...\t\t]}'''class Statement_Verdication(BaseModel):\tstatement: str = Field(description=\"这是内容陈述的内容，你直接复制原始内容。\")\tverdication_reasoning: str = Field(description=\"这是判断陈述是否可以由上下文推断时的思考内容\")\tverdication_result: int = Field(description=\"陈述可信度，如果可以根据上下文直接推断该出陈述，则为1，否则为0。\")class Statement_Verdication_List(BaseModel):\tstatement_verdications: List[Statement_Verdication] = Field(description=\"这是一组内容陈述的是否可由上下文推断出的结果。\")verdict_statement_parser = PydanticOutputParser(pydantic_object=Statement_Verdication_List)verdict_statement_prompt = (\t\"你的任务是根据给定的上下文判断一系列内容陈述的可信度。对于每个陈述，如果可以根据上下文直接推断该出陈述，则返回1，否则返回0。\\n\"\t\"输出格式：\\n{output_format_instructions}\\n\\n\"\t\"[上下文-开始]：\\n{context}\\n[上下文-结束]\\n\"\t\"[内容陈述-开始]：\\n{statements}\\n[内容陈述-结束]\")verdict_statement_prompt = PromptTemplate(\ttemplate=verdict_statement_prompt, \tpartial_variables={\t\t\"output_format_instructions\":verdict_statement_parser.get_format_instructions()\t})verdict_statement_chain = verdict_statement_prompt | llm | verdict_statement_parser\n\n\nverdict_statement_prompt.pretty_print()\n\n你的任务是根据给定的上下文判断一系列内容陈述的可信度。对于每个陈述，如果可以根据上下文直接推断该出陈述，则返回1，否则返回0。\n输出格式：\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n{\"$defs\": {\"Statement_Verdication\": {\"properties\": {\"statement\": {\"description\": \"这是内容陈述的内容，你直接复制原始内容。\", \"title\": \"Statement\", \"type\": \"string\"}, \"verdication_reasoning\": {\"description\": \"这是判断陈述是否可以由上下文推断时的思考内容\", \"title\": \"Verdication Reasoning\", \"type\": \"string\"}, \"verdication_result\": {\"description\": \"陈述可信度，如果可以根据上下文直接推断该出陈述，则为1，否则为0。\", \"title\": \"Verdication Result\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"verdication_reasoning\", \"verdication_result\"], \"title\": \"Statement_Verdication\", \"type\": \"object\"}}, \"properties\": {\"statement_verdications\": {\"description\": \"这是一组内容陈述的是否可由上下文推断出的结果。\", \"items\": {\"$ref\": \"#/$defs/Statement_Verdication\"}, \"title\": \"Statement Verdications\", \"type\": \"array\"}}, \"required\": [\"statement_verdications\"]}\n\n[上下文-开始]：\n[33;1m[1;3m{context}[0m\n[上下文-结束]\n[内容陈述-开始]：\n[33;1m[1;3m{statements}[0m\n[内容陈述-结束]\n\nverdict_results = verdict_statement_chain.invoke(    {        \"context\":'\\n'.join(docs),        \"statements\":'\\n'.join([f'{indice+1}. {item.description}' for indice, item in enumerate(statements)])\t})\n\n\nverdict_results.statement_verdications\n\n\n\n\n[Statement_Verdication(statement='美国最高法院关于堕胎的裁决具有重要的全球影响。', verdication_reasoning='上下文明确提到这一裁决‘超越国界产生了影响’并且‘在全球范围内引发了广泛关注’。', verdication_result=1),\n Statement_Verdication(statement='在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。', verdication_reasoning='上下文直接提到‘三分之一的生育年龄女性和女孩现在生活在堕胎服务几乎完全无法获得的州’。', verdication_result=1),\n Statement_Verdication(statement='那些州的母婴健康支持较弱。', verdication_reasoning='上下文明确指出‘这些堕胎法律最为严格的州，母婴健康支持最为薄弱’。', verdication_result=1),\n Statement_Verdication(statement='那些州的母亲死亡率较高。', verdication_reasoning='上下文提到‘母亲死亡率较高’。', verdication_result=1),\n Statement_Verdication(statement='那些州的儿童贫困率较高。', verdication_reasoning='上下文提到‘儿童贫困率较高’。', verdication_result=1),\n Statement_Verdication(statement='裁决的影响超出了国界。', verdication_reasoning='上下文明确提到‘这一裁决通过美国在全球的地缘政治和文化影响力，超越国界产生了影响’。', verdication_result=1),\n Statement_Verdication(statement='由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。', verdication_reasoning='上下文提到‘这一裁决通过美国在全球的地缘政治和文化影响力，超越国界产生了影响’。', verdication_result=1),\n Statement_Verdication(statement='全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。', verdication_reasoning='上下文提到‘全球的SRR组织和活动家对这一裁决可能为其他国家的反堕胎立法和政策攻击铺路表示担忧’。', verdication_result=1),\n Statement_Verdication(statement='裁决妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。', verdication_reasoning='上下文提到‘观察者还注意到该裁决对某些非洲国家的进步法律改革产生了影响，导致堕胎指导方针的 adoption 和执行停滞不前’。', verdication_result=1),\n Statement_Verdication(statement='该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。', verdication_reasoning='上下文提到‘该裁决在国际政策领域产生了寒蝉效应，助长了反堕胎的国家和非国家行为体破坏人权保护的势头’。', verdication_result=1)]\n\n以下是测试数据的验证结果，主要看verdication_result字段\n[Statement_Verdication(statement='美国最高法院关于堕胎的裁决具有重要的全球影响。', verdication_reasoning='上下文明确提到这一裁决‘超越国界产生了影响’并且‘在全球范围内引发了广泛关注’。', verdication_result=1), Statement_Verdication(statement='在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。', verdication_reasoning='上下文直接提到‘三分之一的生育年龄女性和女孩现在生活在堕胎服务几乎完全无法获得的州’。', verdication_result=1), Statement_Verdication(statement='那些州的母婴健康支持较弱。', verdication_reasoning='上下文明确指出‘这些堕胎法律最为严格的州，母婴健康支持最为薄弱’。', verdication_result=1), Statement_Verdication(statement='那些州的母亲死亡率较高。', verdication_reasoning='上下文提到‘母亲死亡率较高’。', verdication_result=1), Statement_Verdication(statement='那些州的儿童贫困率较高。', verdication_reasoning='上下文提到‘儿童贫困率较高’。', verdication_result=1), Statement_Verdication(statement='裁决的影响超出了国界。', verdication_reasoning='上下文明确提到‘这一裁决通过美国在全球的地缘政治和文化影响力，超越国界产生了影响’。', verdication_result=1), Statement_Verdication(statement='由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。', verdication_reasoning='上下文提到‘这一裁决通过美国在全球的地缘政治和文化影响力，超越国界产生了影响’。', verdication_result=1), Statement_Verdication(statement='全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。', verdication_reasoning='上下文提到‘全球的SRR组织和活动家对这一裁决可能为其他国家的反堕胎立法和政策攻击铺路表示担忧’。', verdication_result=1), Statement_Verdication(statement='裁决妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。', verdication_reasoning='上下文提到‘观察者还注意到该裁决对某些非洲国家的进步法律改革产生了影响，导致堕胎指导方针的 adoption 和执行停滞不前’。', verdication_result=1), Statement_Verdication(statement='该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。', verdication_reasoning='上下文提到‘该裁决在国际政策领域产生了寒蝉效应，助长了反堕胎的国家和非国家行为体破坏人权保护的势头’。', verdication_result=1)]\n\n\nverdications = verdict_results.statement_verdicationsnum_faithful_statements = sum([item.verdication_result for item in verdications])num_statements = len(verdications)score = num_faithful_statements / num_statementsprint(score)\n\n1.0\n\n完整代码继承 Runnable, 可像langchain实例一样invoke调用\n\"\"\"## 忠实度指标（Faithfulness Metric）教程### 指标原理忠实度指标用于评估生成回答是否忠于提供的上下文信息。其核心思想是：1. **从回答中提取陈述**：将回答拆解为多个可验证的原子化陈述。2. **在上下文中验证陈述**：检查这些陈述是否可以从上下文信息推导得出。3. **计算忠实度得分**：忠实度得分是可验证陈述的比例。### 背景原因生成式模型可能会生成幻觉，即回答中包含超出提供上下文的信息。忠实度指标可以帮助检测这种情况，确保模型回答与上下文一致，提高模型的可靠性。### 实现流程1. **解析回答**：使用 LLM 提取回答中的关键陈述。2. **验证陈述**：使用 LLM 判断每个陈述是否可以在上下文中得到支持。3. **计算评分**：统计可验证陈述的比例，作为最终得分。\"\"\"import jsonimport numpy as npimport asynciofrom typing import List, Dictfrom langchain.schema.runnable import Runnablefrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Fieldclass Statement(BaseModel):    description: str = Field(description=\"这是内容陈述的具体内容。\")class Sentence_And_Statement(BaseModel):    origin_sentence: str = Field(description=\"这是回答的某个句子。\")    statements: List[Statement] = Field(description=\"这是由上述句子中提取的一组内容陈述。\")class Sentence_And_Statement_List(BaseModel):    sentence_and_statements: List[Sentence_And_Statement] = Field(description=\"这是由文本中提取的每组句子和对应的内容陈述。\")class Statement_Verdication(BaseModel):    statement: str = Field(description=\"这是内容陈述的内容，你直接复制原始内容。\")    verdication_reasoning: str = Field(description=\"这是判断陈述是否可以由上下文推断时的思考内容\")    verdication_result: int = Field(description=\"陈述可信度，如果可以根据上下文直接推断该出陈述，则为1，否则为0。\")class Statement_Verdication_List(BaseModel):    statement_verdications: List[Statement_Verdication] = Field(description=\"这是一组内容陈述的是否可由上下文推断出的结果。\")class Faithfulness(Runnable):    \"\"\"    评估回答的忠实度，确保回答中的信息能够由提供的上下文支持。    该类支持同步和异步评估。    \"\"\"        def __init__(self, llm: ChatOpenAI):        self.llm = llm                self.statement_output_parser = PydanticOutputParser(pydantic_object=Sentence_And_Statement_List)        self.verdict_statement_parser = PydanticOutputParser(pydantic_object=Statement_Verdication_List)                create_statement_prompt = (            \"给定一个包含若干句子的文本，分析每个句子并将每个句子分解为一个或多个原子化的完全可理解的内容陈述，同时确保每个语句中不使用代词，且每个对象都需要具体，在其所在陈述中可理解。\\n\"            \"输出格式：\\n{output_format_instructions}\\n\\n\"            \"[文本-开始]：\\n{context}\\n[文本-结束]\\n\"        )        self.create_statement_prompt = PromptTemplate(            template=create_statement_prompt,             partial_variables={\"output_format_instructions\": self.statement_output_parser.get_format_instructions()}        )        self.create_statement_chain = self.create_statement_prompt | self.llm | self.statement_output_parser        verdict_statement_prompt = (            \"你的任务是根据给定的上下文判断一系列内容陈述的可信度。对于每个陈述，如果可以根据上下文直接推断该出陈述，则返回1，否则返回0。\\n\"            \"输出格式：\\n{output_format_instructions}\\n\\n\"            \"[上下文-开始]：\\n{context}\\n[上下文-结束]\\n\"            \"[内容陈述-开始]：\\n{statements}\\n[内容陈述-结束]\"        )        self.verdict_statement_prompt = PromptTemplate(            template=verdict_statement_prompt,             partial_variables={\"output_format_instructions\": self.verdict_statement_parser.get_format_instructions()}        )        self.verdict_statement_chain = self.verdict_statement_prompt | self.llm | self.verdict_statement_parser            def create_statements(self, context: str):        statement_list = self.create_statement_chain.invoke({\"context\": context})        return sum([item.statements for item in statement_list.sentence_and_statements], [])        def verdict_statements(self, context: str, statements: List[Statement]):        verdict_result = self.verdict_statement_chain.invoke({            \"context\": context,            \"statements\": '\\n'.join([f'{i+1}. {item.description}' for i, item in enumerate(statements)])        })        return verdict_result.statement_verdications        def compute_score(self, verdicts):        num_faithful_statements = sum([item.verdication_result for item in verdicts])        num_statements = len(verdicts)        return num_faithful_statements / num_statements if num_statements else np.nan    def invoke(self, inputs: Dict[str, List[str]]) -&gt; float:        \"\"\"        评估回答的忠实度。        :param inputs: 包含 `answer` (回答) 和 `document_list` (上下文列表) 的字典。        :return: 计算的忠实度得分。        \"\"\"        answer = inputs[\"answer\"]        document_list = inputs[\"document_list\"]                statements = self.create_statements(answer)        context = '\\n'.join(document_list)        verdicts = self.verdict_statements(context, statements)        return self.compute_score(verdicts)        async def ainvoke(self, inputs: Dict[str, List[str]]) -&gt; float:        \"\"\"        异步评估回答的忠实度。        :param inputs: 包含 `answer` (回答) 和 `document_list` (上下文列表) 的字典。        :return: 计算的忠实度得分。        \"\"\"        answer = inputs[\"answer\"]        document_list = inputs[\"document_list\"]                statements = await asyncio.to_thread(self.create_statements, answer)        context = '\\n'.join(document_list)        verdicts = await asyncio.to_thread(self.verdict_statements, context, statements)        return self.compute_score(verdicts)    \n\n\nllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.5,)tool = Faithfulness(llm)inputs = {\t\"answer\":answer,\t\"document_list\":docs,}score = tool.invoke(inputs)print(score)","categories":["RAGas"],"tags":["RAG 评估"]},{"title":"RAGas 回答相关性 Answer Relevance","url":"/2025/05/04/RAGas-langchain-series/2.Answer_Relevance/","content":"回答相关性是指生成的答案应该针对所提出的实际问题进行作答。以下从正面和反面分别给出一些例子：\n\n正面例子：\n问题为“苹果有哪些营养价值？”，\n答案“苹果富含维生素C、纤维素和抗氧化物质，维生素C有助于增强免疫力，纤维素能促进肠道蠕动，抗氧化物质有利于身体健康”。该\n答案直接针对问题，全面阐述了苹果的营养价值，无多余或不相关内容，回答相关性高。\n\n\n反面例子：\n同样问题“苹果有哪些营养价值？”\n答案是“苹果在很多地方都有种植，吃起来很甜，有些人喜欢把它做成苹果派。不过，它对健康有一定好处”。\n此答案没有直接阐述苹果的营养价值，只是提及种植地、口感和食用方式等不相关信息，且对营养价值的表述模糊，没有有效回答问题，回答相关性低。\n\n\n\n如果答案能以恰当的方式直接回答问题，我们就认为该答案具有相关性。需要特别指出的是，我们对答案相关性的评估并不考虑其真实性，但是会对不完整或包含冗余信息的答案进行扣分。为了评估答案相关性，对于给定的答案，我们促使大语言模型（LLM）基于生成个潜在问题，具体如下：\n根据给定答案生成一个问题。答案：[答案]\n\n然后，我们使用 embedding 模型获取所有问题的嵌入向量。对于每个，我们计算它与原始问题之间的相似度 ，即相应嵌入向量之间的余弦相似度。问题的答案相关性得分的计算方式如下：\n\n这个指标用于评估生成的答案与初始问题或指令的契合程度。 \n这是测试数据\nquery = \"美国最高法院关于堕胎的裁决对全球有什么影响？\"answer = \"美国最高法院关于堕胎的裁决具有重要的全球影响。该裁决导致在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。那些州的母婴健康支持也较弱，母亲死亡率较高，儿童贫困率也较高。此外，裁决的影响超出了国界，由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。裁决还妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。此外，该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。\"docs = [\t\"- 2022年，美国最高法院作出裁决，推翻了50年的判例法，取消了宪法堕胎权。\\n- 这一裁决产生了巨大影响：三分之一的生育年龄女性和女孩现在生活在堕胎服务几乎完全无法获得的州。\\n- 这些堕胎法律最为严格的州，母婴健康支持最为薄弱，母亲死亡率较高，儿童贫困率较高。\\n- 美国最高法院的裁决还通过美国在全球的地缘政治和文化影响力，超越国界产生了影响。\\n- 全球的SRR组织和活动家对这一裁决可能为其他国家的反堕胎立法和政策攻击铺路表示担忧。\\n- 观察者还注意到该裁决对某些非洲国家的进步法律改革产生了影响，导致堕胎指导方针的 adoption 和执行停滞不前。\\n- 该裁决在国际政策领域产生了寒蝉效应，助长了反堕胎的国家和非国家行为体破坏人权保护的势头。\",\t\"美国最高法院的堕胎裁决不仅在国内引发了激烈的辩论和讨论，也在全球范围内引发了广泛关注。许多国家将美国视为法律和社会问题的领导者，因此这一裁决可能会影响其他国家对堕胎的政策和态度。\",\t\"这一裁决还可能对国际组织和非政府组织（NGO）产生影响，尤其是那些致力于生育权和妇女健康问题的团体。根据裁决的结果，可能会出现资金、倡导工作和与美国同行的合作发生变化，进而在全球范围内引发生育正义斗争的连锁反应。\"]\n\n实现介绍指标原理：通过大语言模型（LLM）根据给定答案生成多个潜在问题，将这些问题与原始问题进行相似度计算，利用相似度均值来衡量答案和原始问题的契合程度，进而评估回答相关性。具体计算方式为，其中是答案相关性得分，是生成的潜在问题数量，是原始问题与生成的潜在问题的相似度。\n背景原因：在评估RAG系统时，需要全面考量多个维度，回答相关性是其中重要的一环。传统评估方式存在局限性，难以准确衡量生成答案与问题的匹配程度。该指标聚焦答案是否直接、恰当回应问题，且不考虑答案真实性，仅针对答案不完整或含冗余信息的情况进行扣分，以此更精准地评估RAG系统生成答案的质量。\n实现流程:  \n\n生成潜在问题：将给定答案输入LLM，利用“Generate a question for the given answer. answer: [answer]”的提示，让LLM基于答案生成个潜在问题。\n获取问题嵌入向量：借助OpenAI API的text - embedding - ada - 002模型，获取原始问题和生成的个潜在问题的嵌入向量。\n计算相似度并得出得分：计算每个潜在问题与原始问题嵌入向量之间的余弦相似度，最后按照公式计算出答案相关性得分 ，完成对回答相关性的评估。\n\n生成潜在问题使用chain异步生成伪问题\nimport asyncioimport numpy as npfrom typing import Listfrom langchain_core.documents import Documentfrom langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIEmbeddingsfrom sklearn.metrics.pairwise import cosine_similarityfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Field'''输出格式形如：{\t\"sentence_and_statements\": }'''class Generated_Query(BaseModel):\tquery: str = Field(description=\"生成的问题内容。\")\tanswer_sufficiency: str = Field(description='对于生成的问题, 分析回答是否是清晰地回答。')\tanswer_sufficiency_result: int = Field(description='如果回答清晰地回答生成的问题，为1，否则为0。')query_parser = PydanticOutputParser(pydantic_object=Generated_Query)llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.5,)generate_query_prompt = (\t\"为给定的回答生成一个问题，并分析判断回答是否是清晰地回答。\"\t\"如果回答清晰地回答生成的问题，为1，否则为0。\"\t\"清晰的回答是直接回答问题、不带有歧义和模糊处理的回答；反之，含糊的的回答是指回避、含糊或模棱两可的回答。\\n\"\t\"输出格式：\\n{output_format_instructions}\\n\\n\"\t\"[回答-开始]\\n{answer}\\n[回答-结束]\\n\")generate_query_prompt = PromptTemplate(\ttemplate=generate_query_prompt, \tpartial_variables={\t\t\"output_format_instructions\":query_parser.get_format_instructions()\t})generate_query_chain = generate_query_prompt | llm | query_parser\n\n\ngenerate_query_prompt.pretty_print()\n\n为给定的回答生成一个问题，并分析判断回答是否是清晰地回答。如果回答清晰地回答生成的问题，为1，否则为0。清晰的回答是直接回答问题、不带有歧义和模糊处理的回答；反之，含糊的的回答是指回避、含糊或模棱两可的回答。\n输出格式：\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n{\"properties\": {\"query\": {\"description\": \"生成的问题内容。\", \"title\": \"Query\", \"type\": \"string\"}, \"answer_sufficiency\": {\"description\": \"对于生成的问题, 分析回答是否是清晰地回答。\", \"title\": \"Answer Sufficiency\", \"type\": \"string\"}, \"answer_sufficiency_result\": {\"description\": \"如果回答清晰地回答生成的问题，为1，否则为0。\", \"title\": \"Answer Sufficiency Result\", \"type\": \"integer\"}}, \"required\": [\"query\", \"answer_sufficiency\", \"answer_sufficiency_result\"]}\n\n[回答-开始]\n[33;1m[1;3m{answer}[0m\n[回答-结束]\n\nnum_gen = 10tasks = [    generate_query_chain.ainvoke({'answer':answer})    for i in range(num_gen)]generated_querys = await asyncio.gather(*tasks)\n\n\ngenerated_querys\n\n\n\n\n[Generated_Query(query='美国最高法院关于堕胎的裁决对哪些方面产生了影响？', answer_sufficiency='回答详细描述了美国最高法院关于堕胎的裁决对美国国内各州、母婴健康、儿童贫困以及全球地缘政治和文化影响力等方面的影响。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和国际的影响是什么？', answer_sufficiency='回答详细讨论了美国最高法院关于堕胎裁决的国内影响，包括对生育年龄女性和女孩堕胎服务的限制、母婴健康状况以及儿童贫困率的影响。同时，回答也提到了裁决的国际影响，包括对其他国家反堕胎立法和政策的影响，以及在国际政策领域对人权保护的潜在削弱。回答内容丰富且具体，直接回应了问题。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和国际产生了哪些影响？', answer_sufficiency='回答详细描述了美国最高法院关于堕胎的裁决对美国国内的影响，包括堕胎服务的限制、母婴健康状况以及儿童贫困率的变化，同时也提到了这一裁决对国际社会的影响，例如对其他国家立法的影响和在国际政策领域的寒蝉效应。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和国际产生了哪些影响？', answer_sufficiency='回答详细描述了美国最高法院关于堕胎裁决对美国国内的影响，包括对生育年龄女性和女孩堕胎服务获取的影响，以及对母婴健康的负面影响。同时，回答也提到了裁决的国际影响，包括对其他国家制定反堕胎立法和政策的影响，以及对国际政策领域中人权保护的潜在削弱。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和国际产生了哪些影响？', answer_sufficiency='回答详细地描述了美国最高法院关于堕胎的裁决对美国国内和国际的影响，包括对生育年龄女性和女孩堕胎服务的限制、母婴健康状况、地缘政治和文化影响力以及国际政策领域的影响。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和全球产生了哪些影响？', answer_sufficiency='回答详细说明了美国最高法院关于堕胎的裁决对美国国内的影响，如导致堕胎访问受限州的女性无法获得堕胎服务，母婴健康状况恶化等，同时也指出了裁决的全球影响，包括对其他国家的立法和国际政策领域的影响。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和全球产生了哪些影响？', answer_sufficiency='回答详细地描述了美国最高法院关于堕胎的裁决对美国国内的影响，包括对生育年龄女性和女孩获得堕胎服务的限制，以及对母婴健康、儿童贫困率的影响。此外，回答还提到了裁决的全球影响，包括对其他国家立法和政策的影响，以及在国际政策领域造成的寒蝉效应。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和国际产生了哪些具体影响？', answer_sufficiency='回答详细地讨论了美国最高法院关于堕胎的裁决对美国国内的影响，包括对生育年龄女性和女孩堕胎服务的限制，以及对母婴健康、儿童贫困的影响。此外，回答还讨论了裁决的国际影响，包括对其他国家反堕胎立法的影响，以及在国际政策领域对人权保护的潜在负面影响。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和国际有什么影响？', answer_sufficiency='回答详细地讨论了该裁决对美国国内的影响，包括对生育年龄女性和女孩堕胎服务的限制、母婴健康状况以及儿童贫困率的影响。此外，回答还探讨了裁决的国际影响，包括其对全球组织和活动家的影响，以及对某些非洲国家法律改革的负面影响。', answer_sufficiency_result=1),\n Generated_Query(query='美国最高法院关于堕胎的裁决对美国国内和全球产生了哪些影响？', answer_sufficiency='回答详细地描述了美国最高法院关于堕胎的裁决对美国国内的影响，包括对生育年龄女性和女孩堕胎服务的限制、母婴健康状况以及儿童贫困率的影响。同时，回答还提到了裁决的全球影响，包括地缘政治和文化影响力、对其他国家反堕胎立法的潜在影响以及在国际政策领域造成的寒蝉效应。', answer_sufficiency_result=1)]\n\n生成数据如下:  \n[\tGenerated_Query(query='美国最高法院关于堕胎的裁决对哪些方面产生了影响？', answer_sufficiency='回答详细描述了美国最高法院关于堕胎的裁决对美国国内各州、母婴健康、儿童贫困以及全球地缘政治和文化影响力等方面的影响。', answer_sufficiency_result=1), \tGenerated_Query(query='美国最高法院关于堕胎的裁决对美国国内和国际的影响是什么？', answer_sufficiency='回答详细讨论了美国最高法院关于堕胎裁决的国内影响，包括对生育年龄女性和女孩堕胎服务的限制、母婴健康状况以及儿童贫困率的影响。同时，回答也提到了裁决的国际影响，包括对其他国家反堕胎立法和政策的影响，以及在国际政策领域对人权保护的潜在削弱。回答内容丰富且具体，直接回应了问题。', answer_sufficiency_result=1), \tGenerated_Query(query='美国最高法院关于堕胎的裁决对美国国内和国际产生了哪些影响？', answer_sufficiency='回答详细描述了美国最高法院关于堕胎的裁决对美国国内的影响，包括堕胎服务的限制、母婴健康状况以及儿童贫困率的变化，同时也提到了这一裁决对国际社会的影响，例如对其他国家立法的影响和在国际政策领域的寒蝉效应。', answer_sufficiency_result=1)]\n\n\nquery 字段将用来计算相似度\nanswer_sufficiency_result 字段用于评判回答有效性\n\n获取问题嵌入向量基于 OpenAIEmbeddings 使用 api 获取 embedding\n# Set desired modelopenai_embedding = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)\n\n\nvectors = openai_embedding.embed_documents([query]+[item.query for item in generated_querys])\n\n计算相似度并得出得分real_query_vec = vectors[0]gen_query_vec = vectors[1:]similarity = cosine_similarity([real_query_vec], gen_query_vec)\n\n\nsimilarity\n\n\n\n\narray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n\n相似度有点高, 因为query也是之前用大模型批量生成用来训练embedding模型的\nanswer_sufficiencies = [item.answer_sufficiency_result for item in generated_querys]answer_sufficiencies = np.array(answer_sufficiencies)scores = similarity * answer_sufficienciesaveaged_score = np.mean(scores)print(aveaged_score)\n\n1.000000000000001\n\n完整代码import asyncioimport nest_asyncio# 应用 nest_asyncio 以支持 Jupyter 笔记本中的异步操作nest_asyncio.apply()\n\n\n\"\"\"# 回答相关性 Answer Relevance## 指标原理  通过大语言模型（LLM）根据给定答案生成多个潜在问题，将这些问题与原始问题进行相似度计算，利用相似度均值来衡量答案和原始问题的契合程度，进而评估回答相关性。具体计算方式为$AR=\\frac{1}{n}\\sum_{i = 1}^{n}sim(q,q_{i})$，其中$AR$是答案相关性得分，$n$是生成的潜在问题数量，$sim(q,q_{i})$是原始问题$q$与生成的潜在问题$q_{i}$的相似度。## 背景原因  在评估RAG系统时，需要全面考量多个维度，回答相关性是其中重要的一环。传统评估方式存在局限性，难以准确衡量生成答案与问题的匹配程度。该指标聚焦答案是否直接、恰当回应问题，且不考虑答案真实性，仅针对答案不完整或含冗余信息的情况进行扣分，以此更精准地评估RAG系统生成答案的质量。## 实现流程  1. **生成潜在问题**：将给定答案$a_{s}(q)$输入LLM，利用“Generate a question for the given answer. answer: [answer]”的提示，让LLM基于答案生成$n$个潜在问题$q_{i}$。2. **获取问题嵌入向量**：借助OpenAI API的text - embedding - ada - 002模型，获取原始问题$q$和生成的$n$个潜在问题$q_{i}$的嵌入向量。3. **计算相似度并得出得分**：计算每个潜在问题$q_{i}$与原始问题$q$嵌入向量之间的余弦相似度$sim(q,q_{i})$，最后按照公式$AR=\\frac{1}{n}\\sum_{i = 1}^{n}sim(q,q_{i})$计算出答案相关性得分$AR$ ，完成对回答相关性的评估。 \"\"\"import asyncioimport numpy as npfrom typing import List, Dictfrom langchain.schema.runnable import Runnablefrom langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIEmbeddingsfrom sklearn.metrics.pairwise import cosine_similarityfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Fieldclass Generated_Query(BaseModel):\tquery: str = Field(description=\"生成的问题内容。\")\tanswer_sufficiency: str = Field(description='对于生成的问题, 分析回答是否是清晰地回答。')\tanswer_sufficiency_result: int = Field(description='如果回答清晰地回答生成的问题，为1，否则为0。')class Answer_Relevance(Runnable):\t\"\"\"\t评估回答和问题的相关程度, 好的回答能够推导出对应的问题\t\"\"\"\tdef __init__(self, llm: ChatOpenAI, embedding_model: OpenAIEmbeddings):\t\tself.llm = llm\t\tself.embedding_model = embedding_model\t\tself.query_parser = PydanticOutputParser(pydantic_object=Generated_Query)\t\tgenerate_query_prompt = (\t\t\t\"为给定的回答生成一个问题，并分析判断回答是否是清晰地回答。\"\t\t\t\"如果回答清晰地回答生成的问题，为1，否则为0。\"\t\t\t\"清晰的回答是直接回答问题、不带有歧义和模糊处理的回答；反之，含糊的的回答是指回避、含糊或模棱两可的回答。\\n\"\t\t\t\"输出格式：\\n{output_format_instructions}\\n\\n\"\t\t\t\"[回答-开始]\\n{answer}\\n[回答-结束]\\n\"\t\t)\t\tself.generate_query_prompt = PromptTemplate(\t\t\ttemplate=generate_query_prompt,\t\t\tpartial_variables={\t\t\t\t\"output_format_instructions\": self.query_parser.get_format_instructions()\t\t\t}\t\t)\t\tself.generate_query_chain = self.generate_query_prompt | self.llm | self.query_parser\tdef generate_pseudo_query(self, answer: str, num_of_gen: int = 10):\t\t\"\"\"\t\t异步生成多个伪问题\t\t:param answer: 回答文本\t\t:param num_of_gen: 生成问题的数量\t\t:return: 伪问题列表\t\t\"\"\"\t\ttasks = [\t\t\tself.generate_query_chain.ainvoke({'answer': answer})\t\t\tfor _ in range(num_of_gen)\t\t]\t\tloop = asyncio.get_event_loop()\t\tpseudo_querys = loop.run_until_complete(asyncio.gather(*tasks))\t\treturn pseudo_querys\tdef compute_score(self, real_query: str, pseudo_querys: List[Generated_Query]):\t\t\"\"\"\t\t计算回答和问题的相关性得分\t\t:param real_query: 实际问题\t\t:param pseudo_querys: 伪问题列表\t\t:return: 计算的相关性得分\t\t\"\"\"\t\tdata = [real_query] + [item.query for item in pseudo_querys]\t\tvectors = self.embedding_model.embed_documents(data)\t\t# 分别提取实际问题和伪问题的向量\t\treal_query_vec = vectors[0]\t\tpseudo_query_vec = vectors[1:]\t\t# 计算余弦相似度\t\tsimilarity = cosine_similarity([real_query_vec], pseudo_query_vec)\t\t# 获取伪问题的回答充分性结果\t\tanswer_sufficiencies = [item.answer_sufficiency_result for item in pseudo_querys]\t\tanswer_sufficiencies = np.array(answer_sufficiencies)\t\t# 计算最终的相关性得分\t\tscores = similarity.flatten() * answer_sufficiencies\t\taveaged_score = np.mean(scores)\t\treturn aveaged_score\tdef invoke(self, inputs: Dict[str, List[str]], num_of_gen: int=10) -&gt; float:\t\t\"\"\"\t\t异步评估回答与问题的相关度\t\t:param inputs: 包含问题 `query` 和回答 `answer` 的字典\t\t:param num_of_gen: 生成伪问题的数量\t\t:return: 计算的相关性得分\t\t\"\"\"\t\t\t\tquery = inputs[\"query\"]\t\tanswer = inputs[\"answer\"]\t\tpseudo_querys = self.generate_pseudo_query(answer, num_of_gen)\t\tscore = self.compute_score(query, pseudo_querys)\t\treturn score\tasync def ainvoke(self, inputs: Dict[str, List[str]], num_of_gen: int = 10) -&gt; float:\t\t\"\"\"\t\t异步评估回答与问题的相关度\t\t:param inputs: 包含问题 `query` 和回答 `answer` 的字典\t\t:param num_of_gen: 生成伪问题的数量\t\t:return: 计算的相关性得分\t\t\"\"\"\t\tquery = inputs[\"query\"]\t\tanswer = inputs[\"answer\"]\t\t# 生成伪问题\t\tpseudo_querys = await asyncio.to_thread(self.generate_pseudo_query, answer, num_of_gen)\t\t# 计算相关性得分\t\tscore = await asyncio.to_thread(self.compute_score, query, pseudo_querys)\t\treturn score\n\n\nllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.5,)openai_embedding = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',)tool = Answer_Relevance(llm, openai_embedding)inputs = {\t\"query\":\"美国最高法院关于堕胎的裁决对全球有什么影响？\",\t\"answer\":\"美国最高法院关于堕胎的裁决具有重要的全球影响。该裁决导致在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。那些州的母婴健康支持也较弱，母亲死亡率较高，儿童贫困率也较高。此外，裁决的影响超出了国界，由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。裁决还妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。此外，该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。\",}score = tool.invoke(inputs)print(score)","categories":["RAGas"],"tags":["RAG 评估"]},{"title":"RAGas 有效文档准确率 Useful Context Precision","url":"/2025/05/04/RAGas-langchain-series/3.Useful_Context_Precision/","content":"在RAG系统评估场景下，“计算RAG返回有效文档的平均top-k命中率” 旨在衡量系统检索相关信息的能力，在只有一次查询的情况下，具体解释如下：\n\n背后直觉：在实际运用RAG系统进行单次查询时，用户希望能高效获取有效文档。如果系统能在返回结果的前几个位置（如top-1、top-2）就展示有效文档，说明其检索精准度高、效率好；若有效文档位置靠后，获取有效信息的效率就会降低。计算平均top-k命中率，就是为了综合评估系统在不同检索深度下找到有效文档的能力，帮助我们了解系统此次查询中检索有效信息的表现，数值越高代表系统表现越好。\n实现方式\n数据准备：假设RAG系统针对此次查询返回5个文档。通过大模型分析每个文档对得出答案是否有帮助，为每个文档赋予命中结果（），取值为0或1 ，1表示该文档有助于得出答案，0表示无助于得出答案。\n计算top-k命中率\ntop-1命中率：，即第一个文档的命中结果就是top-1命中率。若，则top-1命中率为1；若，则top-1命中率为0。\ntop-2命中率：，即前两个文档命中结果的平均值。比如， ，则 。\n…\n\n\n计算平均top-k命中率：计算得到的top-1~5命中率的加权平均值。\n\n\n\nquery = \"美国最高法院关于堕胎的裁决对全球有什么影响？\"answer = \"美国最高法院关于堕胎的裁决具有重要的全球影响。该裁决导致在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。那些州的母婴健康支持也较弱，母亲死亡率较高，儿童贫困率也较高。此外，裁决的影响超出了国界，由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。裁决还妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。此外，该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。\"docs = [\t\"- 2022年，美国最高法院作出裁决，推翻了50年的判例法，取消了宪法堕胎权。\\n- 这一裁决产生了巨大影响：三分之一的生育年龄女性和女孩现在生活在堕胎服务几乎完全无法获得的州。\\n- 这些堕胎法律最为严格的州，母婴健康支持最为薄弱，母亲死亡率较高，儿童贫困率较高。\\n- 美国最高法院的裁决还通过美国在全球的地缘政治和文化影响力，超越国界产生了影响。\\n- 全球的SRR组织和活动家对这一裁决可能为其他国家的反堕胎立法和政策攻击铺路表示担忧。\\n- 观察者还注意到该裁决对某些非洲国家的进步法律改革产生了影响，导致堕胎指导方针的 adoption 和执行停滞不前。\\n- 该裁决在国际政策领域产生了寒蝉效应，助长了反堕胎的国家和非国家行为体破坏人权保护的势头。\",\t\"美国最高法院的堕胎裁决不仅在国内引发了激烈的辩论和讨论，也在全球范围内引发了广泛关注。许多国家将美国视为法律和社会问题的领导者，因此这一裁决可能会影响其他国家对堕胎的政策和态度。\",\t\"这一裁决还可能对国际组织和非政府组织（NGO）产生影响，尤其是那些致力于生育权和妇女健康问题的团体。根据裁决的结果，可能会出现资金、倡导工作和与美国同行的合作发生变化，进而在全球范围内引发生育正义斗争的连锁反应。\"]\n\n代码实现判断文档有效性import asyncioimport numpy as npfrom typing import Listfrom langchain_core.documents import Documentfrom langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIEmbeddingsfrom sklearn.metrics.pairwise import cosine_similarityfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Fieldclass Usefulness_Verdication(BaseModel):\treasoning: str = Field(description=\"思考上下文是否有助于得出给定的回答。\")\tresult: int = Field(description=\"判决结果，如果上下文有助于得出给定的回答，则结果为1，否则为0。\")verdication_parser = PydanticOutputParser(pydantic_object=Usefulness_Verdication)llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-14B-Instruct',\ttemperature=0.5,)verdict_prompt = (\t\"给出问题、回答和上下文，验证上下文是否有助于得出给定的回答。\"\t\"你需要先分析内容作为判决理由，如果有用，则判决结果为1，如果没用，则判决结果为0。\\n\\n\"\t\"输出格式：\\n{output_format_instructions}\\n\"\t\"[问题-开始]\\n{query}\\n[问题-结束]\\n\\n\"\t\"[回答-开始]\\n{answer}\\n[回答-结束]\\n\\n\"\t\"[上下文-开始]\\n{context}\\n[上下文-结束]\\n\\n\")verdict_prompt = PromptTemplate(\ttemplate=verdict_prompt, \tpartial_variables={\t\t\"output_format_instructions\":verdication_parser.get_format_instructions()\t})verdict_usefulness_chain = verdict_prompt | llm | verdication_parserverdict_prompt.pretty_print()\n\n给出问题、回答和上下文，验证上下文是否有助于得出给定的回答。你需要先分析内容作为判决理由，如果有用，则判决结果为1，如果没用，则判决结果为0。\n\n输出格式：\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n{\"properties\": {\"reasoning\": {\"description\": \"思考上下文是否有助于得出给定的回答。\", \"title\": \"Reasoning\", \"type\": \"string\"}, \"result\": {\"description\": \"判决结果，如果上下文有助于得出给定的回答，则结果为1，否则为0。\", \"title\": \"Result\", \"type\": \"integer\"}}, \"required\": [\"reasoning\", \"result\"]}\n[问题-开始]\n[33;1m[1;3m{query}[0m\n[问题-结束]\n\n[回答-开始]\n[33;1m[1;3m{answer}[0m\n[回答-结束]\n\n[上下文-开始]\n[33;1m[1;3m{context}[0m\n[上下文-结束]\n\n​    \ntasks = [\tverdict_usefulness_chain.ainvoke(\t\t{\t\t\t\"query\":query, \t\t\t'answer':answer, \t\t\t\"context\":a_doc,\t\t}\t)\tfor a_doc in docs]verdications = await asyncio.gather(*tasks)verdications\n\n\n\n\n[Usefulness_Verdication(reasoning='上下文提供了美国最高法院关于堕胎裁决的具体信息，包括裁决内容、对美国国内的影响（如女性和女孩无法获得堕胎服务、母婴健康支持薄弱等）、以及对全球的影响（如其他国家可能受到的立法和政策影响、对某些非洲国家法律改革的阻碍、国际政策领域的寒蝉效应等）。这些信息直接支持了回答中提到的裁决对全球的影响，因此上下文对得出给定的回答有帮助。', result=1),\n Usefulness_Verdication(reasoning='上下文提到美国最高法院的堕胎裁决在全球范围内引发了广泛关注，并且许多国家将美国视为法律和社会问题的领导者。这与回答中提到的裁决可能激励其他国家出台反堕胎立法和政策以及裁决在全球范围内的跨国影响是一致的。上下文有助于理解回答中的全球影响。', result=1),\n Usefulness_Verdication(reasoning='上下文补充了回答中的信息，特别是关于裁决如何可能影响国际组织和非政府组织（NGO），尤其是那些致力于生育权和妇女健康问题的团体。上下文提到可能会出现资金、倡导工作和与美国同行的合作发生变化，这进一步解释了裁决的跨国影响。因此，上下文提供了额外的信息，有助于理解裁决对全球的影响。', result=1)]\n\n计算加权平均命中率results = [item.result for item in verdications]final_score = np.nandenominator = sum(results)if denominator &lt;=0:\tfinal_score = 0else:\tnumerator = sum(\t\t[\t\t\t(sum(results[: i + 1]) / (i + 1)) * results[i]\t\t\tfor i in range(len(results))\t\t]\t)\tfinal_score = numerator / denominatorfinal_score\n\n\n\n\n1.0\n\n封装import asyncioimport nest_asyncio# 应用 nest_asyncio 以支持 Jupyter 笔记本中的异步操作nest_asyncio.apply()\n\n\nimport asyncioimport numpy as npfrom typing import List, Dictfrom langchain_core.documents import Documentfrom langchain.schema.runnable import Runnablefrom langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIEmbeddingsfrom sklearn.metrics.pairwise import cosine_similarityfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Field# 定义用于判断上下文是否有用的判决结果模型class Usefulness_Verdication(BaseModel):\treasoning: str = Field(description=\"思考上下文是否有助于得出给定的回答。\")\tresult: int = Field(description=\"判决结果，如果上下文有助于得出给定的回答，则结果为1，否则为0。\")class Useful_Context_Precision(Runnable):\t\"\"\"\t该类用于评估回答中上下文的相关性和有用性。其核心任务是验证给定上下文是否有助于\t得出回答，并计算有用性判决的平均精度。\t\"\"\"\tdef __init__(self, llm: ChatOpenAI):\t\t\"\"\"\t\t初始化 Useful_Context_Precision 类，设置 LLM 和判决解析器。\t\t:param llm: 用于生成上下文判决的 LLM 模型实例\t\t\"\"\"\t\tself.llm = llm\t\t# 初始化判决结果解析器，基于 Pydantic 模型 Usefulness_Verdication\t\tself.verdication_parser = PydanticOutputParser(pydantic_object=Usefulness_Verdication)\t\t# 定义生成上下文有用性的判决提示模板\t\tverdict_prompt = (\t\t\t\"给出问题、回答和上下文，验证上下文是否有助于得出给定的回答。\\n\"\t\t\t\"你需要先分析内容作为判决理由，如果有用，则判决结果为1，如果没用，则判决结果为0。\\n\\n\"\t\t\t\"输出格式：\\n{output_format_instructions}\\n\"\t\t\t\"[问题-开始]\\n{query}\\n[问题-结束]\\n\\n\"\t\t\t\"[回答-开始]\\n{answer}\\n[回答-结束]\\n\\n\"\t\t\t\"[上下文-开始]\\n{context}\\n[上下文-结束]\\n\\n\"\t\t)\t\t\t\t# 创建 PromptTemplate，插入解析器的输出格式\t\tself.verdict_prompt = PromptTemplate(\t\t\ttemplate=verdict_prompt, \t\t\tpartial_variables={\t\t\t\t\"output_format_instructions\": self.verdication_parser.get_format_instructions()\t\t\t}\t\t)\t\t# 创建 Chain，将模板、LLM 和解析器组合起来\t\tself.verdict_usefulness_chain = self.verdict_prompt | self.llm | self.verdication_parser\tdef verdict_document_usefulness(self, query: str, answer: str, document_list: List[str]) -&gt; List[Usefulness_Verdication]:\t\t\"\"\"\t\t异步判断一组文档中每个文档对给定问题和回答的上下文是否有用。\t\t:param query: 提出的问题\t\t:param answer: 对问题的回答\t\t:param document_list: 文档列表，包含与问题相关的上下文\t\t:return: 返回每个文档的上下文有用性的判决结果列表\t\t\"\"\"\t\t# 异步地为每个文档生成判决\t\ttasks = [\t\t\tself.verdict_usefulness_chain.ainvoke({\t\t\t\t\"query\": query,\t\t\t\t\"answer\": answer,\t\t\t\t\"context\": a_doc,\t\t\t})\t\t\tfor a_doc in document_list\t\t]\t\t# 使用事件循环运行所有异步任务\t\tloop = asyncio.get_event_loop()\t\tverdications = loop.run_until_complete(asyncio.gather(*tasks))\t\treturn verdications\tdef calculate_average_precision(self, results: List[int]) -&gt; float:\t\t\"\"\"\t\t计算并返回平均精度（AP）得分。\t\t:param results: 判决结果的列表，其中 1 表示有用，0 表示无用\t\t:return: 平均精度得分\t\t\"\"\"\t\tfinal_score = np.nan\t\tdenominator = sum(results)\t\tif denominator &lt;= 0:\t\t\t# 如果没有有用的判决，则得分为 0\t\t\tfinal_score = 0\t\telse:\t\t\t# 计算加权平均精度\t\t\tnumerator = sum([\t\t\t\t(sum(results[:i + 1]) / (i + 1)) * results[i]\t\t\t\tfor i in range(len(results))\t\t\t])\t\t\tfinal_score = numerator / denominator\t\treturn final_score\tdef invoke(self, inputs: Dict[str, List[str]]) -&gt; float:\t\t\"\"\"\t\t执行用于计算有用上下文精度的评估。\t\t:param inputs: 输入字典，包含以下字段：\t\t\t- \"query\": 提出的问题\t\t\t- \"answer\": 对问题的回答\t\t\t- \"document_list\": 文档列表\t\t:return: 计算得到的精度得分\t\t\"\"\"\t\tquery = inputs[\"query\"]\t\tanswer = inputs[\"answer\"]\t\tdocument_list = inputs[\"document_list\"]\t\t# 获取文档的有用性判决\t\tverdications = self.verdict_document_usefulness(query, answer, document_list)\t\t# 提取判决结果（1表示有用，0表示无用）\t\tresults = [item.result for item in verdications]\t\t# 计算并返回平均精度得分\t\tscore = self.calculate_average_precision(results)\t\treturn score\n\n\nllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-14B-Instruct',\ttemperature=0.5,)tool = Useful_Context_Precision(llm)inputs = {\t\"query\":query,\t\"answer\":answer,\t\"document_list\":docs,}tool.invoke(inputs)\n\n\n\n\n1.0\n\n","categories":["RAGas"],"tags":["RAG 评估"]},{"title":"RAGas 上下文相关性 Context Relevance","url":"/2025/05/04/RAGas-langchain-series/4.Context_Relevance/","content":"原理：Context Relevance 指标用于评估一个回答的句子是否受到提供的上下文支持，以及上下文是否受到回答的支持。其核心思想是：\n\n拆分回答：将回答分解为独立的句子。\n判定支持度：对于每个句子，判断它是否可以在上下文中找到支持。\n计算支持度精度：通过统计支持的句子占比，衡量回答对上下文的依赖性。\n\n目的：  \n\n确保模型的回答是基于给定的上下文，而非凭空生成。\n评估回答对上下文的利用率，提高答案的可靠性。\n检测回答中的不可信或无依据的内容。\n\nquery = \"美国最高法院关于堕胎的裁决对全球有什么影响？\"answer = \"美国最高法院关于堕胎的裁决具有重要的全球影响。该裁决导致在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。那些州的母婴健康支持也较弱，母亲死亡率较高，儿童贫困率也较高。此外，裁决的影响超出了国界，由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。裁决还妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。此外，该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。\"docs = [\t\"- 2022年，美国最高法院作出裁决，推翻了50年的判例法，取消了宪法堕胎权。\\n- 这一裁决产生了巨大影响：三分之一的生育年龄女性和女孩现在生活在堕胎服务几乎完全无法获得的州。\\n- 这些堕胎法律最为严格的州，母婴健康支持最为薄弱，母亲死亡率较高，儿童贫困率较高。\\n- 美国最高法院的裁决还通过美国在全球的地缘政治和文化影响力，超越国界产生了影响。\\n- 全球的SRR组织和活动家对这一裁决可能为其他国家的反堕胎立法和政策攻击铺路表示担忧。\\n- 观察者还注意到该裁决对某些非洲国家的进步法律改革产生了影响，导致堕胎指导方针的 adoption 和执行停滞不前。\\n- 该裁决在国际政策领域产生了寒蝉效应，助长了反堕胎的国家和非国家行为体破坏人权保护的势头。\",\t\"美国最高法院的堕胎裁决不仅在国内引发了激烈的辩论和讨论，也在全球范围内引发了广泛关注。许多国家将美国视为法律和社会问题的领导者，因此这一裁决可能会影响其他国家对堕胎的政策和态度。\",\t\"这一裁决还可能对国际组织和非政府组织（NGO）产生影响，尤其是那些致力于生育权和妇女健康问题的团体。根据裁决的结果，可能会出现资金、倡导工作和与美国同行的合作发生变化，进而在全球范围内引发生育正义斗争的连锁反应。\"]\n\n代码实现实现流程：  \n\n输入： 回答 answer，以及文档 document_list。\n支持度判定： 使用 Sentence_Support_Verdication 模型逐句分析回答的句子是否在上下文中得到支持。\n计算精度： 计算被支持的句子占比，以 answer_supported_precision 和 context_supported_precision 进行衡量。\n\n从文本A中抽取句子，再在文本B中验证import asyncioimport numpy as npfrom typing import Listfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Field'''输出格式形如：{\t\"sentence_and_statements\": }'''class Sentence_Support_Verdication(BaseModel):\tsentence: str = Field(description=\"回答的某个句子的原始内容。\")\tsentence_supported_reasoning: str = Field(description='分析该句子是否能得到上下文中任意一句话的支持。')\tsentence_supported_verdication: int = Field(description='如果该句子能得到事实陈述中任意一句话的支持，为1，否则为0。')class Sentence_Support_Verdication_List(BaseModel):\tverdications: List[Sentence_Support_Verdication] = Field(description=\"一组句子和对应的是否被上下文支持的判断\")verdication_parser = PydanticOutputParser(pydantic_object=Sentence_Support_Verdication_List)llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-14B-Instruct',\ttemperature=0.5,)anwer_supported_verdict_prompt = (\t\"给定一段上下文和一个回答，分析回答的每个语句，并判断该语句是否得到上下文的支持，若得到支持则判断为1，若没有得到支持则判断为0。\\n\"\t\"输出格式：\\n{output_format_instructions}\\n\\n\"\t\"[上下文-开始]\\n{context}\\n[上下文-结束]\\n\\n\"\t\"[回答-开始]\\n{answer}\\n[回答-结束]\\n\\n\")anwer_supported_verdict_prompt = PromptTemplate(\ttemplate=anwer_supported_verdict_prompt, \tpartial_variables={\t\t\"output_format_instructions\":verdication_parser.get_format_instructions()\t})anwer_supported_verdict_chain = anwer_supported_verdict_prompt | llm | verdication_parseranwer_supported_verdict_prompt.pretty_print()\n\n给定一段上下文和一个回答，分析回答的每个语句，并判断该语句是否得到上下文的支持，若得到支持则判断为1，若没有得到支持则判断为0。\n输出格式：\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n{\"$defs\": {\"Sentence_Support_Verdication\": {\"properties\": {\"sentence\": {\"description\": \"回答的某个句子的原始内容。\", \"title\": \"Sentence\", \"type\": \"string\"}, \"sentence_supported_reasoning\": {\"description\": \"分析该句子是否能得到上下文中任意一句话的支持。\", \"title\": \"Sentence Supported Reasoning\", \"type\": \"string\"}, \"sentence_supported_verdication\": {\"description\": \"如果该句子能得到事实陈述中任意一句话的支持，为1，否则为0。\", \"title\": \"Sentence Supported Verdication\", \"type\": \"integer\"}}, \"required\": [\"sentence\", \"sentence_supported_reasoning\", \"sentence_supported_verdication\"], \"title\": \"Sentence_Support_Verdication\", \"type\": \"object\"}}, \"properties\": {\"verdications\": {\"description\": \"一组句子和对应的是否被上下文支持的判断\", \"items\": {\"$ref\": \"#/$defs/Sentence_Support_Verdication\"}, \"title\": \"Verdications\", \"type\": \"array\"}}, \"required\": [\"verdications\"]}\n\n[上下文-开始]\n[33;1m[1;3m{context}[0m\n[上下文-结束]\n\n[回答-开始]\n[33;1m[1;3m{answer}[0m\n[回答-结束]\n\n​    \n回答对于上下文的相关度result = anwer_supported_verdict_chain.invoke(    {        \"context\":'\\n'.join(docs),        \"answer\":answer,\t})\n\n这是从回答中抽取的每个句子，如果句子有上下文(RAG文档)支持，那它与上下文关联度高/遵守上下文；否则它纯粹是大模型自己无根据生成的，与上下文关联度低。\n好的上下文可以完全/较全地支持回答，那前者的出现率就应该很高。那我们可以视为分类问题，两者分别为T、F，从而得到 precision 等指标。\n[Sentence_Support_Verdication(sentence='美国最高法院关于堕胎的裁决具有重要的全球影响。', sentence_supported_reasoning='上下文中的最后一段提到了裁决不仅在国内引发了激烈的辩论和讨论，也在全球范围内引发了广泛关注，说明了裁决的全球影响。', sentence_supported_verdication=1), Sentence_Support_Verdication(sentence='该裁决导致在堕胎访问受到限制的州，三分之一的生育年龄女性和女孩无法获得堕胎服务。', sentence_supported_reasoning='上下文中的第二段明确提到三分之一的生育年龄女性和女孩现在生活在堕胎服务几乎完全无法获得的州。', sentence_supported_verdication=1), Sentence_Support_Verdication(sentence='那些州的母婴健康支持也较弱，母亲死亡率较高，儿童贫困率也较高。', sentence_supported_reasoning='上下文中的第三段提到了这些堕胎法律最为严格的州，母婴健康支持最为薄弱，母亲死亡率较高，儿童贫困率较高。', sentence_supported_verdication=1), Sentence_Support_Verdication(sentence='此外，裁决的影响超出了国界，由于美国在全球的地缘政治和文化影响力，这一裁决也产生了跨国影响。', sentence_supported_reasoning='上下文中的第四段提到了美国最高法院的裁决还通过美国在全球的地缘政治和文化影响力，超越国界产生了影响。', sentence_supported_verdication=1), Sentence_Support_Verdication(sentence='全球的组织和活动家担心这一裁决可能会激励其他国家出台反堕胎的立法和政策。', sentence_supported_reasoning='上下文中的第五段提到全球的SRR组织和活动家对这一裁决可能为其他国家的反堕胎立法和政策攻击铺路表示担忧。', sentence_supported_verdication=1), Sentence_Support_Verdication(sentence='裁决还妨碍了某些非洲国家的进步法律改革和堕胎指南的实施。', sentence_supported_reasoning='上下文中的第六段提到裁决对某些非洲国家的进步法律改革产生了影响，导致堕胎指导方针的adoption和执行停滞不前。', sentence_supported_verdication=1), Sentence_Support_Verdication(sentence='此外，该裁决在国际政策领域造成了寒蝉效应，使得反堕胎的力量能够削弱人权保护。', sentence_supported_reasoning='上下文中的第七段提到裁决在国际政策领域产生了寒蝉效应，助长了反堕胎的国家和非国家行为体破坏人权保护的势头。', sentence_supported_verdication=1)]\n\n\nverdications_true = []verdications_false = []for item in result.verdications:    if item.sentence_supported_verdication == 1:        verdications_true.append(item)    else:        verdications_false.append(item)        precision = len(verdications_true) / (len(verdications_true)+len(verdications_false))print(precision)\n\n1.0\n\n上下文对于回答的相关度反转上下文和回答的位置，现在从上下文中提取句子，并验证回答是否支持\nresult = anwer_supported_verdict_chain.invoke(    {        \"context\":answer,        \"answer\":'\\n'.join(docs),\t})\n\n\nverdications_true = []verdications_false = []for item in result.verdications:    if item.sentence_supported_verdication == 1:        verdications_true.append(item)    else:        verdications_false.append(item)        precision = len(verdications_true) / (len(verdications_true)+len(verdications_false))print(precision)\n\n0.5454545454545454\n\n反思实际回答只用了上下文的小部分，所以上下文中部分内容不会在回答中体现/未被回答支持。换句话说，RAG文档中有用的内容如果不多，那准确率就低/RAG效果差。\n感觉上，直接把所有RAG文档拼成单一的上下文不大合理，每个文档的准确率都会不一样，但同样准确率的第3个文档和第10个文档是不一样的，随着排序顺序，准确率下降其实也合理，所以更合理的方式是：单独计算每个文档的准确率，然后基于顺序位置做加权平均，更后位置的文档的权重应该更低。\n一个容易混淆的概念是把上下文和回答的内容识别为TP、FP、TN、FN。假设上下文的内容都是P，而回答做分类。  \n\n上下文中提取的是TP、FP，是上下文有的内容分别被回答体现和没体现。  \n回答中提取的是TP、TN，是回答中的内容分别被上下文支持和没支持。\n\n因为上下文和回答的内容数量肯定会不一样，整体数量上TP+FP != TP+TN，而且回答中的单个TP可能是多个TP的组合，所以我倾向于不用混淆矩阵。\n完整代码import asyncioimport numpy as npfrom typing import List, Dictfrom langchain_openai import ChatOpenAIfrom langchain.schema.runnable import Runnablefrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Fieldclass Sentence_Support_Verdication(BaseModel):    \"\"\"    句子支持度判断结果。    \"\"\"    sentence: str = Field(description=\"回答的某个句子的原始内容。\")    sentence_supported_reasoning: str = Field(description='分析该句子是否能得到上下文中任意一句话的支持。')    sentence_supported_verdication: int = Field(description='如果该句子能得到事实陈述中任意一句话的支持，为1，否则为0。')class Sentence_Support_Verdication_List(BaseModel):    \"\"\"    包含多个句子支持度判断的列表。    \"\"\"    verdications: List[Sentence_Support_Verdication] = Field(description=\"一组句子和对应的是否被上下文支持的判断\")class Context_Relevance(Runnable):    \"\"\"    评估回答是否受到上下文支持。    \"\"\"    def __init__(self, llm: ChatOpenAI):        \"\"\"        初始化 Context_Relevance 类。        :param llm: ChatOpenAI 实例，用于 LLM 调用。        \"\"\"        self.llm = llm        self.verdication_parser = PydanticOutputParser(pydantic_object=Sentence_Support_Verdication_List)                answer_supported_verdict_prompt = (            \"给定一段上下文和一个回答，分析回答的每个语句，并判断该语句是否得到上下文的支持，若得到支持则判断为1，若没有得到支持则判断为0。\\n\"            \"输出格式：\\n{output_format_instructions}\\n\\n\"            \"[上下文-开始]\\n{context}\\n[上下文-结束]\\n\\n\"            \"[回答-开始]\\n{answer}\\n[回答-结束]\\n\\n\"        )                self.answer_supported_verdict_prompt = PromptTemplate(            template=answer_supported_verdict_prompt,             partial_variables={                \"output_format_instructions\": self.verdication_parser.get_format_instructions()            }        )                self.answer_supported_verdict_chain = self.answer_supported_verdict_prompt | self.llm | self.verdication_parser    def verdict_supported(self, context: str, answer: str) -&gt; List[Sentence_Support_Verdication]:        \"\"\"        评估回答中的每个句子是否受到上下文的支持。        :param context: 提供的上下文内容。        :param answer: 回答内容。        :return: 句子的支持度判决列表。        \"\"\"        result = self.answer_supported_verdict_chain.invoke({            \"context\": context,            \"answer\": answer,        })        return result.verdications    def calculate_precision(self, verdications: List[Sentence_Support_Verdication]) -&gt; float:        \"\"\"        计算支持度的精度。        :param verdications: 句子的支持度判决列表。        :return: 支持精度（支持的句子占比）。        \"\"\"        if not verdications:            return 0.0        verdications_true = sum(1 for item in verdications if item.sentence_supported_verdication == 1)        precision = verdications_true / len(verdications)        return precision    def invoke(self, inputs: Dict[str, List[str]]) -&gt; Dict[str, float]:        \"\"\"        计算回答和上下文之间的相互支持度。        :param inputs: 包含 `answer`（回答）和 `document_list`（上下文列表）的输入字典。        :return: 一个字典，包含 answer_supported_precision 和 context_supported_precision。        \"\"\"        answer = inputs[\"answer\"]        document_list = inputs[\"document_list\"]        context = '\\n'.join(document_list)        # 计算回答是否被上下文支持        answer_verdications = self.verdict_supported(context, answer)        answer_supported_precision = self.calculate_precision(answer_verdications)        # 计算上下文是否被回答支持（互相验证）        context_verdications = self.verdict_supported(answer, context)        context_supported_precision = self.calculate_precision(context_verdications)        return {            \"answer_supported_precision\": answer_supported_precision,            \"context_supported_precision\": context_supported_precision        }\n\n\nllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-14B-Instruct',\ttemperature=0.5,)inputs = {    \"answer\":answer,    \"document_list\":docs}tool = Context_Relevance(llm)tool.invoke(inputs)\n\n\n\n\n{'answer_supported_precision': 1.0, 'context_supported_precision': 0.75}\n\n","categories":["RAGas"],"tags":["RAG 评估"]},{"title":"黑盒模型评估 SFT 数据质量","url":"/2025/05/04/SFT-data-selection-series/1.LLM_based/","content":"这种情况下一般指的是文本理解能力很强的黑盒api大模型，如chatgpt。这种情况下，大模型对文本理解不会有什么大的问题，因此对于嵌套数据也能理解，例如我让大模型执行指令微调数据打分，以一个任务的prompt和output为输入。小模型可能会对这种包含了两个指令的文本理解错误，强模型基本能理解。因此，可以以强模型来直接对指令微调数据进行打分。\n\n评分指令如下:\n给定一组指令、输入和输出的三元组，你需要基于以下指标对该三元组进行5分制打分。评分为0-5的整数，分数越高，表示该数据依据某个指标是更好的。以下是评估的指标名称和说明：{metric_descriptions}\n\n其中可以灵活改变的部分有:\n\n打分制度: 几分制，每个分数是否有具体的规则，这部分我们暂时不考虑，同时写死的5分制能打分的制不会很多，通常越多的可打分制对数据结果有一点坏处，主要表现在数值分布很不均匀，有些数值断层式少。\n评估方面: 具体需要评估数据的哪些方面，例如准确度偏向输出答案更贴近真实情况，而友善度偏向语言的表达方式和口吻。我不希望每一个不同指标都需要单独写一个 prompt，因为指标的说明很简单而评估数据会更长，不同指标的 prompt 会线性增加 token 消耗，所以只需要在评估 prompt 里交代指标名称和其说明。同时我们也要求大模型在输出具体的打分结果前，先输出对应的理由或者分析。最终结果以嵌套 dict 输出, 形如 {“result”:[{“metric_name”:”指标A”, “reasoning”:”xxx”, “score”: 0-5}, …]}。嵌套dict只有两次，不会带来理解上的压力，而且比正常 dict 方便后期遍历。\n\nimport jsonwith open('./data/alpaca_gpt4_data_zh_100.json', 'r', encoding='utf-8') as file:\ttriplet_list = json.load(file)\n\nDeepseek R1的特殊处理R1的输出结构如下\n&lt;think&gt;reasoning&lt;/think&gt;answer\n\n由于R1有内容，这部分虽然对后面真正回答有益，但对解析不友好，以下类负责把reasoning部分剥离，正式回答还是正常解析类解析\nfrom langchain_core.runnables import Runnablefrom langchain_core.messages.ai import AIMessageclass ThinkContentParser(Runnable):\tdef __init__(self, output_parser):\t\tsuper().__init__()\t\tself.output_parser = output_parser\t\t\tdef invoke(self, message: AIMessage, config) -&gt; tuple:\t\treturn self.parse(message)\tdef parse(self, message: AIMessage) -&gt; tuple:\t\t\"\"\"\t\t解析AIMessage中的content，找到&lt;/think&gt;标签的位置，并将其分为两个部分：\t\t- 一个是标签前的部分（包括 &lt;think&gt; 前的内容）\t\t- 一个是标签后的部分（包括 &lt;/think&gt; 后的内容）\t\t:param message: 原始的AIMessage对象\t\t:return: 一个元组，包含两个字符串，第一个是&lt;/think&gt;前的内容，第二个是&lt;/think&gt;后的内容\t\t\"\"\"\t\toriginal_content = message.content\t\tthink_end_index = original_content.find('&lt;/think&gt;')\t\tif think_end_index != -1:\t\t\tcontent_before_think = original_content[:think_end_index + len('&lt;/think&gt;')]\t\t\tcontent_after_think = original_content[think_end_index + len('&lt;/think&gt;'):].strip()\t\telse:\t\t\tcontent_before_think = \"\"\t\t\tcontent_after_think = original_content\t\tresult = self.output_parser.parse(content_after_think)\t\treturn content_before_think, result\n\n\nimport numpy as npimport asynciofrom typing import List, Dictfrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import PydanticOutputParser# ============== Parser ==============# 单个指标的结果class Metric_Score(BaseModel):    metric_name: str = Field(description=\"This is the name of the metric currently evaluted. \")    reasoning: str = Field(description=\"This is the reasoning process of evaluating the triplet according to the metric. \")    score: float = Field(description=\"This is a decisive score based on the reasoning process above. \")# 多个指标合并为一个类  class Multi_Metric_Score(BaseModel):    result: List[Metric_Score] = Field(description=\"A list of results.\")    output_parser = PydanticOutputParser(pydantic_object=Multi_Metric_Score)wrap_output_parser = ThinkContentParser(output_parser)# ============== Prompt ==============metric2description = {    \"accuracy\": \"Ensure that the output is logically correct and factually accurate, especially for questions with clear answers, such as knowledge-based Q&amp;A.\",    \"effectiveness\": \"Evaluate whether the content of the answer meets the user's needs, not just superficial relevance.\",    \"readability\": \"Ensure that the answer is easy for the user to read and understand, avoiding excessive jargon or complex sentence structures.\",    \"relevance\": \"Evaluate whether the model accurately understands the question and maintains contextual coherence in the answer.\"}metric_str = '\\n'.join([f'- {metric}:{description}' for metric, description in metric2description.items()])rating_prompt = (\t\"You are a helpful and precise assistant for checking the quality of the data.\"\t\"You will be given a triplet, including the instruction, the optional input and the output. \"\t\"Note that the input will be blank if the instruction needs no input to generate the output. \\n\"\t\"Please rate the quality of the triplet according to the following standards:\\n\"\t\"\\n{metric_str}\\n\"\t\"You will rate on a scale of 0 to {score_scale}, where a higher score indicates higher level of the accuracy. \\n\"\t\"Output format: {format}\\n\\n\"\t\"[START of instruction]:\\n{instruction}\\n[END of instruction]\\n\\n\"\t\"[START of input]:\\n{input}\\n[END of input]\\n\\n\"\t\"[START of output]:\\n{output}\\n[END of output]\\n\\n\"\t\"Present the response in {language} language. \\n\")rating_prompt = PromptTemplate(    template=rating_prompt,    partial_variables={\t\t\"metric_str\":metric_str,\t\t\"format\":output_parser.get_format_instructions(), # 可自定义的评分参考规则\t\t\"language\": \"chinese\",        \"score_scale\":5,\t# 可自定义的打分值\t})# ============== LLM ==============# 这是正常非推理模型# llm_interface = ChatOpenAI(# \tbase_url='http://localhost:5551/v1',# \tapi_key='EMPTY',# \tmodel_name='Qwen2.5-14B-Instruct',# \ttemperature=0.5,# \tmax_retries=3,# )# 这是推理模型llm_interface = ChatOpenAI(\tbase_url='http://172.31.101.26:9995/v1',\tapi_key='EMPTY',\tmodel_name='DeepSeek-R1-Distill-Qwen-32B',\ttemperature=0.5,)# ============== chain ==============# 这是正常非推理模型的chain# LLM_evaluator = rating_prompt | llm_interface | output_parser# 这是推理模型的chainLLM_evaluator = rating_prompt | llm_interface | wrap_output_parser\n\n\na_think, a_result = LLM_evaluator.invoke(triplet_list[0])print(a_think)a_result.result\n\n&lt;think&gt;\n好，我现在需要评估这个triplet的质量，包括准确性、有效性、可读性和相关性。让我一步一步地分析。\n\n首先，准确性。输出中的三个健康提示分别是保持身体活动、均衡饮食和充足睡眠。这些都是经过科学验证的健康建议，内容正确且符合事实。没有错误的信息，所以准确性很高，给5分。\n\n接下来是有效性。这三个提示涵盖了身体健康的主要方面，能够满足用户的需求。用户通过这些建议可以采取实际行动，比如开始运动、调整饮食或改善睡眠习惯。因此，有效性也很高，给5分。\n\n然后是可读性。输出使用了清晰简洁的语言，每个建议都以简短的句子列出，易于理解。没有使用复杂的术语，适合所有读者，所以可读性也很好，给5分。\n\n最后是相关性。输出完全符合用户的指令，提供了三个保持健康的提示，没有任何偏离主题的内容。因此，相关性也很高，给5分。\n\n综合来看，这个triplet在各个方面都表现得很出色，所以每个指标都得了满分。\n&lt;/think&gt;\n\n\n\n\n\n[Metric_Score(metric_name='准确性', reasoning='输出中的三个健康提示均基于科学，内容正确且无误。', score=5),\n Metric_Score(metric_name='有效性', reasoning='建议涵盖了身体活动、饮食和睡眠，满足用户需求。', score=5),\n Metric_Score(metric_name='可读性', reasoning='语言简洁清晰，易于理解。', score=5),\n Metric_Score(metric_name='相关性', reasoning='输出完全符合用户指令，提供三个健康提示。', score=5)]\n\n调用方式取决于api可接受请求的程度，有以下三种方法\n## ======= sequential =======from tqdm import tqdmresult_list = []for a_triplet in tqdm(triplet_list):    a_think, a_result = LLM_evaluator.invoke(a_triplet)    result_list.append(a_result)\n\n100%|██████████| 10/10 [01:04&lt;00:00,  6.45s/it]\n\n## ======= parrallel =======tasks = [    LLM_evaluator.ainvoke(a_triplet)\tfor a_triplet in triplet_list]result_list = await asyncio.gather(*tasks)result_list = [item[-1] for item in result_list]\n\n10条数据，线性执行1min，异步调用10s异步调用的效率还是很高的，但如果自己部署时显存不够多，异步调用会报错\n## ======= parrallel =======from concurrent.futures import ThreadPoolExecutor, as_completedresult_list = [None] * len(triplet_list)with ThreadPoolExecutor(max_workers=2) as executor:\tfutures = {executor.submit(LLM_evaluator.invoke, triplet): data_indice for data_indice, triplet in enumerate(triplet_list)}\tfor future in as_completed(futures):\t\t# 获取完成任务的索引\t\tindex = futures[future]\t\t# 将结果存放到对应的索引位置\t\tresult_list[index] = future.result()\n\n线程方法适用于可控的情况，通过max_workers限制同时调用\n分数示例scores = [[m.score for m in item.result] for item in result_list]scores = np.array(scores)\n\n\nimport matplotlib.pyplot as pltplt.hist(scores, bins=5, edgecolor='black')plt.title('Histogram of Data')plt.xlabel('Value')plt.ylabel('Frequency')plt.grid(True)plt.show()\n\n\n\n图中可见部分低质量数据，零散的几个，3.5就有点欠优化了\n","categories":["SFT 数据质量评估"]},{"title":"白盒模型评估 SFT 数据质量","url":"/2025/05/04/SFT-data-selection-series/2.perplexity_based/","content":"白盒大模型最基础的指标就是困惑度 perplexity (PPL)。低 PPL 表示模型对输出序列 y 的概率分布预测更精确，模型对数据的“困惑”更低。高 PPL 表示模型对输出序列 y 的概率分布预测不准确，困惑程度较高。同时 PPL 是长度归一化的，可以避免直接受到长度的影响。\n给定输入序列 x ，白盒大模型输出序列 y 的 PPL 的计算公式如下：类似于指令微调，x 是给定的，无需大模型主观生产，而 y 是大模型基于 x 主动生成的，PPL 是关于 y 部分的。\n以下是用 PyTorch 和 Hugging Face 计算 PPL 的示例代码：\nimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer# 加载模型和分词器model_name = \"gpt2\"model = AutoModelForCausalLM.from_pretrained(model_name)tokenizer = AutoTokenizer.from_pretrained(model_name)# 定义输入 x 和输出 yinstruction = \"Summarize the following paragraph.\"input_text = \"The quick brown fox jumps over the lazy dog.\"output_text = \"The fox jumps over the dog.\"x = instruction + \" \" + input_texty = output_text# 将 x 和 y 连接为完整序列full_text = x + \" \" + yinputs = tokenizer(full_text, return_tensors=\"pt\")# 获取 tokenized 输入input_ids = inputs[\"input_ids\"]labels = input_ids.clone()# 计算模型输出with torch.no_grad():    outputs = model(input_ids, labels=labels)    loss = outputs.loss  # CrossEntropyLoss# 根据 loss 计算 PPLppl = torch.exp(loss)print(f\"PPL: {ppl.item()}\")\n\n\nimport jsonwith open('./data/alpaca_gpt4_data_zh_10.json', 'r', encoding='utf-8') as file:\ttriplet_list = json.load(file)triplet_list = triplet_list[:5]\n\n\nimport torchfrom transformers import AutoTokenizer, AutoModelForCausalLMmodel_path = '../../DataCollection/officials/Qwen2.5-3B-Instruct'tokenizer = AutoTokenizer.from_pretrained(model_path)model = AutoModelForCausalLM.from_pretrained(\tmodel_path, \ttorch_dtype=torch.bfloat16,\t)print('1')model = model.to('cuda:6')\n\nLoading checkpoint shards: 100%|██████████| 2/2 [00:00&lt;00:00,  3.34it/s]\n\n\n1\n\n假设数据还是三元组的形式，三元组的拼接方式大部分都不会对计算困惑度有啥大影响，除非拼接方式特别离谱。反正这里我们只是筛选数据，并不是改为SFT形式的数据，所以不对三元组的拼接方式作特殊处理。\nprompt 拼接PROMPT_DICT = {    \"zero-shot\": {        \"prompt_input\": (            \"Below is a description of the task and an input providing more context. \"            \"Please write an appropriate response to complete the request.\\n\\n\"            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"        ),        \"prompt_no_input\": (            \"Below is a description of the task. \"            \"Please write an appropriate response to complete the request.\\n\\n\"            \"### Instruction:\\n{instruction}\\n\\n### Response:\"        ),    },    \"one-shot\": {        \"prompt_input\": (            \"Below is a description of the task and an input providing more context. \"            \"Please write an appropriate response to complete the request based on the example task and new task.\\n\\n\"            \"### Instruction:\\n{instruction}\\n\\n\"            \"### Example Task\\n\"            \"#### Input:\\n{example_input}\\n\\n#### Response:\\n{example_output}\\n\\n\"            \"### New Task\\n\"            \"#### Input:\\n{input}\\n\\n#### Response:\"        ),        \"prompt_no_input\": (            \"Below is a description of the task and a reference example. \"            \"Please write an appropriate response to complete the new task based on the reference example.\\n\\n\"            \"### Instruction:\\n{instruction}\\n\\n\"            \"### Example Task\\n\"            \"#### Response:\\n{example_output}\\n\\n\"            \"### New Task\\n\"            \"#### Instruction:\\n{instruction}\\n\\n#### Response:\"        ),    },\t\"reversed-zero-shot\":{        \"prompt_input\": (            \"Below is an input provided for a certain instruction, and a response for completing that instruction. \"            \"Please generate an appropriate instruction based on the output and response.\\n\\n\"            \"### Input:\\n{input}\\n\\n### Response:\\n{output}\\n\\n### Instruction:\"        ),        \"prompt_no_input\": (            \"Below is a response for completing a certain instruction. \"            \"Please generate an appropriate instruction based on the output. \\n\\n\"            \"### Response:\\n{output}\\n\\n### Instruction:\"        ),\t}}def create_zero_shot_prompt(triplet):\t\"\"\"\t产生 zero-shot learning 的 prompt\ttriplet 应该是 \t{\t\t\"instruction\": \"...\",\t\t\"input\": \"...\",\t\t\"output\": \"...\"\t}\t\"\"\"\tprompt_template = PROMPT_DICT[\"zero-shot\"][\"prompt_input\" if triplet[\"input\"] != \"\" else \"prompt_no_input\"]\tprompt = prompt_template.format_map(triplet)\treturn promptdef create_one_shot_prompt(triplet, example_io):\t\"\"\"\t产生 one-shot learning 的 prompt\ttriplet 应该是 \t{\t\t\"instruction\": \"...\",\t\t\"input\": \"...\",\t\t\"output\": \"...\"\t}\texample_io 应该是\t{\t\t\"example_input\": \"...\",\t\t\"example_output\": \"...\"\t}\t虽然这种prompt应该只用于有input的情况, 但也可以用于没有input的情况\t\"\"\"\tprompt_template = PROMPT_DICT[\"one-shot\"][\"prompt_input\" if triplet[\"input\"] != \"\" else \"prompt_no_input\"]\tprompt = prompt_template.format_map({**triplet, **example_io})\treturn promptdef create_reverse_prompt(triplet):\tprompt_template = PROMPT_DICT[\"reversed-zero-shot\"][\"prompt_input\" if triplet[\"input\"] != \"\" else \"prompt_no_input\"]\tprompt = prompt_template.format_map(triplet)\treturn promptprint(create_zero_shot_prompt(triplet_list[0]))\n\nBelow is a description of the task. Please write an appropriate response to complete the request.\n\n### Instruction:\n保持健康的三个提示。\n\n### Response:\n\n计算困惑度这个文章参考了IFD的原始代码，一个问题就是它原来是每次计算一条数据的困惑度，用的是transformers模型forward函数自带的计算loss方法(进而转换为困惑度)，只需要把input_ids和labels正常传入即可。  \ninput_ids = tokenizer(prompt, max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')[\"input_ids\"].to(model.device)input_data = {\t\"input_ids\": input_ids,\t\"labels\": input_ids}model_ret = model(**input_data)loss = model_ret.lossppl = torch.exp(loss).item()\n\n这种方式计算起来太慢了，没能重复利用gpu优势，所以这部分代码我改为批量计算困惑度\nimport torchfrom tqdm import tqdmfrom transformers import  DataCollatorForSeq2Seqdef reorder_arrays(sort_order, *arrays, reverse=False):\t\"\"\"\t根据排序数组的顺序重新排序多个数组，并支持控制反序。\t\"\"\"\t# 获取排序后的索引，考虑 reverse 参数\tsorted_indices = sorted(range(len(sort_order)), key=lambda x: sort_order[x], reverse=reverse)\t\t# 按索引重新排序每个数组\treordered_arrays = tuple([array[i] for i in sorted_indices] for array in arrays)\t\treturn reordered_arraysdef calculate_sample_perplexity(model, tokenizer, input_data, batch_size=5):\t\"\"\"批量计算困惑度\"\"\"\tdata_collator = DataCollatorForSeq2Seq(tokenizer)\tloss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\tmodel.eval()\t\tdata_perplexities = []\twith tqdm(total=len(input_data)) as pbar:\t\tfor i in range(0, len(input_data), batch_size):\t\t\t\t\t\tbatch_data = input_data[i:i + batch_size]\t\t\tbatch_data_tensor = data_collator(batch_data).to(model.device)\t\t\t\t\t\t\t\t\t\t\t\tmodel_outputs = model(\t\t\t\tinput_ids=batch_data_tensor['input_ids'],\t\t\t\tattention_mask=batch_data_tensor['attention_mask']\t\t\t)\t\t\tlogits = model_outputs.logits\t\t\t\t\t\tshift_logits = logits[:, :-1, :].contiguous()\t\t\tshift_labels = batch_data_tensor['labels'][:, 1:].contiguous()\t\t\tper_token_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\t\t\tper_token_loss = per_token_loss.view(shift_labels.size())\t\t\tlabel_valid_mask = (shift_labels != -100)\t\t\tper_sample_loss = (per_token_loss * label_valid_mask).sum(dim=1) / label_valid_mask.sum(dim=1)\t\t\t\t\t\tbatch_perplexities = torch.exp(per_sample_loss).tolist()\t\t\tdata_perplexities.extend(batch_perplexities)\t\t\tpbar.update(len(batch_data))\treturn data_perplexitiesdef calculate_sample_perplexity_resortable(model, tokenizer, input_data, batch_size=5, sort_by_len:bool=False):\t\"\"\"批量计算困惑度, but可以重排序以减少padding\"\"\"\t\tif sort_by_len:\t\tinput_data_len = [len(item[\"input_ids\"]) for item in input_data]\t\tinput_data_indice = list(range(len(input_data)))\t\tinput_data, input_data_indice = reorder_arrays(input_data_len, input_data, input_data_indice)\t\t\t\t\t\tdata_collator = DataCollatorForSeq2Seq(tokenizer)\tloss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\tmodel.eval()\t\tdata_perplexities = []\twith tqdm(total=len(input_data)) as pbar:\t\tfor i in range(0, len(input_data), batch_size):\t\t\t\t\t\tbatch_data = input_data[i:i + batch_size]\t\t\tbatch_data_tensor = data_collator(batch_data).to(model.device)\t\t\t\t\t\t\t\t\t\t\t\tmodel_outputs = model(\t\t\t\tinput_ids=batch_data_tensor['input_ids'],\t\t\t\tattention_mask=batch_data_tensor['attention_mask']\t\t\t)\t\t\tlogits = model_outputs.logits\t\t\t\t\t\tshift_logits = logits[:, :-1, :].contiguous()\t\t\tshift_labels = batch_data_tensor['labels'][:, 1:].contiguous()\t\t\tper_token_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\t\t\tper_token_loss = per_token_loss.view(shift_labels.size())\t\t\tlabel_valid_mask = (shift_labels != -100)\t\t\tper_sample_loss = (per_token_loss * label_valid_mask).sum(dim=1) / label_valid_mask.sum(dim=1)\t\t\t\t\t\tbatch_perplexities = torch.exp(per_sample_loss).tolist()\t\t\tdata_perplexities.extend(batch_perplexities)\t\t\tpbar.update(len(batch_data))\t\tif sort_by_len:\t\tinput_data_perplexities, = reorder_arrays(input_data_indice, input_data_perplexities)\t\t\treturn data_perplexities\n\n困惑度类指标Prompt 困惑度Prompt = Instruction + Input。如果指令写的很清晰的话，大模型理解指令这个 prompt 文本就很容易，理应更不困惑，所以就可以用 PPL(x) 来衡量大模型对 prompt 文本的理解清晰程度。\ndef get_prompt_complexity(triplet_list, model, tokenizer, batch_size:int=10):\t# prepare data\tinput_data = []\tfor triplet in triplet_list:\t\tprompt = create_zero_shot_prompt(triplet)\t\tinput_ids = tokenizer.encode(prompt, max_length=tokenizer.model_max_length, truncation=True)\t\tinput_data.append(\t\t\t{\t\t\t\t\"input_ids\": input_ids,\t\t\t\t\"labels\": input_ids\t\t\t}\t\t)\t\t\t# calculate perplexity\tdata_perplexities = calculate_sample_perplexity_resortable(model, tokenizer, input_data, batch_size)\treturn data_perplexitiesget_prompt_complexity(triplet_list, model, tokenizer)\n\n100%|██████████| 5/5 [00:01&lt;00:00,  3.26it/s]\n\n\n\n\n\n[185.0, 123.0, 115.5, 139.0, 79.5]\n\nResponse 困惑度一方面好的 prompt 更易于让模型输出对应的 output，另一方面好的 output 也更容易在给定 prompt 的情况下生成。所以计算 PPL(y|x) 对 prompt 和 output 都有一定衡量。\nfrom copy import deepcopydef get_response_complexity(triplet_list, model, tokenizer, batch_size:int=10):\t# prepare data\tinput_data = []\tfor triplet in triplet_list:\t\t\t\toutput = triplet[\"output\"]\t\tinput_ids = tokenizer.encode(output, max_length=tokenizer.model_max_length, truncation=True)\t\t\t\tprompt = create_zero_shot_prompt(triplet)\t\twhole_text = prompt + output\t\tinput_ids = tokenizer.encode(whole_text, max_length=tokenizer.model_max_length, truncation=True)\t\ttoken_start_index = len(tokenizer.encode(prompt, max_length=tokenizer.model_max_length, truncation=True))\t\tlabels = deepcopy(input_ids)\t\tlabels[:token_start_index] = [-100] * min(token_start_index, len(labels))\t\tinput_data.append(\t\t\t{\t\t\t\t\"input_ids\": input_ids,\t\t\t\t\"labels\": labels\t\t\t}\t\t)\t# calculate perplexity\tdata_perplexities = calculate_sample_perplexity_resortable(model, tokenizer, input_data, batch_size)\treturn data_perplexitiesget_response_complexity(triplet_list, model, tokenizer)\n\n100%|██████████| 5/5 [00:00&lt;00:00, 33.38it/s]\n\n\n\n\n\n[3.921875, 5.90625, 5.03125, 4.59375, 12.0]\n\n指令跟随难度 Instruction Following Difficulty (IFD)上述的 Output 困惑度虽然包含了 prompt 对生成 output 的帮助程度，但同时也和 output 本身生成也有一定关系，因此Output 困惑度包含了两方面：\n\nprompt 对生成 output 的帮助程度\noutput 本身生成的容易程度\n\n由于后者的存在，该困惑度与 output 本身也耦合，例如更长更复杂的 output 的 PPL 相比于短又直接的 output 更大。因此有必要将 “output 本身生成的容易程度” 剥离，从而仅保留 “prompt 对生成 output 的帮助程度”。而 “output 本身生成的容易程度” 本质上可以由 output 本身的 PPL 代表，因此 “prompt 对生成 output 的帮助程度” 可以表达为:\ndef create_IFD_input_data(triplet_list, tokenizer):\tdata_whole_text = []\tdata_output_only = []\tfor triplet in triplet_list:\t\t\t\toutput = triplet[\"output\"]\t\tinput_ids = tokenizer.encode(output, max_length=tokenizer.model_max_length, truncation=True)\t\tdata_output_only.append(\t\t\t{\t\t\t\t\"input_ids\": input_ids,\t\t\t\t\"labels\": input_ids\t\t\t}\t\t)\t\t\t\tprompt = create_zero_shot_prompt(triplet)\t\twhole_text = prompt + output\t\tinput_ids = tokenizer.encode(whole_text, max_length=tokenizer.model_max_length, truncation=True)\t\ttoken_start_index = len(tokenizer.encode(prompt, max_length=tokenizer.model_max_length, truncation=True))\t\tlabels = deepcopy(input_ids)\t\tlabels[:token_start_index] = [-100] * min(token_start_index, len(labels))\t\tdata_whole_text.append(\t\t\t{\t\t\t\t\"input_ids\": input_ids,\t\t\t\t\"labels\": labels\t\t\t}\t\t)\treturn data_whole_text, data_output_onlydef get_IFD(triplet_list, model, tokenizer, batch_size:int=10):\t# prepare data\tdata_whole_text, data_output_only = create_IFD_input_data(triplet_list, tokenizer)\t\t# calculate perplexity\tppls = []\tfor input_data in [data_whole_text, data_output_only]:\t\t\t\t\tinput_data_perplexities = calculate_sample_perplexity_resortable(model, tokenizer, input_data, batch_size)\t\t\t\tppls.append(input_data_perplexities)\t\t# ppl_whole_text, ppl_output_only = ppls\t# IFD = PPL(y|x) / PPL(y), x = prompt ~= instruction + input, y = output\tifd = [a/b for a, b in zip(*ppls)]\t\treturn ifdget_IFD(triplet_list, model, tokenizer)\n\n100%|██████████| 5/5 [00:00&lt;00:00, 70.08it/s]\n100%|██████████| 5/5 [00:00&lt;00:00, 69.38it/s]\n\n\n\n\n\n[0.9507575757575758,\n 1.0617977528089888,\n 1.0125786163522013,\n 1.027972027972028,\n 0.9411764705882353]\n\nIFD 应该越小越好\n指令生成难度 Instruction Generate Difficulty指令跟随难度评估对象是指令，同理需要评估回复。类似于指令有助于生成回复，回复反向能有助于生成对应的指令。现在将 output 和 instruction 的位置翻转，任务变成基于回复生成指令。\n\ndef create_IGD_input_data(triplet_list, tokenizer):\tdata_whole_text = []\tdata_output_only = []\tfor triplet in triplet_list:\t\t\t\toutput = triplet[\"instruction\"]\t\tinput_ids = tokenizer.encode(output, max_length=tokenizer.model_max_length, truncation=True)\t\tdata_output_only.append(\t\t\t{\t\t\t\t\"input_ids\": input_ids,\t\t\t\t\"labels\": input_ids\t\t\t}\t\t)\t\t\t\tprompt = create_reverse_prompt(triplet)\t\twhole_text = prompt + output\t\tinput_ids = tokenizer.encode(whole_text, max_length=tokenizer.model_max_length, truncation=True)\t\ttoken_start_index = len(tokenizer.encode(prompt, max_length=tokenizer.model_max_length, truncation=True))\t\tlabels = deepcopy(input_ids)\t\tlabels[:token_start_index] = [-100] * min(token_start_index, len(labels))\t\tdata_whole_text.append(\t\t\t{\t\t\t\t\"input_ids\": input_ids,\t\t\t\t\"labels\": labels\t\t\t}\t\t)\treturn data_whole_text, data_output_onlydef get_IGD(triplet_list, model, tokenizer, batch_size:int=10):\t# prepare data\tdata_whole_text, data_output_only = create_IGD_input_data(triplet_list, tokenizer)\t\t# calculate perplexity\tppls = []\tfor input_data in [data_whole_text, data_output_only]:\t\t\t\t\tinput_data_perplexities = calculate_sample_perplexity_resortable(model, tokenizer, input_data, batch_size)\t\t\t\tppls.append(input_data_perplexities)\t\t# ppl_whole_text, ppl_output_only = ppls\t# IFD = PPL(y|x) / PPL(y), x = prompt ~= instruction + input, y = output\tigd = [a/b for a, b in zip(*ppls)]\t\treturn igdget_IGD(triplet_list, model, tokenizer)\n\n  0%|          | 0/5 [00:00&lt;?, ?it/s]\n\n100%|██████████| 5/5 [00:00&lt;00:00, 68.98it/s]\n100%|██████████| 5/5 [00:00&lt;00:00, 72.23it/s]\n\n\n\n\n\n[0.4421768707482993,\n 1.0,\n 0.6465116279069767,\n 0.5043103448275862,\n 0.8049792531120332]\n\nOne-shot 有效性众所周知，当无法微调模型时，如果指令不够清晰或者比较复杂，可以通过加入输入输出例子来让模型通过 Few-shot learning / In-context learning 来学习指令怎么执行。因此这些 shot / 例子可以优化大模型的理解，如果 shot 是合理的情况。反之，如果 shot 不合理，那甚至效果不如没有 shot 的情况。因此通过对比一组数据作为 shot 辅助其他数据生成的有效性，就可知作为 shot 的数据的合理性/有效性。\nOne-shot 有效性可计算为：以需评估的数据为 shot， 将该 shot 以 one-shot learning 的方法对生成其他数据的影响。注意：\n\n计算时需要随机抽取以计算平均值\n由于 PPL 会与对应的 output 相关，最好只取符号值，即二元判断有无帮助\n由于 Few-shot learning 仅在同种任务的数据上有用，所以有必要先根据任务类型再计算\n\nimport randomimport numpy as npfrom typing import Literalfrom collections import defaultdictdef list_to_index_dict(input_list):\t\"\"\"\t将一个列表转换为字典，其中键为列表的元素值，值为该元素的索引数组。\t\"\"\"\tindex_dict = {}\tfor idx, value in enumerate(input_list):\t\tindex_dict.setdefault(value, []).append(idx)\treturn index_dictdef get_random_non_m_values(indice_pool, m, n):\t\"\"\"\t从 indice_pool 中随机抽取 n 个非 m 的不重复值。如果数量不够, 直接返回所有非 m 的值\t\"\"\"\t# 过滤掉值为 m 的元素\tfiltered_pool = [x for x in indice_pool if x != m]\t\t# 检查池中是否有足够的元素\tif len(filtered_pool) &lt; n:\t\treturn filtered_pool\t# 从过滤后的池中随机抽取 n 个不重复的元素\trandom_selection = random.sample(filtered_pool, n)\t\treturn random_selectiondef create_zero_and_one_shot_input_data(triplet, example, tokenizer):\t\t\"\"\"\t\t分别创造 zero-shot 和 one-shot 时的 input_data\t\t注意, example 是评估对象/数据, triplet 是随机抽取的同任务数据\t\t\"\"\"\t\toutput = triplet[\"output\"]\t\tprompt_zero_shot = create_zero_shot_prompt(triplet)\t\twhole_text_zero_shot = prompt_zero_shot + output\t\tinput_ids = tokenizer.encode(whole_text_zero_shot, max_length=tokenizer.model_max_length, truncation=True)\t\ttoken_start_index = len(tokenizer.encode(prompt_zero_shot, max_length=tokenizer.model_max_length, truncation=True))\t\tlabels = deepcopy(input_ids)\t\tlabels[:token_start_index] = [-100] * min(token_start_index, len(labels))\t\t\t\tdata_zero_shot = {\t\t\t\"input_ids\": input_ids,\t\t\t\"labels\": labels\t\t}\t\t\t\tprompt_one_shot = create_one_shot_prompt(triplet, example)\t\twhole_text_one_shot = prompt_one_shot + output\t\t\t\tinput_ids = tokenizer.encode(whole_text_one_shot, max_length=tokenizer.model_max_length, truncation=True)\t\ttoken_start_index = len(tokenizer.encode(prompt_one_shot, max_length=tokenizer.model_max_length, truncation=True))\t\tlabels = deepcopy(input_ids)\t\tlabels[:token_start_index] = [-100] * min(token_start_index, len(labels))\t\t\t\tdata_one_shot = {\t\t\t\"input_ids\": input_ids,\t\t\t\"labels\": labels\t\t}\t\t\t\treturn data_zero_shot, data_one_shotdef get_One_Shot_Example_Validity(triplet_list, triplet_task_list,            model, tokenizer,            one_shot_sample_cnt:int=3, validity_calculation:Literal['raw', 'sign']='sign',             batch_size=10):    \ttask2indexs = list_to_index_dict(triplet_task_list)\tinput_data_zero_shot = []\tinput_data_one_shot = []\tinput_data_indices = []\tfor indice, (triplet, task) in tqdm(enumerate(zip(triplet_list, triplet_task_list))):\t\tindice_pool = get_random_non_m_values(task2indexs[task], indice, one_shot_sample_cnt)\t\texample = {\t\t\t\"example_input\": triplet[\"input\"],\t\t\t\"example_output\": triplet[\"output\"]\t\t}\t\tfor tmp in indice_pool:\t\t\tdata_zero_shot, data_one_shot = create_zero_and_one_shot_input_data(\t\t\t\ttriplet=triplet_list[tmp],\t\t\t\texample=example,\t\t\t\ttokenizer=tokenizer\t\t\t)\t\t\tinput_data_zero_shot.append(data_zero_shot)\t\t\tinput_data_one_shot.append(data_one_shot)\t\t\tinput_data_indices.append(indice)\tppls = []\tfor input_data in [input_data_zero_shot, input_data_one_shot]:\t\t\t\tdata_perplexities = calculate_sample_perplexity_resortable(model, tokenizer, input_data, batch_size)\t\tppls.append(data_perplexities)\t# 计算 one-shot 相比 zero-shot 时的有效性\tvalidity = []\tfor a, b in zip(*ppls):\t\ttmp = a - b\t\tif validity_calculation == \"raw\":\t\t\tpass\t\telif validity_calculation == \"sign\":\t\t\ttmp = np.sign(tmp)\t\telse:\t\t\tpass\t\tvalidity.append(tmp)\t\t# 根据 indice 计算平均值, 没有则为nan\tindex_dict = defaultdict(list)\tfor idx, val in zip(input_data_indices, validity):\t\tindex_dict[idx].append(val)\tresult = []\tfor i in range(len(triplet_list)):\t\tif i in index_dict:\t\t\tavg = np.mean(index_dict[i])\t\telse:\t\t\tavg = np.nan # 如果没有对应的值，返回 NaN\t\tresult.append(avg)\treturn resulttriplet_list_test = [\t{\t\t'instruction': '分类以下句子的情感为伤心、高兴、正常。',\t\t'input': '今天中午吃什么？',\t\t'output': '正常'\t},\t{\t\t'instruction': '分类以下句子的情感为伤心、高兴、正常。',\t\t'input': '我的科三挂了。',\t\t'output': '伤心'\t},\t{\t\t'instruction': '分类以下句子的情感为伤心、高兴、正常。',\t\t'input': '今天上班被老板骂了。',\t\t'output': '高兴'\t},]triplet_task_list_test = ['情感分类']*3get_One_Shot_Example_Validity(triplet_list_test, triplet_task_list_test, model, tokenizer, 2)\n\n3it [00:00, 425.99it/s]\n100%|██████████| 6/6 [00:00&lt;00:00, 115.09it/s]\n100%|██████████| 6/6 [00:00&lt;00:00, 115.74it/s]\n\n\n\n\n\n[1.0, 0.0, 0.0]\n\n这个指标更偏向于评估输入输出，而不是指令。\n更正以上代码可以会出现计算结果为nan的情况，那是因为output只有一个token，因为label shift，最后一个token无法参与loss计算，所以需要在output之后加入一个token，什么token应该不影响\noutput = triplet[\"output\"]\n改为\noutput = triplet[\"output\"] + tokenizer.eos_token","categories":["SFT 数据质量评估"]},{"title":"Langchain 入门教程 - 1.基础应用","url":"/2025/05/04/langchain-tutorial-series/01.Basic/","content":"初始化大模型接口并调用一般都是用ChatOpenAI这个类，以下两个方式引用都可以:\n\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_openai import ChatOpenAI\n\n这是对话的重要参数,关键的是model.Key init args — completion params:\n\nmodel: str, Name of OpenAI model to use.\ntemperature: float, Sampling temperature.\nmax_tokens: Optional[int], Max number of tokens to generate.\nlogprobs: Optional[bool], Whether to return logprobs.\nstream_options: Dict, Configure streaming outputs, like whether to return token usage when streaming ({\"include_usage\": True}).\n\n这是客户端的重要参数，关键的有base_url和api_key.Key init args — client params:\n\ntimeout: Union[float, Tuple[float, float], Any, None], Timeout for requests.\nmax_retries: int, Max number of retries.\napi_key: Optional[str], OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.\nbase_url: Optional[str], Base URL for API requests. Only specify if using a proxy or service\n\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)\n\n使用invoke函数来调用大模型接口\nquestion = \"What is the capital of USA?\"llm.invoke(question)\n\n响应格式（AIMessage类）在使用 ChatOpenAI 对象时，响应以 AI 消息 的格式返回。这包括模型生成的文本内容以及与响应相关的元数据或附加属性。这些信息提供了关于 AI 回复的结构化数据，以及响应的生成方式。\nAI 消息的关键组成部分\n\ncontent  \n\n定义： 由 AI 生成的主要响应文本。  \n示例： “韩国的首都是首尔。”  \n作用： 这是用户与 AI 交互的主要部分。\n\n\nresponse_metadata  \n\n定义： 关于响应生成过程的元数据。  \n主要字段：  \nmodel_name ： 使用的模型名称（例如 \"gpt-4o-mini\"）。  \nfinish_reason ： 生成停止的原因（stop 表示正常完成）。  \ntoken_usage ： 令牌使用详情：\nprompt_tokens ： 输入查询使用的令牌数。  \ncompletion_tokens ： 响应内容使用的令牌数。  \ntotal_tokens ： 输入和输出的总令牌数。\n\n\n\n\n\n\nid  \n\n定义： API 调用的唯一标识符。  \n作用： 便于跟踪或调试特定交互。\n\n\n\n# 示例\"\"\"AIMessage(    content='The capital of the United States is Washington, D.C.',     additional_kwargs={        'refusal': None    },     response_metadata={        'token_usage': {            'completion_tokens': 13,             'prompt_tokens': 36,             'total_tokens': 49,             'completion_tokens_details': None,             'prompt_tokens_details': None        },         'model_name': 'Qwen2.5-7B-Instruct',         'system_fingerprint': None,         'finish_reason': 'stop',         'logprobs': None},     id='run-e2adb89c-7c83-4a53-b68a-be914308c468-0',     usage_metadata={        'input_tokens': 36,         'output_tokens': 13,         'total_tokens': 49,         'input_token_details': {},         'output_token_details': {}    })\"\"\"\n\n流式输出流式选项特别适用于接收查询的实时响应。与等待整个响应生成完成不同，该模型会逐个令牌或按数据块流式传输输出，从而实现更快的交互和即时反馈。\nanswer = llm.stream(    \"Please provide 10 beautiful tourist destinations in USA along with their addresses!\")# 这种流式生成方式本质上是 迭代器 (iterator) 的一种应用。for token in answer:    print(token.content, end=\"\", flush=True)\n\n链式创建（Chain Creation）在这里，我们使用 LCEL（LangChain Expression Language / LangChain 表达式语言） 将多个组件组合成一个完整的链。  \nchain = prompt | model | output_parser\n\n\n| 运算符 类似于 Unix 管道操作符，用于连接不同的组件，并将一个组件的输出作为下一个组件的输入。\n\n在这个链式结构中：\n\n用户输入被传递到 提示模板（PromptTemplate）。  \n提示模板 处理输入并生成结构化的提示。  \n模型（LLM） 接收提示并生成响应。  \n输出解析器（Output Parser） 进一步解析并格式化最终输出。\n\n通过单独检查每个组件，可以清楚地理解每一步的处理过程。\nPrompt 模板PromptTemplate 用于通过用户的输入变量创建完整的提示字符串。\n\ntemplate：模板字符串是一个预定义的格式，其中使用大括号 {} 表示变量。  \ninput_variables：以列表形式定义要插入到大括号 {} 中的变量名称。\n\nfrom langchain_core.prompts import PromptTemplate# Define templatetemplate = \"What is the capital of {country}?\"# Create a `PromptTemplate` object using the `from_template` method.prompt_template = PromptTemplate.from_template(template)prompt_template\n\n\nprompt_template.format(country=\"Korea\")\n\n大模型接口from langchain_openai import ChatOpenAImodel = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)model\n\n\nquestion = \"What is the capital of USA?\"model.invoke(question)\n\n输出解析器（Output Parser）输出解析器（Output Parser） 是一种用于转换或处理 AI 模型响应的工具。由于模型的输出通常是 自由格式文本（free-form text），因此 输出解析器 在以下方面至关重要：  \n\n将输出转换为结构化格式（如 JSON、表格或特定的数据结构）。  \n提取所需的数据，过滤无关信息，以便更高效地使用 AI 生成的内容。\n\nfrom langchain_core.output_parsers import StrOutputParser# 直接返回stroutput_parser = (    StrOutputParser())\n\n组成chain调用 invoke()  \n\n输入值(prompt模板中的变量)以 Python 字典（键值对）的形式提供。\n在调用 invoke() 函数时，这些输入值作为参数传递。\n\nfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import PromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = PromptTemplate.from_template(\"Please explain {topic} in simple terms.\")model = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)output_parser = StrOutputParser()chain = prompt | model | output_parser\n\n\ninput = {\"topic\": \"The Principles of Learning in Artificial Intelligence Models\"}print(chain.invoke(input))\n\n\n# 同理，chain也可以流式生成answer = chain.stream(input)for token in answer:    print(token, end=\"\", flush=True)\n\nSure! The principles of learning in artificial intelligence (AI) models can be explained in simple terms as follows:\n\n### 1. **Data**\n- **What it is:** Data are the raw materials that AI models use to learn.\n- **Why it's important:** Good quality data helps the model understand patterns and make accurate predictions.\n\n### 2. **Training**\n- **What it is:** Training is the process where the AI model learns from the data.\n- **How it works:** The model is shown examples (data) and adjusts its internal parameters to minimize errors in predictions.\n\n### 3. **Model**\n- **What it is:** A model is the mathematical representation of the AI system.\n- **How it works:** It processes input data and produces output predictions or decisions.\n\n### 4. **Parameters**\n- **What they are:** Parameters are the internal settings or weights of the model that are adjusted during training.\n- **Why they're important:** These settings determine how the model makes predictions.\n\n### 5. **Loss Function**\n- **What it is:** A loss function measures how wrong the model's predictions are.\n- **How it works:** The model tries to minimize this function by adjusting its parameters.\n\n### 6. **Optimization**\n- **What it is:** Optimization is the process of finding the best set of parameters to minimize the loss function.\n- **How it works:** Algorithms like gradient descent are used to iteratively adjust the parameters.\n\n### 7. **Validation**\n- **What it is:** Validation is the process of checking how well the model performs on new, unseen data.\n- **Why it's important:** It helps ensure the model generalizes well to new data and isn't just memorizing the training data.\n\n### 8. **Testing**\n- **What it is:** Testing is the final step where the model's performance is evaluated on a completely separate set of data.\n- **Why it's important:** It gives a final assessment of how well the model will perform in real-world scenarios.\n\n### 9. **Feedback Loop**\n- **What it is:** A feedback loop is where the model's predictions are used to improve the training data or the model itself.\n- **Why it's important:** Continuous improvement can lead to better performance over time.\n\n### 10. **Regularization**\n- **What it is:** Regularization is a technique to prevent overfitting by adding a penalty for complex models.\n- **Why it's important:** It helps the model generalize better to new data by avoiding overly complex solutions.\n\n### 11. **Evaluation Metrics**\n- **What they are:** Evaluation metrics are specific measures used to assess the performance of the model.\n- **Examples:** Accuracy, precision, recall, F1 score, etc.\n- **Why they're important:** They provide a quantitative way to compare different models or evaluate the effectiveness of changes.\n\n### 12. **Hyperparameters**\n- **What they are:** Hyperparameters are settings that control the training process but are not learned from the data.\n- **Examples:** Learning rate, batch size, number of layers in a neural network.\n- **Why they're important:** They can significantly affect the model's performance and need to be carefully tuned.\n\nThese principles form the foundation of how AI models learn and improve over time. By understanding these concepts, you can better appreciate how AI systems are developed and deployed in various applications.\n\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 3.输出解析器","url":"/2025/05/04/langchain-tutorial-series/03.OutputParser/","content":"Prompt TemplatePrompt 模板对于生成动态且灵活的提示至关重要，可用于各种场景，例如会话历史记录、结构化输出和特定查询。  \n在本教程中，我们将探讨创建 PromptTemplate 对象的方法，应用部分变量，通过 YAML 文件管理模板，并利用 ChatPromptTemplate 和 MessagePlaceholder 等高级工具来增强功能。\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)\n\n创建 PromptTemplate 对象有两种方法可以创建 PromptTemplate 对象：  \n\n1. 使用 from_template() 方法。  \n2. 直接创建 PromptTemplate 对象并同时生成提示词。\n\n方法 1. 使用 from_template() 方法\n使用 {variable} 语法定义模板，其中 variable 代表可替换的变量。\n\nfrom langchain_core.prompts import PromptTemplate# {}内部是变量template = \"What is the capital of {country}?\"# 使用`from_template`函数来创建模板prompt = PromptTemplate.from_template(template)prompt\n\n\n\n\nPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')\n\nPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')\n类已经解析出country这个变量，可以通过为变量 country 赋值来完成提示词。\n# 类似str的`format`方法来创建实例prompt.format(country=\"United States of America\")\n\n\n\n\n'What is the capital of United States of America?'\n\n进一步用chain来简化流程\ntemplate = \"What is the capital of {country}?\"prompt = PromptTemplate.from_template(template)chain = prompt | llmchain.invoke(\"United States of America\").content\n\n\n\n\n'The capital of the United States of America is Washington, D.C.'\n\n方法 2. 直接创建 PromptTemplate 对象并同时生成提示\n明确指定 input_variables 以进行额外的验证。  \n否则，如果 input_variables 与模板字符串中的变量不匹配，实例化时可能会引发异常。\n\nfrom langchain_core.prompts import PromptTemplate# Define templatetemplate = \"What is the capital of {country}?\"# Create a prompt template with `PromptTemplate` objectprompt = PromptTemplate(    template=template,    input_variables=[\"country\"],)prompt\n\n\n\n\nPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')\n\npartial variables可临时固定的可变参数, 是特殊的 input_variables, 是对应 input_variables 在缺失时的默认值。使用 partial_variables，您可以部分应用函数。这在需要共享 通用变量 时特别有用。  \n常见示例：  \n\n日期或时间（date or time） 是典型的应用场景。\n\n例如，假设您希望在提示中指定当前日期：  \n\n直接硬编码日期 或 每次手动传递日期变量 可能不太灵活。  \n更好的方法 是使用一个返回当前日期的函数，将其部分应用于提示模板，从而动态填充日期变量，使提示更具适应性。\n\nfrom langchain_core.prompts import PromptTemplate# Define templatetemplate = \"What are the capitals of {country1} and {country2}, respectively?\"# Create a prompt template with `PromptTemplate` objectprompt = PromptTemplate(    template=template,    input_variables=[\"country1\"],    partial_variables={        \"country2\": \"United States of America\"  # Pass `partial_variables` in dictionary form    },)prompt\n\n\n\n\nPromptTemplate(input_variables=['country1'], input_types={}, partial_variables={'country2': 'United States of America'}, template='What are the capitals of {country1} and {country2}, respectively?')\n\nprompt.format(country1=\"South Korea\")\n\n\n\n\n'What are the capitals of South Korea and United States of America, respectively?'\n\n通过partial()函数修改或者增加临时变量, 或者直接修改 PromptTemplate.partial_variables\n\nprompt_partial = prompt.partial(country2=”India”), 可创建新实例的同时保留原实例\nprompt.partial_variables = {‘country2’:’china’}, 直接修改原实例\n\nprompt_partial = prompt.partial(country2=\"India\")prompt_partial.format(country1=\"South Korea\")\n\n\n\n\n'What are the capitals of South Korea and India, respectively?'\n\nprompt.partial_variables = {'country2':'china'}prompt.format(country1=\"South Korea\")\n\n\n\n\n'What are the capitals of South Korea and china, respectively?'\n\npartial variables 可以临时用新值, 不会影响缺失时的默认值\nprint(prompt_partial.format(country1=\"South Korea\", country2=\"Canada\"))print(prompt_partial.format(country1=\"South Korea\"))\n\nWhat are the capitals of South Korea and Canada, respectively?\nWhat are the capitals of South Korea and India, respectively?\n\npartial variables 可用函数传递, 不需要手动设置新值\nfrom datetime import datetimedef get_today():    return datetime.now().strftime(\"%B %d\")prompt = PromptTemplate(    template=\"Today's date is {today}. Please list {n} celebrities whose birthday is today. Please specify their date of birth.\",    input_variables=[\"n\"],    partial_variables={        \"today\": get_today  # Pass `partial_variables` in dictionary form    },)prompt.format(n=3)\n\n\n\n\n\"Today's date is January 30. Please list 3 celebrities whose birthday is today. Please specify their date of birth.\"\n\n从 YAML 文件加载 Prompt 模板您可以将 Prompt 模板 存储在单独的 YAML 文件 中，并使用 load_prompt 进行加载和管理。\n以下是一个yaml示例: \n\n_type: \"prompt\"template: \"What is the color of {fruit}?\"input_variables: [\"fruit\"]\n\nfrom langchain_core.prompts import load_promptprompt = load_prompt(\"prompts/fruit_color.yaml\", encoding=\"utf-8\")prompt\n\nChatPromptTemplateChatPromptTemplate 可用于将会话历史记录包含到提示词中，以提供上下文信息。消息以 (role, message) 元组的形式组织，并存储在 列表 中。\n角色（role）:\n\n\"system\" ：系统设置信息，通常用于全局指令或设定 AI 的行为。  \n\"human\" ：用户输入的消息。  \n\"ai\" ：AI 生成的响应消息。\n\nfrom langchain_core.prompts import ChatPromptTemplatechat_prompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")chat_prompt\n\n\n\n\nChatPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?'), additional_kwargs={})])\n\nChatPromptTemplate(input_variables=[‘country’], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[‘country’], input_types={}, partial_variables={}, template=’What is the capital of {country}?’), additional_kwargs={})])\n注意这个prompt被 HumanMessagePromptTemplate包装了，而且位于一个list中\nchat_prompt.format(country=\"United States of America\")\n\n\n\n\n'Human: What is the capital of United States of America?'\n\n多角色使用 ChatPromptTemplate.from_messages来定义模板, 内部是 chat list, 每个 chat 都是以 (role, message) 元组的形式组织\nfrom langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"You are a friendly AI assistant. Your name is {name}.\"),        (\"human\", \"Nice to meet you!\"),        (\"ai\", \"Hello! How can I assist you?\"),        (\"human\", \"{user_input}\"),    ])# Create chat messagesmessages = chat_template.format_messages(name=\"Teddy\", user_input=\"What is your name?\")messages\n\n\n\n\n[SystemMessage(content='You are a friendly AI assistant. Your name is Teddy.', additional_kwargs={}, response_metadata={}),\n HumanMessage(content='Nice to meet you!', additional_kwargs={}, response_metadata={}),\n AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}),\n HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n\n可直接用上面的 Message list 的形式调用大模型\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)llm.invoke(messages).content\n\n\n\n\n\"My name is Teddy. It's nice to meet you! How can I help you today?\"\n\nMessagePlaceholderLangChain 提供了 MessagePlaceholder，用途包括:\n\n当不确定使用哪些角色 作为消息提示模板的一部分时，它可以提供灵活性。  \n在格式化时插入一组消息列表，适用于动态会话历史记录的场景。\n\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderchat_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.\",        ),        MessagesPlaceholder(variable_name=\"conversation\"),        (\"human\", \"Summarize the conversation so far in {word_count} words.\"),    ])chat_prompt\n\n\n\n\nChatPromptTemplate(input_variables=['conversation', 'word_count'], input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=&lt;function _get_type at 0x7ff1a966cfe0&gt;, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='Summarize the conversation so far in {word_count} words.'), additional_kwargs={})])\n\nformatted_chat_prompt = chat_prompt.format(    word_count=5,    conversation=[        (\"human\", \"Hello! I’m Teddy. Nice to meet you.\"),        (\"ai\", \"Nice to meet you! I look forward to working with you.\"),    ],)print(formatted_chat_prompt)\n\nSystem: You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.\nHuman: Hello! I’m Teddy. Nice to meet you.\nAI: Nice to meet you! I look forward to working with you.\nHuman: Summarize the conversation so far in 5 words.\n\nFew-Shot PromptingLangChain 的 Few-Shot Prompting 提供了一种强大的框架，通过提供精心挑选的示例，引导语言模型生成高质量的输出。此技术减少了大量模型微调的需求，同时确保在各种应用场景中提供精准且符合上下文的结果。  \n\nFew-Shot Prompt 模板：  \n\n通过嵌入示例定义提示的结构和格式，指导模型生成一致的输出。\n\n\n示例选择策略（Example Selection Strategies）：  \n\n动态选择最相关的示例 以匹配特定查询，增强模型的上下文理解能力，提高响应准确性。\n\n\nChroma 向量存储（Chroma Vector Store）：  \n\n用于存储和检索基于语义相似度的示例，提供可扩展且高效的 Prompt 结构构建。\n\n\n\nFewShotPromptTemplateFew-shot prompting 是一种强大的技术，它通过提供少量精心设计的示例，引导语言模型生成准确且符合上下文的输出。LangChain 的 FewShotPromptTemplate 简化了这一过程，使用户能够构建灵活且可复用的提示，适用于问答、摘要、文本校正等任务。  \n1. 设计 Few-Shot 提示（Designing Few-Shot Prompts）  \n\n定义示例，展示所需的输出结构和风格。  \n确保示例覆盖边界情况，以增强模型的理解能力和性能。\n\n2. 动态示例选择（Dynamic Example Selection）  \n\n利用语义相似性或向量搜索，选择最相关的示例，以匹配输入查询。\n\n3. 集成 Few-Shot 提示（Integrating Few-Shot Prompts）  \n\n结合 Prompt 模板与语言模型，构建强大的链式调用，以生成高质量的响应。\n\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)# User queryquestion = \"What is the capital of United States of America?\"# Query the modelresponse = llm.invoke(question)# Print the responseprint(response.content)\n\nThe capital of the United States of America is Washington, D.C.\n\n以下是一个 CoT 的示例prompt\nfrom langchain_core.prompts import PromptTemplate, FewShotPromptTemplate# Define examples for the few-shot promptexamples = [    {        \"question\": \"Who lived longer, Steve Jobs or Einstein?\",        \"answer\": \"\"\"Does this question require additional questions: Yes.Additional Question: At what age did Steve Jobs die?Intermediate Answer: Steve Jobs died at the age of 56.Additional Question: At what age did Einstein die?Intermediate Answer: Einstein died at the age of 76.The final answer is: Einstein\"\"\",    },    {        \"question\": \"When was the founder of Naver born?\",        \"answer\": \"\"\"Does this question require additional questions: Yes.Additional Question: Who is the founder of Naver?Intermediate Answer: Naver was founded by Lee Hae-jin.Additional Question: When was Lee Hae-jin born?Intermediate Answer: Lee Hae-jin was born on June 22, 1967.The final answer is: June 22, 1967\"\"\",    },    {        \"question\": \"Who was the reigning king when Yulgok Yi's mother was born?\",        \"answer\": \"\"\"Does this question require additional questions: Yes.Additional Question: Who is Yulgok Yi's mother?Intermediate Answer: Yulgok Yi's mother is Shin Saimdang.Additional Question: When was Shin Saimdang born?Intermediate Answer: Shin Saimdang was born in 1504.Additional Question: Who was the king of Joseon in 1504?Intermediate Answer: The king of Joseon in 1504 was Yeonsangun.The final answer is: Yeonsangun\"\"\",    },    {        \"question\": \"Are the directors of Oldboy and Parasite from the same country?\",        \"answer\": \"\"\"Does this question require additional questions: Yes.Additional Question: Who is the director of Oldboy?Intermediate Answer: The director of Oldboy is Park Chan-wook.Additional Question: Which country is Park Chan-wook from?Intermediate Answer: Park Chan-wook is from South Korea.Additional Question: Who is the director of Parasite?Intermediate Answer: The director of Parasite is Bong Joon-ho.Additional Question: Which country is Bong Joon-ho from?Intermediate Answer: Bong Joon-ho is from South Korea.The final answer is: Yes\"\"\",    },]example_prompt = PromptTemplate.from_template(    \"Question:\\n{question}\\nAnswer:\\n{answer}\")# Print the first formatted exampleprint(example_prompt.format(**examples[0]))\n\nQuestion:\nWho lived longer, Steve Jobs or Einstein?\nAnswer:\nDoes this question require additional questions: Yes.\nAdditional Question: At what age did Steve Jobs die?\nIntermediate Answer: Steve Jobs died at the age of 56.\nAdditional Question: At what age did Einstein die?\nIntermediate Answer: Einstein died at the age of 76.\nThe final answer is: Einstein\n\n以下这个 FewShotPromptTemplate 将 examples 以 example_prompt 格式添加到真正 QA 的前面。真正的 QA 按照 suffix 格式展示\n# Initialize the FewShotPromptTemplatefew_shot_prompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question:\\n{question}\\nAnswer:\",    input_variables=[\"question\"],)# Example questionquestion = \"How old was Bill Gates when Google was founded?\"# Generate the final promptfinal_prompt = few_shot_prompt.format(question=question)print(final_prompt)\n\nQuestion:\nWho lived longer, Steve Jobs or Einstein?\nAnswer:\nDoes this question require additional questions: Yes.\nAdditional Question: At what age did Steve Jobs die?\nIntermediate Answer: Steve Jobs died at the age of 56.\nAdditional Question: At what age did Einstein die?\nIntermediate Answer: Einstein died at the age of 76.\nThe final answer is: Einstein\n\n​    Question:    When was the founder of Naver born?    Answer:    Does this question require additional questions: Yes.    Additional Question: Who is the founder of Naver?    Intermediate Answer: Naver was founded by Lee Hae-jin.    Additional Question: When was Lee Hae-jin born?    Intermediate Answer: Lee Hae-jin was born on June 22, 1967.    The final answer is: June 22, 1967\n​    Question:    Who was the reigning king when Yulgok Yi’s mother was born?    Answer:    Does this question require additional questions: Yes.    Additional Question: Who is Yulgok Yi’s mother?    Intermediate Answer: Yulgok Yi’s mother is Shin Saimdang.    Additional Question: When was Shin Saimdang born?    Intermediate Answer: Shin Saimdang was born in 1504.    Additional Question: Who was the king of Joseon in 1504?    Intermediate Answer: The king of Joseon in 1504 was Yeonsangun.    The final answer is: Yeonsangun\n​    Question:    Are the directors of Oldboy and Parasite from the same country?    Answer:    Does this question require additional questions: Yes.    Additional Question: Who is the director of Oldboy?    Intermediate Answer: The director of Oldboy is Park Chan-wook.    Additional Question: Which country is Park Chan-wook from?    Intermediate Answer: Park Chan-wook is from South Korea.    Additional Question: Who is the director of Parasite?    Intermediate Answer: The director of Parasite is Bong Joon-ho.    Additional Question: Which country is Bong Joon-ho from?    Intermediate Answer: Bong Joon-ho is from South Korea.    The final answer is: Yes\n​    Question:    How old was Bill Gates when Google was founded?    Answer:\nresponse = llm.invoke(final_prompt)print(response.content)\n\nDoes this question require additional questions: Yes.\nAdditional Question: When was Google founded?\nIntermediate Answer: Google was founded in 1998.\nAdditional Question: When was Bill Gates born?\nIntermediate Answer: Bill Gates was born on October 28, 1955.\nThe final answer is: Bill Gates was 43 years old when Google was founded.\n\n特殊 promptRAG 文档分析基于检索到的文档上下文处理并回答问题，确保高准确性和高相关性。\nfrom langchain.prompts import ChatPromptTemplatesystem = \"\"\"You are a precise and helpful AI assistant specializing in question-answering tasks based on provided context.Your primary task is to:1. Analyze the provided context thoroughly2. Answer questions using ONLY the information from the context3. Preserve technical terms and proper nouns exactly as they appear4. If the answer cannot be found in the context, respond with: 'The provided context does not contain information to answer this question.'5. Format responses in clear, readable paragraphs with relevant examples when available6. Focus on accuracy and clarity in your responses\"\"\"human = \"\"\"#Question:{question}#Context:{context}#Answer:Please provide a focused, accurate response that directly addresses the question using only the information from the provided context.\"\"\"prompt = ChatPromptTemplate.from_messages(\t[\t\t(\"system\", system), \t\t(\"human\", human)\t])prompt\n\n\n\n\nChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are a precise and helpful AI assistant specializing in question-answering tasks based on provided context.\\nYour primary task is to:\\n1. Analyze the provided context thoroughly\\n2. Answer questions using ONLY the information from the context\\n3. Preserve technical terms and proper nouns exactly as they appear\\n4. If the answer cannot be found in the context, respond with: 'The provided context does not contain information to answer this question.'\\n5. Format responses in clear, readable paragraphs with relevant examples when available\\n6. Focus on accuracy and clarity in your responses\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='#Question:\\n{question}\\n\\n#Context:\\n{context}\\n\\n#Answer:\\nPlease provide a focused, accurate response that directly addresses the question using only the information from the provided context.'), additional_kwargs={})])\n\n具有来源归因的 RAG（RAG with Source Attribution）增强型 RAG 实现，支持详细的来源追踪和引用，以提高可追溯性和验证可靠性。\nfrom langchain.prompts import ChatPromptTemplatesystem = \"\"\"You are a precise and thorough AI assistant that provides well-documented answers with source attribution.Your responsibilities include:1. Analyzing provided context thoroughly2. Generating accurate answers based solely on the given context3. Including specific source references for each key point4. Preserving technical terminology exactly as presented5. Maintaining clear citation format [source: page/document]6. If information is not found in the context, state: 'The provided context does not contain information to answer this question.'Format your response as:1. Main Answer2. Sources Used (with specific locations)3. Confidence Level (High/Medium/Low)\"\"\"human = \"\"\"#Question:{question}#Context:{context}#Answer:Please provide a detailed response with source citations using only information from the provided context.\"\"\"prompt = ChatPromptTemplate.from_messages(\t[\t\t(\"system\", system), \t\t(\"human\", human)\t])PROMPT_OWNER = \"eun\"hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, new_repo_is_public=True)\n\n其实在回答要求里加入了源引用的要求\nLLM 响应评估（LLM Response Evaluation）基于多项质量指标对 LLM 响应进行全面评估，并提供详细的评分方法。\nfrom langchain.prompts import PromptTemplateevaluation_prompt = \"\"\"Evaluate the LLM's response based on the following criteria:INPUT:Question: {question}Context: {context}LLM Response: {answer}EVALUATION CRITERIA:1. Accuracy (0-10)- Perfect (10): Completely accurate, perfectly aligned with context- Good (7-9): Minor inaccuracies- Fair (4-6): Some significant inaccuracies- Poor (0-3): Major inaccuracies or misalignment2. Completeness (0-10)- Perfect (10): Comprehensive coverage of all relevant points- Good (7-9): Covers most important points- Fair (4-6): Missing several key points- Poor (0-3): Critically incomplete3. Context Relevance (0-10)- Perfect (10): Optimal use of context- Good (7-9): Good use with minor omissions- Fair (4-6): Partial use of relevant context- Poor (0-3): Poor context utilization4. Clarity (0-10)- Perfect (10): Exceptionally clear and well-structured- Good (7-9): Clear with minor issues- Fair (4-6): Somewhat unclear- Poor (0-3): Confusing or poorly structuredSCORING METHOD:1. Calculate individual scores2. Compute weighted average:   - Accuracy: 40%   - Completeness: 25%   - Context Relevance: 25%   - Clarity: 10%3. Normalize to 0-1 scaleOUTPUT FORMAT:{    \"individual_scores\": {        \"accuracy\": float,        \"completeness\": float,        \"context_relevance\": float,        \"clarity\": float    },    \"weighted_score\": float,    \"normalized_score\": float,    \"evaluation_notes\": string}Return ONLY the normalized_score as a decimal between 0 and 1.\"\"\"prompt = PromptTemplate.from_template(evaluation_prompt)","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 4.模型","url":"/2025/05/04/langchain-tutorial-series/04.Model/","content":"VLLM 部署本地大模型\n单卡部署\n\nexport CUDA_VISIBLE_DEVICES=5modelpath=../DataCollection/officials/Qwen2.5-7B-Instructmodelname=Qwen2.5-7B-Instructnohup python -m vllm.entrypoints.openai.api_server \\    --model $modelpath \\    --served-model-name $modelname \\    --port 5551 \\    --gpu-memory-utilization 0.4 \\    --dtype=half \\\t// 不建议加，现在模型默认都是fp16或者bf16，half会强制转换为fp16，没必要    &gt; output.log 2&gt;&amp;1 &amp;\n\n\n多卡部署\n\nexport CUDA_VISIBLE_DEVICES=2,3modelpath=../DataCollection/officials/Qwen2.5-7B-Instructmodelname=Qwen2.5-7B-Instructnohup python -m vllm.entrypoints.openai.api_server \\    --model $modelpath \\    --served-model-name $modelname \\    --port 5551 \\    --gpu-memory-utilization 0.4 \\\t--tensor_parallel_size 2 \\\t// !!!!!占卡数量，不可能是计数!!!!!    &gt; output.log 2&gt;&amp;1 &amp;\n\n基本模型选项以下是 API 的基本选项：  \n\nmodel_name : str该选项允许您选择适用的模型，也可以使用 model 作为别名。  \n\ntemperature : float = 0.7该选项用于设置采样温度（temperature）。取值范围为 0 到 2，较高的值（如 0.8）会使输出更加随机，而较低的值（如 0.2）会使输出更具集中性和确定性。  \n\nmax_tokens : int | None = None指定聊天补全（chat completion）中要生成的最大 token 数。该选项控制模型在一次调用中可以生成的文本长度。\n\n\nfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)query = \"Tell me one joke about Computer Science\"# Stream the response instead of invoking it directlyresponse = model.stream(query)# Print the streamed response token by tokenfor token in response:    print(token.content, end=\"\", flush=True)\n\nSure! Here's a light-hearted joke about computer science:\n\nWhy did the computer go to the doctor?\n\nBecause it had a virus and needed to get \"anti-virus\"!\n\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 5.文档加载","url":"/2025/05/04/langchain-tutorial-series/05.DocumentLoader/","content":"Document用于存储一段文本及其相关元数据的类。  \n\npage_content （必需）：以字符串形式存储一段文本。  \nmetadata （可选）：以字典形式存储与 page_content 相关的元数据。\n\nfrom langchain_core.documents import Documentdocument = Document(page_content=\"Hello, welcome to LangChain Open Tutorial!\")document\n\n\n\n\nDocument(metadata={}, page_content='Hello, welcome to LangChain Open Tutorial!')\n\n文档加载器（Document Loader）文档加载器是一个用于从各种来源加载 Document 的类。  \n以下是一些常见的文档加载器示例：  \n\nPyPDFLoader ：加载 PDF 文件  \nCSVLoader ：加载 CSV 文件  \nUnstructuredHTMLLoader ：加载 HTML 文件  \nJSONLoader ：加载 JSON 文件  \nTextLoader ：加载纯文本文件  \nDirectoryLoader ：从目录中批量加载文档\n\nfrom langchain_community.document_loaders import PyPDFLoader# Set up the loaderFILE_PATH = \"./data/01-document-loader-sample.pdf\"loader = PyPDFLoader(FILE_PATH)\n\nload()\n加载文档，并以 list[Document] 的形式返回。\n\ndocs = loader.load()print(len(docs))print('-'*3)docs[0:2]\n\n48\n---\n\n\n\n\n\n[Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 0}, page_content=' \\n \\n \\nOctober 2016 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTHE NATIONAL  \\nARTIFICIAL INTELLIGENCE \\nRESEARCH AND DEVELOPMENT \\nSTRATEGIC PLAN  \\nNational Science and Technology Council \\n \\nNetworking and Information Technology \\nResearch and Development Subcommittee \\n '),\n Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 1}, page_content=' ii \\n \\n ')]\n\naload()\n异步加载文档，并以 list[Document] 的形式返回。\n\n# Load Documents asynchronouslydocs = await loader.aload()\n\nlazy_load()\n顺序加载文档，并以 Iterator[Document] 的形式返回。\n\ndocs = loader.lazy_load()for doc in docs:    print(doc.metadata)    break  # Used to limit the output length\n\nalazy_load()\n异步顺序加载文档，并以 AsyncIterator[Document] 的形式返回。\n\n可以观察到，这种方法作为一个 async_generator 工作。它是一种特殊类型的异步迭代器，能够按需生成值，而不需要一次性将所有值存储在内存中。\nloader.alazy_load()docs = loader.alazy_load()async for doc in docs:    print(doc.metadata)    break  # Used to limit the output length\n\nload_and_split()\n加载文档，并使用 TextSplitter 自动拆分为多个文本块，最终以 list[Document] 的形式返回。\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Set up the TextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)# Split Documents into chunksdocs = loader.load_and_split(text_splitter=text_splitter)print(len(docs))docs[0:3]\n\n1430\n\n\n\n\n\n[Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 0}, page_content='October 2016 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTHE NATIONAL  \\nARTIFICIAL INTELLIGENCE \\nRESEARCH AND DEVELOPMENT \\nSTRATEGIC PLAN'),\n Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 0}, page_content='National Science and Technology Council \\n \\nNetworking and Information Technology \\nResearch and Development Subcommittee'),\n Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 1}, page_content='ii')]\n\nPDF LoaderPyPDF 是最广泛使用的 Python 库之一，用于 PDF 处理。\n在这里，我们使用 pypdf 将 PDF 加载为文档数组，每个文档包含一个 page 页码，并包含页面内容和元数据。\nLangChain 的 PyPDFLoader 集成了 PyPDF，将 PDF 文档解析为 LangChain 的 Document 对象。\nfrom langchain_community.document_loaders import PyPDFLoader# Initialize the PDF loaderFILE_PATH = \"./data/01-document-loader-sample.pdf\"loader = PyPDFLoader(FILE_PATH)# Load data into Document objectsdocs = loader.load()# Print the contents of the documentprint(docs[10].page_content[:300])\n\nNATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN \n \n 3 \nExecutive Summary \nArtificial intelligence (AI) is a transformative technology that holds promise for tremendous societal and \neconomic benefit. AI has the potential to revolutionize how we live, work, learn, discover, a\n\nPyPDF(OCR)有些 PDF 包含扫描文档或图片中的文本图像。你也可以使用 rapidocr-onnxruntime 包从图像中提取文本。\nloader = PyPDFLoader(FILE_PATH, extract_images=True)docs = loader.load()\n\nPyPDF Directory从目录中导入所有 PDF 文档。\nfrom langchain_community.document_loaders import PyPDFDirectoryLoader# directory pathloader = PyPDFDirectoryLoader(\"./data/\")# load documentsdocs = loader.load()# print the number of documentsprint(len(docs))\n\n96\n\nPyMuPDFPyMuPDF 经过速度优化，并提供关于 PDF 及其页面的详细元数据。它每页返回一个文档。  \nLangChain 的 PyMuPDFLoader 集成了 PyMuPDF，可将 PDF 文档解析为 LangChain 的 Document 对象。\nfrom langchain_community.document_loaders import PyMuPDFLoader# create an instance of the PyMuPDF loaderFILE_PATH = \"./data/01-document-loader-sample.pdf\"loader = PyMuPDFLoader(FILE_PATH)# load the documentdocs = loader.load()# print the contents of the documentprint(len(docs))\n\n48\n\n\n/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_community/document_loaders/parsers/pdf.py:322: UserWarning: Warning: Empty content on page 4 of document ./data/01-document-loader-sample.pdf\n  warnings.warn(\n\nWebBaseLoaderWebBaseLoader 是 LangChain 中一个专门用于处理基于网页内容的文档加载器。\n它利用 BeautifulSoup4 库有效地解析网页，并通过 SoupStrainer 和其他 bs4 参数提供可自定义的解析选项。\nimport bs4from langchain_community.document_loaders import WebBaseLoader# Load news article content using WebBaseLoaderloader = WebBaseLoader(   web_paths=(\t\"https://blog.csdn.net/wait_for_taht_day5/article/details/50570827\",\t\"https://blog.csdn.net/teethfairy/article/details/7287307\"\t),   encoding='utf-8')# Load and process the documentsdocs = loader.load()print(f\"Number of documents: {len(docs)}\")import reprint(re.sub(r'\\n+', '\\n', docs[0].page_content)[:100])\n\nNumber of documents: 2\n \n【Python】Hello World 输入输出_python你好世界输入,输出英文-CSDN博客\n【Python】Hello World 输入输出\n最新推荐文章于 2024-12-04 08:5\n\nCSV Loaderfrom langchain_community.document_loaders.csv_loader import CSVLoader# Create CSVLoader instanceloader = CSVLoader(file_path=\"./data/titanic.csv\")# Load documentsdocs = loader.load()print(docs[0])\n\npage_content='PassengerId: 1\nSurvived: 0\nPclass: 3\nName: Braund, Mr. Owen Harris\nSex: male\nAge: 22\nSibSp: 1\nParch: 0\nTicket: A/5 21171\nFare: 7.25\nCabin: \nEmbarked: S' metadata={'source': './data/titanic.csv', 'row': 0}\n\ndocs[0].page_content\n\n\n\n\n'PassengerId: 1\\nSurvived: 0\\nPclass: 3\\nName: Braund, Mr. Owen Harris\\nSex: male\\nAge: 22\\nSibSp: 1\\nParch: 0\\nTicket: A/5 21171\\nFare: 7.25\\nCabin: \\nEmbarked: S'\n\n它会读取 header 然后把每行数据重新组织\n原始数据\nPassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked 1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n\nPassengerId: 1\\nSurvived: 0\\nPclass: 3\\nName: Braund, Mr. Owen Harris\\nSex: male\\nAge: 22\\nSibSp: 1\\nParch: 0\\nTicket: A/5 21171\\nFare: 7.25\\nCabin: \\nEmbarked: S\n\n\n自定义 CSV 解析和加载CSVLoader 接受一个 csv_args 关键字参数，用于定制传递给 Python 的 csv.DictReader 的参数。这使得你可以处理各种 CSV 格式，如自定义分隔符、引号字符或特定的换行符处理。\n有关支持的 csv_args 及如何根据你的特定需求定制解析的更多信息，请参见 Python 的 csv 模块 文档。\nloader = CSVLoader(    file_path=\"./data/titanic.csv\",    csv_args={        \"delimiter\": \",\",        \"quotechar\": '\"',        \"fieldnames\": [            \"Passenger ID\",            \"Survival (1: Survived, 0: Died)\",            \"Passenger Class\",            \"Name\",            \"Sex\",            \"Age\",            \"Number of Siblings/Spouses Aboard\",            \"Number of Parents/Children Aboard\",            \"Ticket Number\",            \"Fare\",            \"Cabin\",            \"Port of Embarkation\",        ],    },)docs = loader.load()print(docs[1].page_content)\n\nPassenger ID: 1\nSurvival (1: Survived, 0: Died): 0\nPassenger Class: 3\nName: Braund, Mr. Owen Harris\nSex: male\nAge: 22\nNumber of Siblings/Spouses Aboard: 1\nNumber of Parents/Children Aboard: 0\nTicket Number: A/5 21171\nFare: 7.25\nCabin: \nPort of Embarkation: S\n\nloader = CSVLoader(    file_path=\"./data/titanic.csv\",    csv_args={        \"delimiter\": \",\",        \"quotechar\": '\"',        \"fieldnames\": [            \"Passenger ID\",            \"Survival (1: Survived, 0: Died)\",            \"Passenger Class\",            \"Name\",            \"Sex\",            \"Age\",        ],    },)docs = loader.load()print(docs[1].page_content)\n\nPassenger ID: 1\nSurvival (1: Survived, 0: Died): 0\nPassenger Class: 3\nName: Braund, Mr. Owen Harris\nSex: male\nAge: 22\nNone: 1,0,A/5 21171,7.25,,S\n\n这些参数其实没啥好解释的, fieldnames 不是用于选择列的, 而是重新命名的, 如果部分列没命名, 那会分到 None 列\n你应该使用 source_column 参数来指定每一行生成文档的来源。否则，file_path 将作为所有从 CSV 文件创建的文档的来源。\n当在一个用于基于源信息回答问题的链中使用从 CSV 文件加载的文档时，这一点尤其有用。\nloader = CSVLoader(    file_path=\"./data/titanic.csv\",    source_column=\"PassengerId\",  # Specify the source column)docs = loader.load()  print(docs[1])\n\npage_content='PassengerId: 2\nSurvived: 1\nPclass: 1\nName: Cumings, Mrs. John Bradley (Florence Briggs Thayer)\nSex: female\nAge: 38\nSibSp: 1\nParch: 0\nTicket: PC 17599\nFare: 71.2833\nCabin: C85\nEmbarked: C' metadata={'source': '2', 'row': 1}\n\n注意 metadata.source 变成了对应的 PassengerId\nDataFrameLoaderPandas 是一个开源的数据分析和处理工具，专为 Python 编程语言设计。该库在数据科学、机器学习以及多个领域中广泛应用，用于处理各种数据。\nLangChain 的 DataFrameLoader 是一个强大的工具，旨在无缝地将 Pandas DataFrame 集成到 LangChain 工作流中。\nfrom langchain_community.document_loaders import DataFrameLoaderimport pandas as pddf = pd.read_csv(\"./data/titanic.csv\")# df = pd.read_excel(\"./data/titanic.xlsx\")print(df.head(n=5))# The Name column of the DataFrame is specified to be used as the content of each document.loader = DataFrameLoader(df, page_content_column=\"Name\")docs = loader.load()print(docs[0].page_content)\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \nBraund, Mr. Owen Harris\n\n可通过 page_content_column 来指定哪些 dataframe 的列被读取. csv 和 excel 文件都可以用 DataFrameLoader 加载\nDocx2txtLoader\n采用轻量级的 Python 模块 docx2txt 进行文本提取。  \n快速、简单地从 .docx 文件中提取文本。  \n适用于高效且直接的文本处理任务。\n\nfrom langchain_community.document_loaders import Docx2txtLoader# Initialize the document loaderloader = Docx2txtLoader(\"data/sample-word-document_eng.docx\")# Load the documentdocs = loader.load()# Print the metadata of the documentprint(f\"Document Metadata: {docs[0].metadata}\\n\")# Note: The entire docx file is converted into a single document.# It needs to be split into smaller parts using a text splitter.print('-'*5)print(docs[0].page_content[:100])\n\nDocument Metadata: {'source': 'data/sample-word-document_eng.docx'}\n\n-----\nSemantic Search\n\n\n\n\nDefinition: Semantic search is a search methodology that goes beyond simple keywo\n\n不像 PDF Loader 会默认根据页切分文档, Docx2txtLoader 会把一个文件读取为一个 Document\nTXT Loaderfrom langchain_community.document_loaders import TextLoader# Create a text loaderloader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")# Load the documentdocs = loader.load()print(f\"Number of documents: {len(docs)}\\n\")print(\"[Metadata]\\n\")print(docs[0].metadata)print(\"\\n========= [Preview - First 500 Characters] =========\\n\")print(docs[0].page_content[:500])\n\nNumber of documents: 1\n\n[Metadata]\n\n{'source': 'data/appendix-keywords.txt'}\n\n========= [Preview - First 500 Characters] =========\n\nSemantic Search\n\nDefinition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\nExample: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\nRelated Keywords: Natural Language Processing, Search Algorithms, Data Mining\n\nEmbedding\n\nDefinition: Embedding is the process of converting textual data, such as words\n\nJSONLoaderfrom langchain_community.document_loaders import JSONLoader# Create JSONLoaderloader = JSONLoader(    file_path=\"data/people.json\",    jq_schema=\".people[]\",  # Access each item in the people array    text_content=False,)# Example: extract only contact_details# loader = JSONLoader(#     file_path=\"data/people.json\",#     jq_schema=\".people[].contact_details\",#     text_content=False,# )# Or extract only hobbies from personal_preferences# loader = JSONLoader(#     file_path=\"data/people.json\",#     jq_schema=\".people[].personal_preferences.hobbies\",#     text_content=False,# )# Load documentsdocs = loader.load()docs\n\n\n\n\n[Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 1}, page_content='{\"name\": {\"first\": \"Alice\", \"last\": \"Johnson\"}, \"age\": 28, \"contact\": {\"email\": \"alice.johnson@example.com\", \"phone\": \"+1-555-0123\", \"social_media\": {\"twitter\": \"@alice_j\", \"linkedin\": \"linkedin.com/in/alicejohnson\"}}, \"address\": {\"street\": \"123 Maple St\", \"city\": \"Springfield\", \"state\": \"IL\", \"zip\": \"62704\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Reading\", \"Hiking\", \"Cooking\"], \"favorite_food\": \"Italian\", \"music_genre\": \"Jazz\", \"travel_destinations\": [\"Japan\", \"Italy\", \"Canada\"]}, \"interesting_fact\": \"Alice has traveled to over 15 countries and speaks 3 languages.\"}'),\n Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 2}, page_content='{\"name\": {\"first\": \"Bob\", \"last\": \"Smith\"}, \"age\": 34, \"contact\": {\"email\": \"bob.smith@example.com\", \"phone\": \"+1-555-0456\", \"social_media\": {\"twitter\": \"@bobsmith34\", \"linkedin\": \"linkedin.com/in/bobsmith\"}}, \"address\": {\"street\": \"456 Oak Ave\", \"city\": \"Metropolis\", \"state\": \"NY\", \"zip\": \"10001\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Photography\", \"Cycling\", \"Video Games\"], \"favorite_food\": \"Mexican\", \"music_genre\": \"Rock\", \"travel_destinations\": [\"Brazil\", \"Australia\", \"Germany\"]}, \"interesting_fact\": \"Bob is an avid gamer and has competed in several national tournaments.\"}'),\n Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 3}, page_content='{\"name\": {\"first\": \"Charlie\", \"last\": \"Davis\"}, \"age\": 45, \"contact\": {\"email\": \"charlie.davis@example.com\", \"phone\": \"+1-555-0789\", \"social_media\": {\"twitter\": \"@charliedavis45\", \"linkedin\": \"linkedin.com/in/charliedavis\"}}, \"address\": {\"street\": \"789 Pine Rd\", \"city\": \"Gotham\", \"state\": \"NJ\", \"zip\": \"07001\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Gardening\", \"Fishing\", \"Woodworking\"], \"favorite_food\": \"Barbecue\", \"music_genre\": \"Country\", \"travel_destinations\": [\"Canada\", \"New Zealand\", \"Norway\"]}, \"interesting_fact\": \"Charlie has a small farm where he raises chickens and grows organic vegetables.\"}'),\n Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 4}, page_content='{\"name\": {\"first\": \"Dana\", \"last\": \"Lee\"}, \"age\": 22, \"contact\": {\"email\": \"dana.lee@example.com\", \"phone\": \"+1-555-0111\", \"social_media\": {\"twitter\": \"@danalee22\", \"linkedin\": \"linkedin.com/in/danalee\"}}, \"address\": {\"street\": \"234 Birch Blvd\", \"city\": \"Star City\", \"state\": \"CA\", \"zip\": \"90001\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Dancing\", \"Sketching\", \"Traveling\"], \"favorite_food\": \"Thai\", \"music_genre\": \"Pop\", \"travel_destinations\": [\"Thailand\", \"France\", \"Spain\"]}, \"interesting_fact\": \"Dana is a dance instructor and has won several local competitions.\"}'),\n Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 5}, page_content='{\"name\": {\"first\": \"Ethan\", \"last\": \"Garcia\"}, \"age\": 31, \"contact\": {\"email\": \"ethan.garcia@example.com\", \"phone\": \"+1-555-0999\", \"social_media\": {\"twitter\": \"@ethangarcia31\", \"linkedin\": \"linkedin.com/in/ethangarcia\"}}, \"address\": {\"street\": \"345 Cedar St\", \"city\": \"Central City\", \"state\": \"TX\", \"zip\": \"75001\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Running\", \"Travel Blogging\", \"Cooking\"], \"favorite_food\": \"Indian\", \"music_genre\": \"Hip-Hop\", \"travel_destinations\": [\"India\", \"Italy\", \"Mexico\"]}, \"interesting_fact\": \"Ethan runs a popular travel blog where he shares his adventures and culinary experiences.\"}')]\n\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 6.文档切分","url":"/2025/05/04/langchain-tutorial-series/06.TextSplitter/","content":"Character Text SplitterCharacterTextSplitter 提供了高效的文本切分功能，带来了以下几个关键优势：\n\nToken 限制： 克服了 LLM 上下文窗口大小的限制。\n搜索优化： 实现更精确的基于块的检索。\n内存效率： 有效处理大规模文档。\n上下文保持： 通过 chunk_overlap 保持文本的连贯性。\n\nCharacterTextSplitter 是通过将文本按指定的分隔符拆分成多个块来进行切分的。它的几个重要参数如下:\n\n分隔符 (separator)：首先，CharacterTextSplitter 会按照用户指定的分隔符（如换行符、空格、逗号等）拆分文本。分隔符可以是一个简单的字符串，也可以是一个正则表达式。\n\n块大小 (chunk_size)：每个生成的块会被限制在一个最大大小（chunk_size）内，超出这个大小的文本会被分割成新的块。\n\n重叠部分 (chunk_overlap)：为了保持上下文的连贯性，CharacterTextSplitter 可以在相邻块之间保持一定的字符重叠。重叠的字符数由 chunk_overlap 参数指定，通常用来避免分割导致的上下文丢失。\n\n\n它的切分方式主要是基于字符级别的，具体步骤如下：\n\n从文本的开头开始，按照指定的分隔符拆分文本。\n每当拆分一个块时，会检查该块的字符数是否超过了 chunk_size。如果超过，则将其拆成更小的块，直到所有块的字符数都在 chunk_size 限制内。\n若在块的边界有字符重叠（由 chunk_overlap 控制），则相邻块会共享这些重叠字符。\n\nfrom langchain_community.document_loaders import TextLoaderloader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")docs = loader.load()\n\n创建 CharacterTextSplitter 并设置以下参数：\n\nseparator: 用于拆分文本的字符串（例如，换行符、空格、自定义分隔符）\nchunk_size: 返回的每个块的最大大小\nchunk_overlap: 块与块之间的字符重叠部分\nlength_function: 用于测量每个块长度的函数\nis_separator_regex: 布尔值，指示分隔符是否应作为正则表达式模式处理\n\nfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter(   separator=\" \",           # Splits whenever a space is encountered in text   chunk_size=250,          # Each chunk contains maximum 250 characters   chunk_overlap=50,        # Two consecutive chunks share 50 characters   length_function=len,     # Counts total characters in each chunk   is_separator_regex=False # Uses space as literal separator, not as regex)\n\n\n\n\n['Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.']\n\n\n直接拆分 Document 实例\n\ndoc_splits = text_splitter.split_documents(docs)print(len(doc_splits))\n\n\n拆分 text 为 Document\n\nmetadatas = [   {\"document\": 1},   {\"document\": 2},]texts = ['Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.'] * 2documents = text_splitter.create_documents(   texts=texts,  # List of texts to split   metadatas=metadatas,  # Corresponding metadata)\n\n\n只拆分 text\n\ntext_splits = text_splitter.split_text('Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.')\n\nRecursiveCharacterTextSplitter这是推荐的文本切分方式。\n该方法通过接收一个字符列表作为参数来工作。它尝试按照给定字符列表的顺序将文本拆分成更小的片段，直到这些片段变得非常小。默认情况下，字符列表为 [‘\\n\\n’, ‘\\n’, ‘ ‘, ‘,’]。它会递归地按照以下顺序进行拆分：段落 -&gt; 句子 -&gt; 单词。这意味着段落（然后是句子，再然后是单词）被视为最具语义关联性的文本片段，因此我们尽量将它们保持在一起。\n具体拆分方式：\n\n首先按段落切分：它会尝试用 \\n\\n（段落分隔符）将文本分割成较小的块。如果文本块的长度超出了 chunk_size 限制，继续向下进入下一步。\n\n按换行符切分：如果按段落切分后仍然没有满足 chunk_size 限制，文本将继续使用换行符 \\n 进行进一步的切分。\n\n按其他分隔符切分：如果仍然需要进一步缩小块的大小，它会根据设定的其他分隔符（如空格、逗号等）进行切分。\n\n按句子或单词切分：如果文本块仍然过大，最后可能会按句子或单词进行切分，直到满足 chunk_size 限制。\n\n\n通过这种递归方式，RecursiveCharacterTextSplitter 确保了文本被合理切分，同时尽可能保持语义上的一致性。\n总结来说，这种拆分方式会优先保留语义上关联性较强的部分（如段落、句子和单词），以确保文本在拆分后仍然保持较强的上下文关联性。\n使用 RecursiveCharacterTextSplitter 将文本拆分为小块的示例：\n\n将 chunk_size 设置为 250，以限制每个块的大小。\n将 chunk_overlap 设置为 50，以允许相邻块之间有 50 个字符的重叠。\n使用 len 函数作为 length_function 来计算文本的长度。\n将 is_separator_regex 设置为 False，以禁用正则表达式作为分隔符的使用。\n\nfrom langchain_community.document_loaders import TextLoaderloader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")docs = loader.load()\n\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    # Set the chunk size to very small. These settings are for illustrative purposes only.    chunk_size=250,    # Sets the number of overlapping characters between chunks.    chunk_overlap=50,    # Specifies a function to calculate the length of the string.    length_function=len,    # Sets whether to use regular expressions as delimiters.    is_separator_regex=False,)\n\n切分方式还是有类似的三种\n# 1.doc_splits = text_splitter.split_documents(docs)# 2.doc_splits = text_splitter.create_documents(\ttexts=['*'*20]*2,\tmetadatas=[{\"document\": i} for i in range(2)])# 3.text_splits = text_splitter.split_text(docs[0].page_content)\n\nText Splitting Methods in NLP\n基于词元的切分  \n\nTiktoken：OpenAI的高性能BPE（字节对编码）分词器  \nHugging Face tokenizers：针对各种预训练模型的分词器\n\n\n基于句子的切分  \n\nSentenceTransformers：在保持语义一致性的情况下对文本进行切分  \nNLTK：基于自然语言处理的句子和单词切分  \nspaCy：利用先进的语言处理能力进行文本切分\n\n\n语言特定的工具  \n\nKoNLPy：用于韩文文本处理的专业切分工具\n\n\n\n每个工具都有其独特的特点和优势：  \n\nTiktoken 提供快速的处理速度，并与OpenAI模型兼容  \nSentenceTransformers 提供基于语义的句子切分  \nNLTK 和 spaCy 实现了基于语言学规则的切分  \nKoNLPy 专注于韩文的形态学分析和切分\n\ntiktokentiktoken 是OpenAI创建的一个快速BPE（字节对编码）分词器。\nfrom langchain_community.document_loaders import TextLoaderloader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")docs = loader.load()\n\n\nfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    # Set the chunk size to 300.    chunk_size=300,    # Ensure there is no overlap between chunks.    chunk_overlap=50,)# Split the file text into chunks.doc_splits = text_splitter.split_documents(docs)\n\nCharacterTextSplitter.from_tiktoken_encoder 和 CharacterTextSplitter 的主要区别在于它们如何计算文本的“长度”或“单位”：\n\nCharacterTextSplitter:\n\n计算的是字符数量。它按字符的数量来切分文本。\n使用 chunk_size 和 chunk_overlap 参数时，切分的依据是字符数，而不是 token 数量。\n适用于不涉及复杂文本编码的场景，尤其是处理纯文本时。\n\n\nCharacterTextSplitter.from_tiktoken_encoder:\n\n计算的是 token 数量，而不是字符数量。\n这个方法使用了 tiktoken 编码器来计算文本中的 token 数量，因此它可以准确处理各种文本编码方式，尤其是在与语言模型（如OpenAI的GPT模型）交互时，token数量更为关键。\n适用于需要考虑 token 数量限制的场景，特别是当你要确保文本能够适配特定模型的输入大小时。\n\n\n\n简单来说，CharacterTextSplitter 按字符进行切分，而 CharacterTextSplitter.from_tiktoken_encoder 按 token 进行切分，后者更适合用于与大型语言模型交互时的文本预处理。\n\n使用 CharacterTextSplitter.from_tiktoken_encoder 时，文本仅由 CharacterTextSplitter 进行切分，Tiktoken 分词器仅用于测量和合并切分后的文本。（这意味着切分后的文本可能会超过 Tiktoken 分词器测量的 chunk 大小。）\n使用 RecursiveCharacterTextSplitter.from_tiktoken_encoder 时，确保切分后的文本不会超过语言模型允许的 chunk 大小。如果切分后的文本超过此大小，则会进行递归切分。此外，您还可以直接加载 Tiktoken 分词器，它会确保每个切分后的文本都小于 chunk 大小。\n\n注意他们都不是直接基于 token 来切分的, 只用于测量文本长度\nTokenTextSplitter使用 TokenTextSplitter 类将文本分割成基于 token 的块。\nfrom langchain_text_splitters import TokenTextSplitterfrom langchain_community.document_loaders import TextLoaderloader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")docs = loader.load()text_splitter = TokenTextSplitter(    chunk_size=200,  # Set the chunk size to 10.    chunk_overlap=50,  # Set the overlap between chunks to 0.)# Split the state_of_the_union text into chunks.doc_splits = text_splitter.split_documents(docs)print(doc_splits[0].page_content)  # Print the first chunk of the divided text.\n\nSemantic Search\n\nDefinition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\nExample: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\nRelated Keywords: Natural Language Processing, Search Algorithms, Data Mining\n\nEmbedding\n\nDefinition: Embedding is the process of converting textual data, such as words or sentences, into low-dimensional continuous vectors. This allows computers to better understand and process the text.\nExample: The word “apple” might be represented as a vector like [0.65, -0.23, 0.17].\nRelated Keywords: Natural Language Processing, Vectorization, Deep Learning\n\nToken\n\nDefinition: A token refers to a smaller unit of text obtained\n\nspaCyspaCy 是一个用于高级自然语言处理的开源软件库，使用 Python 和 Cython 编程语言编写。\n除了 NLTK，另一个替代方案是使用 spaCy 分词器。\n\n文本如何被分割：文本使用 spaCy 分词器进行分割。\n块的大小如何测量：它通过字符数来测量。\n\n执行以下代码前需提前下载 SpaCy en_core_web_sm 模型\npython -m spacy download en_core_web_sm --quiet\nimport warningsfrom langchain_text_splitters import SpacyTextSplitterfrom langchain_community.document_loaders import TextLoaderloader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")docs = loader.load()# Ignore  warning messages.warnings.filterwarnings(\"ignore\")# Create the SpacyTextSplitter.text_splitter = SpacyTextSplitter(    chunk_size=200,  # Set the chunk size to 200.    chunk_overlap=50,  # Set the overlap between chunks to 50.)doc_splits = text_splitter.split_documents(docs)print(doc_splits[0].page_content)\n\nSentenceTransformersSentenceTransformersTokenTextSplitter 是一个专门为 sentence-transformer 模型设计的文本切分器。\n它的默认行为是将文本分割成适合当前使用的 sentence-transformer 模型的 token 窗口大小的块。\nfrom langchain_text_splitters import SentenceTransformersTokenTextSplitterfrom langchain_community.document_loaders import TextLoaderloader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")docs = loader.load()# Create a sentence splitter and set the overlap between chunks to 50.import osos.environ['CUDA_VISIBLE_DEVICES'] = '4'splitter = SentenceTransformersTokenTextSplitter(\tchunk_size=200, \tchunk_overlap=50, \tmodel_name='../DataCollection/officials/bge-large-zh-v1.5/'\t)doc_splits = splitter.split_documents(docs)print(doc_splits[0].page_content)\n\n\n# the number of start and stop tokens is 2.text_token_count = splitter.count_tokens(text=doc_splits[0].page_content) - 2print(text_token_count)\n\nHuggingFace TokenizerHuggingFace 提供了各种分词器。\n这段代码展示了如何使用 HuggingFace 的 AutoTokenizer 分词器来计算文本的 token 长度。\nfrom transformers import AutoTokenizerhf_tokenizer = AutoTokenizer.from_pretrained('../DataCollection/officials/Qwen2.5-14B-Instruct')\n\n\nfrom langchain_community.document_loaders import TextLoaderfrom langchain_text_splitters import CharacterTextSplitterloader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")docs = loader.load()text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\ttokenizer=hf_tokenizer,    chunk_size=300,    chunk_overlap=50,)doc_splits = text_splitter.split_documents(docs)print(doc_splits[0].page_content)\n\nSemantic Search\n\nDefinition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\nExample: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\nRelated Keywords: Natural Language Processing, Search Algorithms, Data Mining\n\nEmbedding\n\nDefinition: Embedding is the process of converting textual data, such as words or sentences, into low-dimensional continuous vectors. This allows computers to better understand and process the text.\nExample: The word “apple” might be represented as a vector like [0.65, -0.23, 0.17].\nRelated Keywords: Natural Language Processing, Vectorization, Deep Learning\n\nToken\n\nDefinition: A token refers to a smaller unit of text obtained by splitting a larger text. It can be a word, sentence, or phrase.\nExample: The sentence “I go to school” can be split into tokens: “I”, “go”, “to”, “school”.\nRelated Keywords: Tokenization, Natural Language Processing, Parsing\n\nTokenizer\n\nDefinition: A tokenizer is a tool that splits text data into tokens. It is commonly used in natural language processing for data preprocessing.\nExample: The sentence “I love programming.” can be tokenized into [“I”, “love”, “programming”, “.”].\nRelated Keywords: Tokenization, Natural Language Processing, Parsing\n\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 7.embedding模型","url":"/2025/05/04/langchain-tutorial-series/07.Embedding/","content":"OpenAI Embeddings本教程探讨了在 LangChain 框架中使用 OpenAI 文本嵌入 模型。\n展示如何为文本查询和文档生成嵌入，使用 PCA 降维，并将其可视化为 2D 图形以便更好地理解。\n通过分析查询与文档之间的 余弦相似度，该教程提供了嵌入如何增强工作流的洞察，包括 文本分析 和 数据可视化。\nfrom langchain_openai import OpenAIEmbeddings# Set desired modelopenai_embedding = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)query_vector = openai_embedding.embed_query(\"What is the Open AI's gpt embedding model?\")\n\n\nquery = \"How does AI improve healthcare?\"# Various embedding modelsdocuments = [    \"AI helps doctors diagnose diseases faster, improving patient outcomes.\",    \"AI can analyze medical images to detect conditions like cancer.\",    \"Machine learning predicts patient outcomes based on health data.\",    \"AI speeds up drug discovery by predicting the effectiveness of compounds.\",    \"AI monitors patients remotely, enabling proactive care for chronic diseases.\",    \"AI automates administrative tasks, saving time for healthcare workers.\",    \"NLP extracts insights from electronic health records for better care.\",    \"AI chatbots help with patient assessments and symptom checking.\",    \"AI improves drug manufacturing, ensuring better quality and efficiency.\",    \"AI optimizes hospital operations and reduces healthcare costs.\"]\n\nembed 文本query_vector = openai_embedding.embed_query(query)docs_vector = openai_embedding.embed_documents(documents)print(\"number of documents: \" + str(len(docs_vector)))print(\"dimension: \" + str(len(docs_vector[0])))# Part of the sliced vectorprint(\"query: \" + str(query_vector[:5]))print(\"documents[0]: \" + str(docs_vector[0][:5]))print(\"documents[1]: \" + str(docs_vector[1][:5]))\n\nnumber of documents: 10\ndimension: 1024\nquery: [-0.008217299357056618, 0.05857884883880615, -0.015418872237205505, 0.02231411822140217, -0.028306419029831886]\ndocuments[0]: [-0.04019297659397125, 0.020847423002123833, -0.019935578107833862, -0.03813512995839119, -0.04728339985013008]\ndocuments[1]: [0.03994722664356232, 0.029537664726376534, -0.027887631207704544, -0.025838112458586693, -0.05827672779560089]\n\nembed_query 和 embed_documents 没有根本的不同, 前者只是后者的特殊情况\ndef embed_query(self, text: str) -&gt; List[float]:    \"\"\"Call out to OpenAI's embedding endpoint for embedding query text.    Args:        text: The text to embed.    Returns:        Embedding for the text.    \"\"\"    return self.embed_documents([text])[0]\n\n相似度计算（余弦相似度）这段代码通过余弦相似度计算查询与文档之间的相似度。找出与查询最相似的前 3 个文档和最不相似的 3 个文档。\nfrom sklearn.metrics.pairwise import cosine_similarity# Calculate Cosine Similaritysimilarity = cosine_similarity([query_vector], docs_vector)# Sorting by in descending ordersorted_idx = similarity.argsort()[0][::-1]# Display top 3 and bottom 3 documents based on similarityprint(\"query: \", query)print(\"Top 3 most similar document:\")for i in range(0, 3):    print(        f\"[{i+1}] similarity: {similarity[0][sorted_idx[i]]:.3f} | {documents[sorted_idx[i]]}\"    )print(\"\\nBottom 3 least similar documents:\")for i in range(1, 4):    print(        f\"[{i}] similarity: {similarity[0][sorted_idx[-i]]:.3f} | {documents[sorted_idx[-i]]}\"    )\n\nquery:  How does AI improve healthcare?\nTop 3 most similar document:\n[1] similarity: 0.641 | AI monitors patients remotely, enabling proactive care for chronic diseases.\n[2] similarity: 0.596 | AI chatbots help with patient assessments and symptom checking.\n[3] similarity: 0.592 | AI automates administrative tasks, saving time for healthcare workers.\n\nBottom 3 least similar documents:\n[1] similarity: 0.432 | Machine learning predicts patient outcomes based on health data.\n[2] similarity: 0.485 | AI optimizes hospital operations and reduces healthcare costs.\n[3] similarity: 0.503 | NLP extracts insights from electronic health records for better care.\n\n使用向量点积进行相似度计算  \n\n相似度通过向量的点积来确定。\n\n相似度计算公式：\n\n\n\n\n 📐 向量点积的数学意义\n向量点积的定义\n两个向量  和  的点积，在数学上定义为：\n\n\n与余弦相似度的关系\n点积与余弦相似度也有关系，遵循以下性质：\n\n其中：  \n\n 和  表示向量  和  的模长（范数， 特指欧几里得范数）。  \n 是两向量之间的夹角。  \n 表示两向量之间的余弦相似度。\n\n\n🔍 向量点积在相似度计算中的解释\n当点积值很大（即一个较大的正值）时：  \n\n两个向量的模长 ( 和 ) 很大。  \n两个向量之间的夹角 () 很小（ 接近 1）。\n\n这表示两个向量指向相似的方向，并且语义上更为相似，特别是当它们的模长也较大时。\n\n📏 向量的模长（范数）计算\n欧几里得范数的定义\n对于一个向量 ，其欧几里得范数  的计算方式为：\n\n这个模长表示向量在多维空间中的长度或大小。\n\n理解这些数学基础有助于确保精确的相似度计算，进而在语义搜索、检索系统和推荐引擎等任务中提高性能。 🚀\nimport numpy as npdef search_similar_documents(q, docs, hf_embeddings):    \"\"\"    Search for the most relevant documents based on a query using text embeddings.    Args:        q (str): The query string for which relevant documents are to be found.        docs (list of str): A list of document strings to compare against the query.        hf_embeddings: An embedding model object with `embed_query` and `embed_documents` methods.    Returns:        tuple:            - embedded_query (numpy.ndarray): The embedding vector of the query.            - embedded_documents (numpy.ndarray): The embedding matrix of the documents.    \"\"\"    # Embed the query and documents using the embedding model    embedded_query = hf_embeddings.embed_query(q)    embedded_documents = hf_embeddings.embed_documents(docs)    # Calculate similarity scores using dot product and normalize with the magnitudes    query_norm = np.linalg.norm(embedded_query)    document_norms = np.linalg.norm(embedded_documents, axis=1)    # Calculate cosine similarity: dot product / (query norm * document norm)    similarity_scores = (embedded_query @ embedded_documents.T) / (query_norm * document_norms)    # Sort documents by similarity scores in descending order    sorted_idx = similarity_scores.argsort()[::-1]    # Display the results    print(f\"[Query] {q}\\n\" + \"=\" * 40)    for i, idx in enumerate(sorted_idx):        print(f\"[{i}] {docs[idx]}\")        print()    # Return embeddings for potential further processing or analysis    return embedded_query, embedded_documents\n\n嵌入可视化（PCA）为了可视化的目的，减少嵌入的维度。这段代码使用 主成分分析（PCA） 将高维嵌入向量降至 二维。生成的 二维点 在散点图中显示，每个点都标有对应文档的标签。\n高维嵌入向量直接进行解读和分析非常具有挑战性。通过将它们降到二维，我们可以：\n\n可视化探索嵌入之间的关系（例如，聚类、分组）。\n识别数据中的模式或异常，这些模式在高维空间中可能不那么显眼。\n提高可解释性，使数据更加易于人类分析和决策。\n\nimport matplotlib.pyplot as pltfrom sklearn.decomposition import PCAimport numpy as np# Combine documents and query for PCAall_vectors = np.vstack([docs_vector, query_vector])  # Stack query vector with docspca = PCA(n_components=2)reduced_vectors = pca.fit_transform(all_vectors)# Separate reduced vectors for documents and querydoc_vectors_2d = reduced_vectors[:-1]  # All but the last point (documents)query_vector_2d = reduced_vectors[-1]  # Last point (query)# Plot the reduced vectorsplt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1], color=\"blue\", label=\"Documents\")plt.scatter(query_vector_2d[0], query_vector_2d[1], color=\"red\", label=\"Query\", marker=\"x\", s=300,)# Annotate document pointsfor i, doc in enumerate(documents):    plt.text(doc_vectors_2d[i, 0], doc_vectors_2d[i, 1], doc, fontsize=8)# Add plot detailsplt.title(\"2D Visualization of Embedding Vectors with Query\")plt.xlabel(\"PCA Dimension 1\")plt.ylabel(\"PCA Dimension 2\")plt.legend()plt.show()\n\n\n\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddingsmodel_name = \"../DataCollection/officials/bge-large-zh-v1.5\"hf_embeddings = HuggingFaceEmbeddings(    model_name=model_name,    model_kwargs={\"device\": 'cuda:1'},  # mps, cuda, cpu    encode_kwargs={\"normalize_embeddings\": True},)\n\n\nvector = hf_embeddings.embed_query(\"Please tell me more about LangChain.\")print(len(vector))vector = hf_embeddings.embed_documents([    \"Hi, nice to meet you.\",    \"LangChain simplifies the process of building applications with large language models.\",    \"The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\",    \"LangChain simplifies the process of building applications with large-scale language models.\",    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",])print(len(vector))print([len(item) for item in vector])\n\n1024\n5\n[1024, 1024, 1024, 1024, 1024]\n\nCacheBackedEmbeddings嵌入值可以存储或临时缓存，以避免重新计算。\n缓存嵌入值可以使用 CacheBackedEmbeddings 来完成。缓存支持的嵌入器是对嵌入器的包装，它将嵌入存储在一个键值存储中。文本被哈希处理，哈希值作为缓存中的键。\nimport osos.makedirs(\"./cache/\", exist_ok=True)print(os.path.exists(\"./cache/\"))  # Check if the directory existsprint(os.access(\"./cache/\", os.W_OK))  # Check if the directory is writable\n\nTrue\nTrue\n\n使用嵌入与本地文件存储（持久化存储）初始化 CacheBackedEmbeddings 的主要支持方法是 from_bytes_store。\n它接受以下参数：\n\nunderlying_embeddings: 用于生成嵌入的嵌入器。\ndocument_embedding_cache: 用于缓存文档嵌入的 ByteStore 实现之一。\nnamespace: （可选，默认值为 \"\"）命名空间用于文档缓存。这样可以避免与其他缓存发生冲突。例如，可以将其设置为正在使用的嵌入模型的名称。\n\n注意：设置 namespace 参数非常重要，以避免在使用不同的嵌入模型对相同文本进行嵌入时发生冲突。\n首先，让我们看一个使用本地文件系统存储嵌入，并使用 FAISS 向量存储进行检索的例子。\nfrom langchain.storage import LocalFileStorefrom langchain_openai import OpenAIEmbeddingsfrom langchain.embeddings import CacheBackedEmbeddingsfrom langchain_community.vectorstores.faiss import FAISS# Configure basic embeddings using OpenAI embeddingsunderlying_embeddings = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)# Set up a local file storagestore = LocalFileStore(\"./cache/\")# Create embeddings with caching supportcached_embedder = CacheBackedEmbeddings.from_bytes_store(    underlying_embeddings=underlying_embeddings,     document_embedding_cache=store,     namespace=underlying_embeddings.model, # Create a cache-backed embedder using the base embedding and storage)\n\n在 embedding 之前 cache 是空的\nlist(store.yield_keys())\n\n\n\n\n[]\n\n加载文档，将其拆分为多个块，对每个块进行嵌入，并将嵌入加载到向量存储中。\nfrom langchain.document_loaders import TextLoaderfrom langchain_text_splitters import CharacterTextSplitterraw_documents = TextLoader(\"./data/appendix-keywords.txt\", encoding=\"utf-8\").load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)documents = text_splitter.split_documents(raw_documents)\n\n从文档创建 FAISS 数据库。\n%time db = FAISS.from_documents(documents, cached_embedder)\n\nCPU times: user 375 ms, sys: 43 ms, total: 418 ms\nWall time: 809 ms\n\n如果我们尝试再次创建向量存储，它会更快，因为无需重新计算任何嵌入。\n%time db2 = FAISS.from_documents(documents, cached_embedder)\n\nCPU times: user 13.7 ms, sys: 1.04 ms, total: 14.7 ms\nWall time: 13.8 ms\n\nlist(store.yield_keys())[:5]\n\n\n\n\n['bge-m34b802135-9b69-54ac-835f-f31f0a8f73cf',\n 'bge-m34fd4987e-f5b6-52f8-91e2-886802754643',\n 'bge-m3229c1600-8452-5938-b611-45db25315327',\n 'bge-m3fed9c955-3b6d-5ce9-b7d2-235f35d18610',\n 'bge-m39668cb63-4ad2-528c-9bf2-aecbfa54e1cd']\n\n使用 InMemoryByteStore（非持久化）要使用不同的 ByteStore，只需在创建 CacheBackedEmbeddings 时指定所需的 ByteStore。\n以下是使用非持久化的 InMemoryByteStore 创建相同缓存嵌入对象的示例。\nfrom langchain.embeddings import CacheBackedEmbeddingsfrom langchain.storage import InMemoryByteStore# Create an in-memory byte storestore = InMemoryByteStore()underlying_embeddings = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)cached_embedder = CacheBackedEmbeddings.from_bytes_store(    underlying_embeddings, store, namespace=underlying_embeddings.model)\n\n\n%time db = FAISS.from_documents(documents, cached_embedder)  list(store.yield_keys())[:5]\n\nCPU times: user 6.92 ms, sys: 905 μs, total: 7.82 ms\nWall time: 7.34 ms\n\n\n\n\n\n['bge-m34b802135-9b69-54ac-835f-f31f0a8f73cf',\n 'bge-m34fd4987e-f5b6-52f8-91e2-886802754643',\n 'bge-m3229c1600-8452-5938-b611-45db25315327',\n 'bge-m3fed9c955-3b6d-5ce9-b7d2-235f35d18610',\n 'bge-m39668cb63-4ad2-528c-9bf2-aecbfa54e1cd']\n\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 8.向量数据库","url":"/2025/05/04/langchain-tutorial-series/08.VectorStore/","content":"向量存储的概念指南向量存储是专门设计的数据库，用于通过向量表示（嵌入）来索引和检索信息。\n它们通常用于通过识别语义相似的内容而非依赖精确的关键词匹配，来搜索非结构化数据，如文本、图像和音频。\n为什么向量存储至关重要\n快速高效的搜索\n\n通过正确存储和索引嵌入向量，向量存储能够快速检索相关信息，即使在处理海量数据集时也能高效工作。\n\n应对数据增长的可扩展性\n\n随着数据不断扩展，向量存储必须能够高效扩展。一个结构良好的向量存储可以确保系统处理大规模数据时不会出现性能问题，支持无缝增长。\n\n促进语义搜索\n\n与传统的基于关键词的搜索不同，语义搜索是根据内容的含义来检索信息。向量存储通过查找与用户查询上下文密切相关的段落或部分来实现这一点。这相比于仅能进行精确关键词匹配的原始文本数据库具有明显优势。\n理解搜索方法\n基于关键词的搜索这种方法依赖于通过查询与文档中精确的单词或短语匹配来检索结果。它简单，但无法捕捉单词之间的语义关系。\n\n基于相似度的搜索使用向量表示来评估查询与文档之间的语义相似度。它提供了更准确的结果，尤其是对于自然语言查询。\n\n基于分数的相似度搜索根据查询为每个文档分配一个相似度分数。更高的分数表示相关性更强。常用的度量标准包括 余弦相似度 或 基于距离的评分。\n\n\n相似度搜索的工作原理\n嵌入和向量的概念嵌入 是单词或文档在高维空间中的数值表示。它们捕捉了语义信息，从而能够更好地比较查询和文档。\n\n相似度度量方法  \n\n余弦相似度：衡量两个向量之间的夹角的余弦值。值越接近 1 表示相似度越高。  \n欧几里得距离：计算向量空间中两个点之间的直线距离。距离越小表示相似度越高。\n\n\n评分和排序搜索结果计算相似度后，文档将被分配一个分数。根据这些分数，结果将按相关性降序排列。\n\n搜索算法简要概述  \n\nTF-IDF：根据单词在文档中的频率及其在所有文档中的出现频率来赋予单词权重。  \nBM25：TF-IDF 的改进版，优化了信息检索的相关性。  \n神经搜索：利用深度学习生成上下文感知的嵌入，以获得更准确的结果。\n\n\n\n向量存储中的搜索类型\n相似度搜索：找到与查询最相似的文档。适用于语义搜索应用。\n\n最大边际相关性 (MMR) 搜索：通过优先选择多样且相关的文档，在搜索结果中平衡相关性和多样性。\n\n稀疏检索器：使用传统的基于关键词的方法，如 TF-IDF 或 BM25 来检索文档。适用于上下文有限的数据集。\n\n密集检索器：依赖密集的向量嵌入来捕捉语义信息。常见于使用深度学习的现代搜索系统。\n\n混合搜索：结合稀疏和密集的检索方法。通过将密集方法的精确度与稀疏方法的广泛覆盖性结合，达到最优结果。\n\n\n向量存储作为检索器\n功能：通过使用 .as_retriever() 方法将向量存储转换为检索器，你可以创建一个符合 LangChain 检索器接口的轻量级包装器。这使得你可以使用不同的检索策略，如 相似度搜索 和 最大边际相关性 (MMR) 搜索，并允许自定义检索参数。  \n使用案例：适用于复杂的应用场景，在这些场景中，检索器需要作为更大流程的一部分，例如检索增强生成 (RAG) 系统。它便于与 LangChain 中的其他组件无缝集成，支持诸如集成检索方法和高级查询分析等功能。\n\n总之，虽然直接的向量存储搜索提供了基本的检索能力，但将向量存储转换为检索器，可以提供更强的灵活性和在 LangChain 生态系统中的集成，支持更复杂的检索策略和应用。\n集成向量数据库\nChroma：一个开源的向量数据库，专为 AI 应用设计，支持高效存储和检索嵌入向量。\nFAISS：由 Facebook AI 开发，FAISS（Facebook AI Similarity Search）是一个用于高效相似度搜索和密集向量聚类的库。\nPinecone：一个托管的向量数据库服务，提供高性能的向量相似度搜索，使开发人员能够构建可扩展的 AI 应用程序。\nQdrant：Qdrant（读作 quadrant）是一个向量相似度搜索引擎。它提供了一个生产就绪的服务，具有便捷的 API，用于存储、搜索和管理向量，支持额外的有效载荷和扩展过滤功能。它适用于各种神经网络或基于语义的匹配、分面搜索和其他应用。\nElasticsearch：一个分布式的 RESTful 搜索和分析引擎，支持向量搜索，允许在大型数据集中高效进行相似度搜索。\nMongoDB：MongoDB Atlas 向量搜索支持向量嵌入的高效存储、索引和查询，并与操作数据一起使用，方便无缝实现 AI 驱动的应用。\npgvector (PostgreSQL)：一个 PostgreSQL 扩展，添加了向量相似度搜索功能，使得在关系数据库中高效存储和查询向量数据成为可能。\nNeo4j：一个图形数据库，存储节点和关系，并原生支持向量搜索，方便执行涉及图形和向量数据的复杂查询。\nWeaviate：一个开源的向量数据库，允许存储数据对象和向量嵌入，支持多种数据类型并提供语义搜索功能。\nMilvus：一个数据库，专为存储、索引和管理机器学习模型生成的大规模嵌入向量而设计，支持高性能的向量相似度搜索。\n\n这些向量存储在构建需要高效相似度搜索和高维数据管理的应用程序中起着至关重要的作用。\nLangChain 提供了一个统一的接口，用于与向量存储交互，使用户可以轻松切换不同的实现方式。\n该接口包括用于 写入、删除 和 搜索 向量存储中的文档的核心方法。\n主要方法如下：\n\nadd_documents : 将一组文本添加到向量存储中。\nupsert_documents : 向向量存储中添加新文档，或者如果文档已存在，则更新它们。\n在本教程中，我们还将介绍 upsert_documents_parallel 方法，它在适用时支持高效的大规模数据处理。\n\n\ndelete_documents : 从向量存储中删除一组文档。\nsimilarity_search : 搜索与给定查询相似的文档。\n\nChormaCreate dbfrom langchain_community.vectorstores import Chromafrom langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)vector_store = Chroma(\tcollection_name=\"langchain_store\", \tembedding_function=embeddings\t)\n\nAdd Documents将一组 Documents 添加到向量存储中, 并生成一组 id, id 可以自己预设置返回每个 Documents 的 id\n如果 embedding 功能会出现 InternalServerError, 可能文档文本有问题\n如果不指定 ID，Chroma 会自动为文档生成唯一 ID，但手动指定 ID 可以帮助你更方便地管理文档。\nfrom langchain_core.documents import Documenttexts = [    \"AI helps doctors diagnose diseases faster, improving patient outcomes.\",    \"AI can analyze medical images to detect conditions like cancer.\",    \"Machine learning predicts patient outcomes based on health data.\",    \"AI speeds up drug discovery by predicting the effectiveness of compounds.\",    \"AI monitors patients remotely, enabling proactive care for chronic diseases.\",    \"AI automates administrative tasks, saving time for healthcare workers.\",    \"NLP extracts insights from electronic health records for better care.\",    \"AI chatbots help with patient assessments and symptom checking.\",    \"AI improves drug manufacturing, ensuring better quality and efficiency.\",    \"AI optimizes hospital operations and reduces healthcare costs.\"]# 重复内容会导致报错# texts = [#     \"AI \",#     \"AI\",#     \"Machine learning\",# ]documents = [\tDocument(text, metadata={\"source\":text})\tfor text in texts]ids = [f'{i}' for i in range(len(documents))]ret_ids = vector_store.add_documents(documents=documents, ids=ids)print(ret_ids)\n\n['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n重复添加对于不同向量数据集的处理方式不一样。重复添加在 chrome 中不会报错, 应该是直接替代了 id 的原数据, 但换做 Faiss 就会报错, 报错信息显示 id 已存在。\nret_ids = vector_store.add_documents(documents=documents, ids=ids)print(ret_ids)\n\n\nret_ids = vector_store.add_documents(documents=documents)print(ret_ids)\n\n\nfrom langchain_core.documents import Documentdoc1 = Document(page_content=\"This is a test document.\", metadata={\"source\": \"test\"})doc2 = Document(page_content=\"This is an updated document.\", metadata={\"source\": \"test\"})# 文档 ID 可以根据文本的哈希值来生成唯一的 IDids = [\"doc_1\", \"doc_2\"]# 第一次插入vector_store.add_documents(documents=[doc1], ids=[\"doc_1\"])# 使用 upsert_documents 更新或插入文档vector_store.upsert_documents(documents=[doc2], ids=[\"doc_1\"])\n\n\n---------------------------------------------------------------------------\n\nAttributeError                            Traceback (most recent call last)\n\nCell In[4], line 13\n     10 vector_store.add_documents(documents=[doc1], ids=[\"doc_1\"])\n     12 # 使用 upsert_documents 更新或插入文档\n---&gt; 13 vector_store.upsert_documents(documents=[doc2], ids=[\"doc_1\"])\n\n\nAttributeError: 'Chroma' object has no attribute 'upsert_documents'\n\nChroma 没有 upsert_documents 方法, 但有同功能的函数 update_documents, 貌似 add_documents 起到类似的功能(对于一个 id, 更新为新的 Document)\nret_ids = vector_store.update_documents(ids=ids, documents=documents)print(ret_ids)\n\nNone\n\nDelete Documents by idchrome 没有 delete_documents 函数, 只能基于 id 删除\nids_4_delete = [\"3\"]import tracebacktry:\tvector_store.delete(ids=ids_4_delete)\tprint(f'ID:{ids_4_delete} deleted')except Exception as e:\tprint(traceback.format_exc())\tprint('deleted non-successfully')\n\n删除整个数据库\nvector_store.delete_collection()\n\nSearch\nsimilarity_search 只返回 Documents\nsimilarity_search_with_score 返回 Documents 和对应的 scores\nsimilarity_search_by_vector 可自己转入 embedding vector, 因此可以自己控制 query 的 embedding 与 vecotr database 的 embedding 方法不同. 该函数也有带 scores 返回的版本\n\nsimilarity_search 方法会基于给定的查询和嵌入向量，在向量存储中查找与之最相似的文档。它通常用于简单的、单次的检索操作。\na_query = \"How does AI improve healthcare?\"results = vector_store.similarity_search(\tquery=a_query,\tk=3\t)for index, doc in enumerate(results):    print(f\"[{index}]{doc.page_content} [{doc.metadata}]\")\n\n[0]AI monitors patients remotely, enabling proactive care for chronic diseases. [{'source': 'AI monitors patients remotely, enabling proactive care for chronic diseases.'}]\n[1]AI chatbots help with patient assessments and symptom checking. [{'source': 'AI chatbots help with patient assessments and symptom checking.'}]\n[2]AI automates administrative tasks, saving time for healthcare workers. [{'source': 'AI automates administrative tasks, saving time for healthcare workers.'}]\n\na_query = \"How does AI improve healthcare?\"results = vector_store.similarity_search_with_score(\tquery=a_query,\tk=3\t)for index, (doc, score) in enumerate(results):    print(f\"[{index}][SIM={score:3f}]{doc.page_content} [{doc.metadata}]\")\n\n[0][SIM=0.718768]AI monitors patients remotely, enabling proactive care for chronic diseases. [{'source': 'AI monitors patients remotely, enabling proactive care for chronic diseases.'}]\n[1][SIM=0.807140]AI chatbots help with patient assessments and symptom checking. [{'source': 'AI chatbots help with patient assessments and symptom checking.'}]\n[2][SIM=0.815210]AI automates administrative tasks, saving time for healthcare workers. [{'source': 'AI automates administrative tasks, saving time for healthcare workers.'}]\n\nquery_embedder = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)query_vector = query_embedder.embed_query(a_query)results = db.similarity_search_by_vector(query_vector, k=3)for index, doc in enumerate(results):    print(f\"[{index}]{doc.page_content} [{doc.metadata}]\")\"\"\" 带 score 版本results = db.similarity_search_with_relevance_scores(query_vector, k=3)for index, (doc, score) in enumerate(results):    print(f\"[{index}][SIM={score:3f}]{doc.page_content} [{doc.metadata}]\")\"\"\"\n\nas_retrieveras_retriever 方法是 Chroma 向量数据库的一个重要功能，它将 向量数据库 转换为一个 Retriever 对象，这样你就可以在 LangChain 的检索管道中使用它。as_retriever 使得 Chroma 与 LangChain 更好地集成，支持不同的检索策略，并能与其他组件（如问答系统、文档检索系统等）无缝协作。\n以下是参数说明\nretriever = vector_store.as_retriever(\t# Defines the type of search that the Retriever should perform. Can be “similarity” (default), “mmr”, or “similarity_score_threshold”.    search_type=\"mmr\",\t# k: num of documents returned\t# fetch_k: Amount of documents to pass to MMR algorithm\t# lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum.    search_kwargs={\t\t\"k\": 1, \t\t\"fetch_k\": 2, \t\t\"lambda_mult\": 0.5\t\t},)\n\n\n\n# Retrieve more documents with higher diversity# Useful if your dataset has many similar documentsvector_store.as_retriever(    search_type=\"mmr\",    search_kwargs={'k': 6, 'lambda_mult': 0.25})# Fetch more documents for the MMR algorithm to consider# But only return the top 5vector_store.as_retriever(    search_type=\"mmr\",    search_kwargs={'k': 5, 'fetch_k': 50})# Only retrieve documents that have a relevance score# Above a certain thresholdvector_store.as_retriever(    search_type=\"similarity_score_threshold\",    search_kwargs={'score_threshold': 0.8})# Only get the single most similar document from the datasetvector_store.as_retriever(search_kwargs={'k': 1})# Use a filter to only retrieve documents from a specific papervector_store.as_retriever(    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}})\n\n\n\n\nVectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=&lt;langchain_community.vectorstores.faiss.FAISS object at 0x7fe87b72e3f0&gt;, search_kwargs={'filter': {'paper_title': 'GPT-4 Technical Report'}})\n\n最重要的是集成到 langchain 框架, 直接 invoke 调用\nretriever = vector_store.as_retriever(search_kwargs={'k': 3})a_query = \"How does AI improve healthcare?\"retriever.invoke(a_query)\n\n\n\n\n[Document(metadata={'source': 'AI monitors patients remotely, enabling proactive care for chronic diseases.'}, page_content='AI monitors patients remotely, enabling proactive care for chronic diseases.'),\n Document(metadata={'source': 'AI chatbots help with patient assessments and symptom checking.'}, page_content='AI chatbots help with patient assessments and symptom checking.'),\n Document(metadata={'source': 'AI automates administrative tasks, saving time for healthcare workers.'}, page_content='AI automates administrative tasks, saving time for healthcare workers.')]\n\nFaiss初始化import faissfrom langchain_community.vectorstores import FAISSfrom langchain_community.docstore.in_memory import InMemoryDocstorefrom langchain_openai import OpenAIEmbeddingsopenai_embedding = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)embed_dim = len(openai_embedding.embed_query(\"hello world\"))index = faiss.IndexFlatL2(embed_dim)vector_store = FAISS(    embedding_function=openai_embedding,    index=index,    docstore= InMemoryDocstore(),    index_to_docstore_id={})\n\nAdd Documentsfrom langchain_core.documents import Documenttexts = [    \"AI helps doctors diagnose diseases faster, improving patient outcomes.\",    \"AI can analyze medical images to detect conditions like cancer.\",    \"Machine learning predicts patient outcomes based on health data.\",    \"AI speeds up drug discovery by predicting the effectiveness of compounds.\",    \"AI monitors patients remotely, enabling proactive care for chronic diseases.\",    \"AI automates administrative tasks, saving time for healthcare workers.\",    \"NLP extracts insights from electronic health records for better care.\",    \"AI chatbots help with patient assessments and symptom checking.\",    \"AI improves drug manufacturing, ensuring better quality and efficiency.\",    \"AI optimizes hospital operations and reduces healthcare costs.\"]documents = [\tDocument(text)\tfor text in texts]ids = [f'{i}' for i in range(len(documents))]ret_ids = vector_store.add_documents(documents=documents, ids=ids)print(ret_ids)\n\n['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n增加重复 Document 会报错\nValueError: Tried to add ids that already exist: {'9', '5', '7', '0', '6', '8', '1', '4', '2', '3'}\n\n\nimport tracebacktry:\tret_ids = vector_store.add_documents(documents=documents, ids=ids)\tprint(ret_ids)except Exception as e:\tprint(traceback.format_exc())\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_177930/1610519831.py\", line 3, in &lt;module&gt;\n    ret_ids = vector_store.add_documents(documents=documents, ids=ids)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_core/vectorstores/base.py\", line 286, in add_documents\n    return self.add_texts(texts, metadatas, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py\", line 341, in add_texts\n    return self.__add(texts, embeddings, metadatas=metadatas, ids=ids)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py\", line 316, in __add\n    self.docstore.add({id_: doc for id_, doc in zip(ids, documents)})\n  File \"/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_community/docstore/in_memory.py\", line 28, in add\n    raise ValueError(f\"Tried to add ids that already exist: {overlapping}\")\nValueError: Tried to add ids that already exist: {'9', '5', '7', '0', '6', '8', '1', '4', '2', '3'}\n\nGet by id通过 id 返回 Documents. Chroma 没有这个函数\ndocs = vector_store.get_by_ids([\"1\",\"2\"])docs\n\n\n\n\n[Document(id='1', metadata={}, page_content='AI can analyze medical images to detect conditions like cancer.'),\n Document(id='2', metadata={}, page_content='Machine learning predicts patient outcomes based on health data.')]\n\n其他功能和 chroma 类似, 不赘述了\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 10.重排","url":"/2025/05/04/langchain-tutorial-series/10.Reranker/","content":"Reranker目的:  \n\n对检索到的文档进行重排序，优化其排名，优先展示与查询最相关的结果。\n\n结构:  \n\n接受查询和文档作为单一输入对，进行联合处理。\n\n机制:  \n\n单一输入对：将查询和文档作为组合输入，直接输出相关性评分。\n自注意力机制：使用自注意力机制联合分析查询和文档，有效捕捉它们之间的语义关系。\n\n优势:  \n\n更高的准确性：提供更精确的相似度评分。\n深度语境分析：探索查询和文档之间的语义细微差别。\n\n局限性:  \n\n高计算成本：处理可能会消耗较多时间。\n扩展性问题：未经过优化时，不适用于大规模文档集合。\n\n实际应用:  \n\n双编码器（Bi-Encoder）通过计算轻量级的相似度分数快速检索候选文档。\n交叉编码器（Cross Encoder）通过深入分析查询与检索到的文档之间的语义关系，进一步优化这些结果。\n\nfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Load documentsdocuments = TextLoader(\"./data/appendix-keywords.txt\").load()# Configure text splittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)# Split documents into chunkstexts = text_splitter.split_documents(documents)# # Set up the embedding modelembeddings_model = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)# Create FAISS index from documents and set up retrieverretriever = FAISS.from_documents(texts, embeddings_model).as_retriever(    search_kwargs={\"k\": 30})# Define the queryquery = \"Can you tell me about Word2Vec?\"# Execute the query and retrieve resultsdocs = retriever.invoke(query)# Display the retrieved documentsfor i, d in enumerate(docs):\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)\tprint('-' * 100)\n\n使用 ContextualCompressionRetriever 来包装 base_retriever。CrossEncoderReranker 利用 HuggingFaceCrossEncoder 对检索到的结果进行重新排序。\n之前 ContextualCompressionRetriever 只是对文档压缩, 现在起到过滤作用\nfrom langchain.retrievers import ContextualCompressionRetrieverfrom langchain.retrievers.document_compressors import CrossEncoderRerankerfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder# Initialize the modelmodel = HuggingFaceCrossEncoder(\tmodel_name=\"../DataCollection/officials/bge-reranker-v2-m3\",\tmodel_kwargs = {'device': 'cuda:6'}\t)# Select the top 3 documentscompressor = CrossEncoderReranker(model=model, top_n=3)# Initialize the contextual compression retrievercompression_retriever = ContextualCompressionRetriever(    base_compressor=compressor, base_retriever=retriever)# Retrieve compressed documentscompressed_docs = compression_retriever.invoke(\"Can you tell me about Word2Vec?\")# Display the documentsfor i, d in enumerate(docs):\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)\tprint('-' * 100)\n\n我没有在 langchain 上找到基于 api 的 reranker 类\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 9.向量数据库检索器","url":"/2025/05/04/langchain-tutorial-series/09.Retriever/","content":"VectorStore-backed Retriever基于VectorStore的检索器 是一种文档检索系统，它利用向量存储根据文档的向量表示来进行搜索。这种方法使得基于相似度的搜索变得高效，特别适用于处理非结构化数据。\nRAG系统中的文档搜索和响应生成步骤包括：\n\n文档加载：导入原始文档。\n文本切分：将文本切分成可管理的块。\n向量嵌入：使用嵌入模型将文本转换为数值向量。\n存储到向量数据库：将生成的嵌入向量存储到向量数据库中，以便高效检索。\n\n在查询阶段：\n\n流程：用户查询 → 嵌入 → 在向量存储中搜索 → 检索相关块 → LLM生成响应\n用户的查询被转化为一个嵌入向量，使用嵌入模型。\n该查询嵌入向量与向量数据库中存储的文档向量进行比较，以 检索最相关的结果。\n检索到的文档块被传递给大语言模型（LLM），该模型基于检索到的信息生成最终响应。\n\nimport faissfrom langchain_core.documents import Documentfrom langchain_community.vectorstores import FAISSfrom langchain_community.docstore.in_memory import InMemoryDocstorefrom langchain_openai import OpenAIEmbeddingsopenai_embedding = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)embed_dim = len(openai_embedding.embed_query(\"hello world\"))\n\n\ntexts = [    \"AI helps doctors diagnose diseases faster, improving patient outcomes.\",    \"AI can analyze medical images to detect conditions like cancer.\",    \"Machine learning predicts patient outcomes based on health data.\",    \"AI speeds up drug discovery by predicting the effectiveness of compounds.\",    \"AI monitors patients remotely, enabling proactive care for chronic diseases.\",    \"AI automates administrative tasks, saving time for healthcare workers.\",    \"NLP extracts insights from electronic health records for better care.\",    \"AI chatbots help with patient assessments and symptom checking.\",    \"AI improves drug manufacturing, ensuring better quality and efficiency.\",    \"AI optimizes hospital operations and reduces healthcare costs.\"]documents = [\tDocument(text, metadata={\"source\":text})\tfor text in texts]db = FAISS.from_documents(documents, openai_embedding)\n\n一旦向量数据库创建完成，就可以使用检索方法，如 相似度搜索 和 最大边际相关性（MMR），加载并查询数据库，从中搜索相关的文本。\nas_retriever 方法允许你将一个向量数据库转换为一个检索器，从而实现从向量库中高效地搜索和检索文档。\n工作原理：\n\nas_retriever() 方法将一个向量库（如 FAISS）转换为一个检索器对象，使其与 LangChain 的检索工作流兼容。\n这个检索器可以直接用于 RAG 流水线，或与大型语言模型（LLM）结合，用于构建智能搜索系统。\n\nretriever = db.as_retriever()\n\n高级检索器配置as_retriever 方法允许你配置高级检索策略，如 相似度搜索、最大边际相关性（MMR） 和 基于相似度分数阈值的过滤。\n参数：\n\n**kwargs：传递给检索函数的关键字参数：\nsearch_type：指定搜索方法。\n\"similarity\"：基于余弦相似度返回最相关的文档。\n\"mmr\"：利用最大边际相关性算法，平衡 相关性 和 多样性。\n\"similarity_score_threshold\"：返回相似度分数超过指定阈值的文档。\n\n\nsearch_kwargs：其他用于微调结果的搜索选项：\nk：返回的文档数量（默认值：4）。\nscore_threshold：用于 \"similarity_score_threshold\" 搜索类型的最小相似度分数（例如：0.8）。\nfetch_k：在 MMR 搜索过程中最初检索的文档数量（默认值：20）。\nlambda_mult：控制 MMR 结果中的多样性（0 = 最大多样性，1 = 最大相关性，默认值：0.5）。\nfilter：用于选择性文档检索的元数据过滤。\n\n\n\n\n\n返回值：\n\nVectorStoreRetriever：初始化后的检索器对象，可以直接用于文档搜索任务。\n\n注意事项：\n\n支持多种搜索策略（similarity、MMR、similarity_score_threshold）。\nMMR 通过减少结果中的冗余，提升结果多样性同时保持相关性。\n元数据过滤使得根据文档属性选择性地检索文档成为可能。\ntags 参数可以用于给检索器加标签，以便更好地组织和识别。\n\n警告：\n\n使用 MMR 时的多样性控制：\n小心调整 fetch_k（最初检索的文档数量）和 lambda_mult（多样性控制因子）以获得最佳平衡。\nlambda_mult：\n较低值（&lt; 0.5）→ 优先考虑多样性。\n较高值（&gt; 0.5）→ 优先考虑相关性。\n\n\n为有效的多样性控制，设置 fetch_k 大于 k。\n\n\n阈值设置：\n使用较高的 score_threshold（例如 0.95）可能会导致没有结果。\n\n\n元数据过滤：\n在应用过滤器之前，确保元数据结构已经定义好。\n\n\n平衡配置：\n为了获得最佳的检索性能，保持 search_type 和 search_kwargs 设置之间的适当平衡。\n\n\n\nretriever = db.as_retriever(    search_type=\"similarity_score_threshold\",     search_kwargs={        \"k\": 5,  # Return the top 5 most relevant documents        \"score_threshold\": 0.5  # Only return documents with a similarity score of 0.4 or higher    })query = \"How does AI improve healthcare?\"results = retriever.invoke(query)# Display search resultsfor doc in results:    print(doc.page_content)\n\nNo relevant docs were retrieved using the relevance score threshold 0.5\n\n检索器的 invoke() 方法invoke() 方法是与检索器交互的主要入口点。它用于根据给定的查询搜索并检索相关的文档。\n工作原理：\n\n查询提交：用户提交查询字符串作为输入。\n嵌入生成：如果需要，查询会被转换成向量表示。\n搜索过程：检索器使用指定的搜索策略（如相似度、MMR 等）在向量数据库中进行搜索。\n结果返回：该方法返回一组相关的文档片段。\n\n参数：\n\ninput（必需）：\n\n用户提供的查询字符串。\n查询会被转换成向量，并与存储的文档向量进行相似度比较，以进行基于相似度的检索。\n\n\nconfig（可选）：\n\n允许对检索过程进行细粒度控制。\n可用于指定 标签、元数据插入和搜索策略。\n\n\n**kwargs（可选）：\n\n允许直接传递 search_kwargs 进行高级配置。\n示例选项包括：\nk：返回的文档数量。\nscore_threshold：文档被包括的最低相似度分数。\nfetch_k：MMR 搜索中最初检索的文档数量。\n\n\n\n\n\n返回值：\n\nList[Document]：\n返回包含检索到的文本和元数据的文档对象列表。\n每个文档对象包括：\npage_content：文档的主要内容。\nmetadata：与文档相关联的元数据（例如，来源、标签）。\n\n\n\n\n\n用例 1\ndocs = retriever.invoke(\"What is an embedding?\")for doc in docs:    print(doc.page_content)    print(\"=========================================================\")\n\nMachine learning predicts patient outcomes based on health data.\n=========================================================\nAI monitors patients remotely, enabling proactive care for chronic diseases.\n=========================================================\nAI chatbots help with patient assessments and symptom checking.\n=========================================================\n\n用例 2\n# search options: top 5 results with a similarity score ≥ 0.7docs = retriever.invoke(    \"What is a vector database?\",    search_kwargs={\"k\": 5, \"score_threshold\": 0.7})for doc in docs:    print(doc.page_content)    print(\"=========================================================\")\n\nMachine learning predicts patient outcomes based on health data.\n=========================================================\nAI monitors patients remotely, enabling proactive care for chronic diseases.\n=========================================================\nAI chatbots help with patient assessments and symptom checking.\n=========================================================\n\n最大边际相关性 (MMR)最大边际相关性 (MMR) 搜索方法是一种文档检索算法，旨在通过平衡相关性和多样性来减少冗余，从而返回结果时提高多样性。\nMMR 的工作原理：与仅根据相似度分数返回最相关文档的基本相似度搜索不同，MMR 考虑了两个关键因素：\n\n相关性：衡量文档与用户查询的匹配程度。\n多样性：确保检索到的文档彼此不同，避免重复的结果。\n\n关键参数：\n\nsearch_type=\"mmr\"：启用 MMR 检索策略。\nk：应用多样性过滤后返回的文档数量（默认值：4）。\nfetch_k：应用多样性过滤前最初检索的文档数量（默认值：20）。\nlambda_mult：多样性控制因子（0 = 最大多样性，1 = 最大相关性，默认值：0.5）。\n\n相似度分数阈值搜索相似度分数阈值搜索是一种检索方法，只有当文档的相似度分数超过预定义的阈值时才会返回。该方法有助于筛选出低相关性的结果，确保返回的文档与查询高度相关。\n关键特性：\n\n相关性过滤：仅返回相似度分数高于指定阈值的文档。\n可调精度：通过 score_threshold 参数调整阈值。\n启用搜索类型：通过设置 search_type=\"similarity_score_threshold\" 启用此搜索方法。\n\n这种搜索方法非常适用于需要高度精确结果的任务，例如事实核查或回答技术性查询。\n配置 top_k（调整返回文档的数量）\n参数 k 指定在向量搜索过程中返回的文档数量。它决定了从向量数据库中检索到的 排名最高（基于相似度分数）的文档数量。\n\n通过在 search_kwargs 中设置 k 值，可以调整检索到的文档数量。\n\n例如，设置 k=1 将仅返回 最相关的 1 篇文档，该文档基于相似度排序。\n\n\nContextualCompressionRetrieverContextualCompressionRetriever 是 LangChain 中的一种强大工具，旨在通过根据上下文压缩检索到的文档来优化检索过程。这个检索器特别适用于需要对大量数据进行动态总结或过滤的场景，确保只有最相关的信息传递到后续处理步骤。\nContextualCompressionRetriever 的主要特点包括：\n\n上下文感知压缩：文档会根据特定的上下文或查询进行压缩，确保相关性并减少冗余。\n灵活的集成：与其他 LangChain 组件无缝工作，便于集成到现有的管道中。\n可定制的压缩：支持使用不同的压缩技术，包括摘要模型和基于嵌入的方法，来根据需求定制检索过程。\n\nContextualCompressionRetriever 特别适用于以下应用：\n\n为问答系统总结大量数据。\n通过提供简洁且相关的回答来提升聊天机器人性能。\n提高文档密集型任务（如法律分析或学术研究）的效率。\n\n通过使用这个检索器，开发者可以显著减少计算开销，并提高提供给最终用户的信息质量。\nfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import CharacterTextSplitter# 1. Generate Loader to lthe text file using TextLoaderloader = TextLoader(\"./data/appendix-keywords.txt\")\\# 2. Generate text chunks using CharacterTextSplitter and split the text into chunks of 300 characters with no overlap.text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=0)texts = loader.load_and_split(text_splitter)# 3. Generate vector store using FAISS and convert it to retrieverembedder = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)retriever = FAISS.from_documents(texts, embedder).as_retriever(search_kwargs={\"k\": 10})# 4. Query the retriever to find relevant documentsdocs = retriever.invoke(\"What is the definition of Multimodal?\")# 5. Print the relevant documentsfor i, d in enumerate(docs):\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)\n\nCreated a chunk of size 419, which is longer than the specified 400\n\n\ndocument 1:\n\nSemantic Search\ndocument 2:\n\nDefinition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\nExample: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\nRelated Keywords: Natural Language Processing, Search Algorithms, Data Mining\ndocument 3:\n\nDefinition: A token refers to a smaller unit of text obtained by splitting a larger text. It can be a word, sentence, or phrase.\nExample: The sentence “I go to school” can be split into tokens: “I”, “go”, “to”, “school”.\nRelated Keywords: Tokenization, Natural Language Processing, Parsing\n\nTokenizer\ndocument 4:\n\nDefinition: A tokenizer is a tool that splits text data into tokens. It is commonly used in natural language processing for data preprocessing.\nExample: The sentence “I love programming.” can be tokenized into [“I”, “love”, “programming”, “.”].\nRelated Keywords: Tokenization, Natural Language Processing, Parsing\n\nVectorStore\ndocument 5:\n\nDefinition: A vector store is a system for storing data in vector form. It is used for tasks like retrieval, classification, and other data analysis.\nExample: Word embedding vectors can be stored in a database for quick access.\nRelated Keywords: Embedding, Database, Vectorization\n\nSQL\ndocument 6:\n\nDefinition: SQL (Structured Query Language) is a programming language for managing data in databases. It supports operations like querying, modifying, inserting, and deleting data.\nExample: SELECT * FROM users WHERE age &gt; 18; retrieves information about users older than 18.\nRelated Keywords: Database, Query, Data Management\n\nCSV\ndocument 7:\n\nDefinition: CSV (Comma-Separated Values) is a file format for storing data where each value is separated by a comma. It is often used for simple data storage and exchange in tabular form.\nExample: A CSV file with headers “Name, Age, Job” might contain data like “John Doe, 30, Developer”.\nRelated Keywords: File Format, Data Handling, Data Exchange\n\nJSON\ndocument 8:\n\nDefinition: JSON (JavaScript Object Notation) is a lightweight data exchange format that represents data objects in a human- and machine-readable text format.\nExample: {\"name\": \"John Doe\", \"age\": 30, \"job\": \"Developer\"} is an example of JSON data.\nRelated Keywords: Data Exchange, Web Development, API\n\nTransformer\ndocument 9:\n\nDefinition: A transformer is a type of deep learning model used in natural language processing for tasks like translation, summarization, and text generation. It is based on the attention mechanism.\nExample: Google Translate uses transformer models to perform translations between languages.\nRelated Keywords: Deep Learning, Natural Language Processing, Attention\n\nHuggingFace\ndocument 10:\n\nDefinition: HuggingFace is a library that provides pre-trained models and tools for natural language processing, making NLP tasks more accessible to researchers and developers.\nExample: HuggingFace’s Transformers library can be used for tasks like sentiment analysis and text generation.\nRelated Keywords: Natural Language Processing, Deep Learning, Library\n\nDigital Transformation\n\n使用 LLMChainExtractor 创建的 DocumentCompressor 正是应用于检索器的，即 ContextualCompressionRetriever。\nContextualCompressionRetriever 会通过去除无关信息并专注于最相关的信息来压缩文档。\nLLMChainFilterLLMChainFilter 是一个简单但强大的压缩器，它使用 LLM 链来决定从最初检索到的文档中哪些应该被过滤，哪些应该被返回。\nfrom langchain.retrievers import ContextualCompressionRetrieverfrom langchain.retrievers.document_compressors import LLMChainExtractorfrom langchain_openai import ChatOpenAI# Before applying ContextualCompressionRetrieverdocs = retriever.invoke(\"What is the definition of Multimodal?\")for i, d in enumerate(docs):\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)print(\"=\"*62)print(\"=\"*15 + \"After applying LLMChainExtractor\" + \"=\"*15)# After applying ContextualCompressionRetriever# 1. Generate LLMllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)# 2. Generate compressor using LLMChainExtractorcompressor = LLMChainExtractor.from_llm(llm)# 3. Generate compression retriever using ContextualCompressionRetrievercompression_retriever = ContextualCompressionRetriever(    base_compressor=compressor,    base_retriever=retriever,)# 4. Query the compression retriever to find relevant documentscompressed_docs = (    compression_retriever.invoke(         \"What is the definition of Multimodal?\"    ))# 5. Print the relevant documentsfor i, d in enumerate(compressed_docs):\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)\n\ndocument 1:\n\nSemantic Search\ndocument 2:\n\nDefinition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\nExample: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\nRelated Keywords: Natural Language Processing, Search Algorithms, Data Mining\ndocument 3:\n\nDefinition: A token refers to a smaller unit of text obtained by splitting a larger text. It can be a word, sentence, or phrase.\nExample: The sentence “I go to school” can be split into tokens: “I”, “go”, “to”, “school”.\nRelated Keywords: Tokenization, Natural Language Processing, Parsing\n\nTokenizer\ndocument 4:\n\nDefinition: A tokenizer is a tool that splits text data into tokens. It is commonly used in natural language processing for data preprocessing.\nExample: The sentence “I love programming.” can be tokenized into [“I”, “love”, “programming”, “.”].\nRelated Keywords: Tokenization, Natural Language Processing, Parsing\n\nVectorStore\ndocument 5:\n\nDefinition: A vector store is a system for storing data in vector form. It is used for tasks like retrieval, classification, and other data analysis.\nExample: Word embedding vectors can be stored in a database for quick access.\nRelated Keywords: Embedding, Database, Vectorization\n\nSQL\ndocument 6:\n\nDefinition: SQL (Structured Query Language) is a programming language for managing data in databases. It supports operations like querying, modifying, inserting, and deleting data.\nExample: SELECT * FROM users WHERE age &gt; 18; retrieves information about users older than 18.\nRelated Keywords: Database, Query, Data Management\n\nCSV\ndocument 7:\n\nDefinition: CSV (Comma-Separated Values) is a file format for storing data where each value is separated by a comma. It is often used for simple data storage and exchange in tabular form.\nExample: A CSV file with headers “Name, Age, Job” might contain data like “John Doe, 30, Developer”.\nRelated Keywords: File Format, Data Handling, Data Exchange\n\nJSON\ndocument 8:\n\nDefinition: JSON (JavaScript Object Notation) is a lightweight data exchange format that represents data objects in a human- and machine-readable text format.\nExample: {\"name\": \"John Doe\", \"age\": 30, \"job\": \"Developer\"} is an example of JSON data.\nRelated Keywords: Data Exchange, Web Development, API\n\nTransformer\ndocument 9:\n\nDefinition: A transformer is a type of deep learning model used in natural language processing for tasks like translation, summarization, and text generation. It is based on the attention mechanism.\nExample: Google Translate uses transformer models to perform translations between languages.\nRelated Keywords: Deep Learning, Natural Language Processing, Attention\n\nHuggingFace\ndocument 10:\n\nDefinition: HuggingFace is a library that provides pre-trained models and tools for natural language processing, making NLP tasks more accessible to researchers and developers.\nExample: HuggingFace’s Transformers library can be used for tasks like sentiment analysis and text generation.\nRelated Keywords: Natural Language Processing, Deep Learning, Library\n\nDigital Transformation\n==============================================================\n===============After applying LLMChainExtractor===============\n\n大模型把无关内容都过滤了, 虽然我 embedding 很拉, 没能抽到相关内容以下是一个过滤效果的展示, 把定义成功保留, 示例被过滤掉\ntext = \\\"\"\"MultimodalDefinition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.Relate\"\"\"docs = [Document(text)]query = \"What is the definition of Multimodal?\"compressed_docs = compressor.compress_documents(docs, query)print(compressed_docs[0].page_content)\n\nMultimodal\nDefinition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n\n源码分析\n这是 ContextualCompressionRetriever 的检索函数 _get_relevant_documents的关键代码:\ndocs = self.base_retriever.invoke(\tquery, config={\"callbacks\": run_manager.get_child()}, **kwargs)if docs:\tcompressed_docs = self.base_compressor.compress_documents(\t\tdocs, query, callbacks=run_manager.get_child()\t)\treturn list(compressed_docs)else:\treturn []\n首先还是 base_retriever 支持返回检索结果, 再接过 base_compressor 压缩\n这是 base_compresser 类 LLMChainExtractor 的 compress_documents函数关键部分:\ncompressed_docs = []for doc in documents:\t_input = self.get_input(query, doc) # 产生 {\"question\": query, \"context\": doc.page_content}\toutput_ = self.llm_chain.invoke(_input, config={\"callbacks\": callbacks}) # 调用大模型抽取内容\tif isinstance(self.llm_chain, LLMChain):\t\toutput = output_[self.llm_chain.output_key]\t\tif self.llm_chain.prompt.output_parser is not None:\t\t\toutput = self.llm_chain.prompt.output_parser.parse(output)\telse:\t\toutput = output_\tif len(output) == 0:\t\tcontinue\tcompressed_docs.append(\t\tDocument(page_content=cast(str, output), metadata=doc.metadata)\t)return compressed_docs\n\n这是调用大模型抽取内容的 prompt 模板\n\"\"\"Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return {no_output_str}. Remember, *DO NOT* edit the extracted parts of the context.&gt; Question: {{question}}&gt; Context:&gt;&gt;&gt;{{context}}&gt;&gt;&gt;Extracted relevant parts:\"\"\"\n\nEmbeddingsFilter对每个检索到的文档执行额外的 LLM 调用既昂贵又缓慢。EmbeddingsFilter 提供了一个更经济且更快速的选项，通过嵌入文档和查询，只返回那些与查询的嵌入相似度足够高的文档。\n这种方法在保持搜索结果相关性的同时，节省了计算成本和时间。该过程涉及使用 EmbeddingsFilter 和 ContextualCompressionRetriever 压缩并检索相关文档。\n\nEmbeddingsFilter 用于过滤超过指定相似度阈值（0.86）的文档。\n\nfrom langchain.retrievers.document_compressors import EmbeddingsFilterfrom langchain_openai import OpenAIEmbeddings# 1. Generate embeddings using OpenAIEmbeddingsembeddings = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)# 2. Generate EmbedingsFilter object that has similarity threshold of 0.86embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)# 3. Generate ContextualCompressionRetriever object using EmbeddingsFilter and retrievercompression_retriever = ContextualCompressionRetriever(    base_compressor=embeddings_filter,     base_retriever=retriever)# 4. Query the compression retriever to find relevant documentscompressed_docs = compression_retriever.invoke(    \"What is the definition of Multimodal?\")# 5. Print the relevant documentsfor i, d in enumerate(compressed_docs):\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)\n\n这个方法也只是将 base_retriever 的返回结果经过 EmbeddingsFilter 的相似度阈值过滤, 可以选择更强的 embedding model 来强化相似度准确度\nEnsemble Retriever 多路召回EnsembleRetriever 集成了稀疏和密集检索算法的优点，通过使用权重和运行时配置来定制性能。\n关键特点\n\n集成多个检索器：接受不同类型的检索器作为输入并结合结果。\n结果重新排序：使用倒排排名融合算法重新排序结果。\n混合检索：主要使用稀疏检索器（例如 BM25）和密集检索器（例如 嵌入相似度）相结合。\n\n优势\n\n稀疏检索器：有效进行基于关键词的检索。\n密集检索器：有效进行基于语义相似度的检索。\n\n由于这些互补特性，EnsembleRetriever 可以在各种检索场景中提供更好的性能。\nfrom langchain.retrievers import BM25Retriever, EnsembleRetrieverfrom langchain.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddings# list sample documentsdoc_list = [    \"I like apples\",    \"I like apple company\",    \"I like apple's iphone\",    \"Apple is my favorite company\",    \"I like apple's ipad\",    \"I like apple's macbook\",]# Initialize the bm25 retriever and faiss retriever.bm25_retriever = BM25Retriever.from_texts(    doc_list,)bm25_retriever.k = 2  # Set the number of search results for BM25Retriever to 1.embedding = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,\t)faiss_vectorstore = FAISS.from_texts(    doc_list,    embedding,)faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})# Initialize the ensemble retriever.ensemble_retriever = EnsembleRetriever(    retrievers=[bm25_retriever, faiss_retriever],    weights=[0.7, 0.3],)\n\n\n# Get the search results document.query = \"my favorite fruit is apple\"ensemble_result = ensemble_retriever.invoke(query)bm25_result = bm25_retriever.invoke(query)faiss_result = faiss_retriever.invoke(query)# Output the fetched documents.print(\"[Ensemble Retriever]\")for doc in ensemble_result:    print(f\"Content: {doc.page_content}\")    print()print(\"[BM25 Retriever]\")for doc in bm25_result:    print(f\"Content: {doc.page_content}\")    print()print(\"[FAISS Retriever]\")for doc in faiss_result:    print(f\"Content: {doc.page_content}\")    print()\n\n[Ensemble Retriever]\nContent: Apple is my favorite company\n\nContent: I like apple company\n\nContent: I like apples\n\n[BM25 Retriever]\nContent: Apple is my favorite company\n\nContent: I like apple company\n\n[FAISS Retriever]\nContent: Apple is my favorite company\n\nContent: I like apples\n\n源码分析\nEnsembleRetriever 的 rank_fusion 函数:\nretriever_docs = [\tretriever.invoke(\t\tquery,\t\tpatch_config(\t\t\tconfig, callbacks=run_manager.get_child(tag=f\"retriever_{i + 1}\")\t\t),\t)\tfor i, retriever in enumerate(self.retrievers)]# Enforce that retrieved docs are Documents for each list in retriever_docsfor i in range(len(retriever_docs)):\tretriever_docs[i] = [\t\tDocument(page_content=cast(str, doc)) if isinstance(doc, str) else doc\t\tfor doc in retriever_docs[i]\t]# apply rank fusionfused_documents = self.weighted_reciprocal_rank(retriever_docs)\n\n每个 retriever 单独调用, 返回多组 Documents, 再经过 weighted_reciprocal_rank:\nrrf_score: Dict[str, float] = defaultdict(float)for doc_list, weight in zip(doc_lists, self.weights):\tfor rank, doc in enumerate(doc_list, start=1):\t\trrf_score[\t\t\t(\t\t\t\tdoc.page_content\t\t\t\tif self.id_key is None\t\t\t\telse doc.metadata[self.id_key]\t\t\t)\t\t] += weight / (rank + self.c)# Docs are deduplicated by their contents then sorted by their scoresall_docs = chain.from_iterable(doc_lists)sorted_docs = sorted(\tunique_by_key(\t\tall_docs,\t\tlambda doc: (\t\t\tdoc.page_content\t\t\tif self.id_key is None\t\t\telse doc.metadata[self.id_key]\t\t),\t),\treverse=True,\tkey=lambda doc: rrf_score[\t\tdoc.page_content if self.id_key is None else doc.metadata[self.id_key]\t],)\n基于 weights 对 Documents 重排序\nLong Context Reorder无论模型的架构如何，当检索的文档超过 10 个时，性能都会显著下降。\n简单来说，当模型需要在长上下文的中间部分访问相关信息时，它往往会忽视提供的文档。\n更多细节，请参阅以下论文：\n\nhttps://arxiv.org/abs/2307.03172\n\n为了避免这个问题，您可以在检索后重新排序文档，从而防止性能下降。\n可以创建一个检索器，它使用 Chroma 向量数据库存储和搜索文本数据。然后，使用检索器的 invoke 方法，针对给定的查询搜索出高度相关的文档。\nfrom langchain_core.prompts import PromptTemplatefrom langchain_community.vectorstores import Chromafrom langchain_openai import OpenAIEmbeddings# Get embeddingsembeddings = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,\t)texts = [    \"This is just a random text I wrote.\",    \"ChatGPT, an AI designed to converse with users, can answer various questions.\",    \"iPhone, iPad, MacBook are representative products released by Apple.\",    \"ChatGPT was developed by OpenAI and is continuously being improved.\",    \"ChatGPT has learned from vast amounts of data to understand user questions and generate appropriate answers.\",    \"Wearable devices like Apple Watch and AirPods are also part of Apple's popular product line.\",    \"ChatGPT can be used to solve complex problems or suggest creative ideas.\",    \"Bitcoin is also called digital gold and is gaining popularity as a store of value.\",    \"ChatGPT's capabilities are continuously evolving through ongoing learning and updates.\",    \"The FIFA World Cup is held every four years and is the biggest event in international football.\",]# Create a retriever (Set K to 10)retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(    search_kwargs={\"k\": 10})\n\n\nquery = \"What can you tell me about ChatGPT?\"# Retrieves relevant documents sorted by relevance score.docs = retriever.invoke(query)docs\n\n\n\n\n[Document(metadata={}, page_content='Bitcoin is also called digital gold and is gaining popularity as a store of value.'),\n Document(metadata={}, page_content='The FIFA World Cup is held every four years and is the biggest event in international football.'),\n Document(metadata={}, page_content=\"Wearable devices like Apple Watch and AirPods are also part of Apple's popular product line.\"),\n Document(metadata={}, page_content='iPhone, iPad, MacBook are representative products released by Apple.'),\n Document(metadata={}, page_content='This is just a random text I wrote.'),\n Document(metadata={}, page_content='ChatGPT, an AI designed to converse with users, can answer various questions.'),\n Document(metadata={}, page_content='ChatGPT was developed by OpenAI and is continuously being improved.'),\n Document(metadata={}, page_content='ChatGPT has learned from vast amounts of data to understand user questions and generate appropriate answers.'),\n Document(metadata={}, page_content='ChatGPT can be used to solve complex problems or suggest creative ideas.'),\n Document(metadata={}, page_content=\"ChatGPT's capabilities are continuously evolving through ongoing learning and updates.\")]\n\n创建一个 LongContextReorder 类的实例。\n\n调用 reordering.transform_documents(docs) 来重新排序文档列表。\n相关性较低的文档会被置于列表的中间，而相关性较高的文档会被放置在列表的开头和结尾。\n\nfrom langchain_community.document_transformers import LongContextReorderreordering = LongContextReorder()reordered_docs = reordering.transform_documents(docs)reordered_docs\n\n\n\n\n[Document(metadata={}, page_content='The FIFA World Cup is held every four years and is the biggest event in international football.'),\n Document(metadata={}, page_content='iPhone, iPad, MacBook are representative products released by Apple.'),\n Document(metadata={}, page_content='ChatGPT, an AI designed to converse with users, can answer various questions.'),\n Document(metadata={}, page_content='ChatGPT has learned from vast amounts of data to understand user questions and generate appropriate answers.'),\n Document(metadata={}, page_content=\"ChatGPT's capabilities are continuously evolving through ongoing learning and updates.\"),\n Document(metadata={}, page_content='ChatGPT can be used to solve complex problems or suggest creative ideas.'),\n Document(metadata={}, page_content='ChatGPT was developed by OpenAI and is continuously being improved.'),\n Document(metadata={}, page_content='This is just a random text I wrote.'),\n Document(metadata={}, page_content=\"Wearable devices like Apple Watch and AirPods are also part of Apple's popular product line.\"),\n Document(metadata={}, page_content='Bitcoin is also called digital gold and is gaining popularity as a store of value.')]\n\n源码分析\ndocuments.reverse()reordered_result = []for i, value in enumerate(documents):\tif i % 2 == 1:\t\treordered_result.append(value)\telse:\t\treordered_result.insert(0, value)\n原顺序是相似度由高到低的, 他只是在原顺序的基础上把高相似度的放散在头部和尾部, 低相关的放在中部.  \n\n当模型需要在长上下文的中间部分访问相关信息时，它往往会忽视提供的文档\n\n按这种说法, 模型会更注重头部和尾部的文档\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"Langchain 入门教程 - 11.检索增强生成 RAG","url":"/2025/05/04/langchain-tutorial-series/11.RAG/","content":"Reranker目的:  \n\n对检索到的文档进行重排序，优化其排名，优先展示与查询最相关的结果。\n\n结构:  \n\n接受查询和文档作为单一输入对，进行联合处理。\n\n机制:  \n\n单一输入对：将查询和文档作为组合输入，直接输出相关性评分。\n自注意力机制：使用自注意力机制联合分析查询和文档，有效捕捉它们之间的语义关系。\n\n优势:  \n\n更高的准确性：提供更精确的相似度评分。\n深度语境分析：探索查询和文档之间的语义细微差别。\n\n局限性:  \n\n高计算成本：处理可能会消耗较多时间。\n扩展性问题：未经过优化时，不适用于大规模文档集合。\n\n实际应用:  \n\n双编码器（Bi-Encoder）通过计算轻量级的相似度分数快速检索候选文档。\n交叉编码器（Cross Encoder）通过深入分析查询与检索到的文档之间的语义关系，进一步优化这些结果。\n\nfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Load documentsdocuments = TextLoader(\"./data/appendix-keywords.txt\").load()# Configure text splittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)# Split documents into chunkstexts = text_splitter.split_documents(documents)# # Set up the embedding modelembeddings_model = OpenAIEmbeddings(\tmodel=\"bge-m3\",\tbase_url='http://localhost:9997/v1',\tapi_key='cannot be empty',\t# dimensions=1024,)# Create FAISS index from documents and set up retrieverretriever = FAISS.from_documents(texts, embeddings_model).as_retriever(    search_kwargs={\"k\": 30})# Define the queryquery = \"Can you tell me about Word2Vec?\"# Execute the query and retrieve resultsdocs = retriever.invoke(query)# Display the retrieved documentsfor i, d in enumerate(docs):\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)\tprint('-' * 100)\n\n使用 ContextualCompressionRetriever 来包装 base_retriever。CrossEncoderReranker 利用 HuggingFaceCrossEncoder 对检索到的结果进行重新排序。\n之前 ContextualCompressionRetriever 只是对文档压缩, 现在起到过滤作用\nfrom langchain.retrievers import ContextualCompressionRetrieverfrom langchain.retrievers.document_compressors import CrossEncoderRerankerfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder# Initialize the modelmodel = HuggingFaceCrossEncoder(\tmodel_name=\"../DataCollection/officials/bge-reranker-v2-m3\",\tmodel_kwargs = {'device': 'cuda:6'}\t)# Select the top 3 documentscompressor = CrossEncoderReranker(model=model, top_n=3)# Initialize the contextual compression retrievercompression_retriever = ContextualCompressionRetriever(    base_compressor=compressor, base_retriever=retriever)# Retrieve compressed documentscompressed_docs = compression_retriever.invoke(\"Can you tell me about Word2Vec?\")# Display the documentsfor i, d in enumerate(docs):\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)\tprint('-' * 100)\n\n我没有在 langchain 上找到基于 api 的 reranker 类\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"单轮对话->多轮对话->简单agent示例","url":"/2025/05/04/agent/simple_agent/","content":"单轮对话以下是一个很基础的调用大模型的方式，除了 model, temperature 等参数外，最重要的就是 messages，即对话记录。大模型将基于传入的对话记录生成一个新的 message 。\nimport osfrom openai import OpenAIdefault_client = OpenAI(    api_key=\"xxx\",     base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",)if __name__ == \"__main__\":\tcompletion = default_client.chat.completions.create(\t\tmodel=\"qwen-turbo\",\t\tmessages=[\t\t\t{'role': 'system', 'content': 'You are a helpful assistant.'},\t\t\t{'role': 'user', 'content': '你是谁？'}],\t\t\ttemperature=0.8,\t\tmax_tokens=120,\t\ttop_p=0.9,\t\tfrequency_penalty=0.5,\t\tpresence_penalty=0.0,\t\t)\t\t# print(completion.choices[0].message.content)\tprint(completion.model_dump_json(indent=2))\t\n\n以下是结果\n{  \"id\": \"chatcmpl-cf215b2e-e870-9acd-90cb-e5e6a6a531b0\",  \"choices\": [    {      \"finish_reason\": \"stop\",      \"index\": 0,      \"logprobs\": null,      \"message\": {        \"content\": \"我是通义千问，阿里巴巴集团旗下的超大规模语言模型。我能够帮助你回答问题、创作文字，如写故事、公文、技术文档等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，随时可以告诉我！\",        \"refusal\": null,        \"role\": \"assistant\",        \"audio\": null,        \"function_call\": null,        \"tool_calls\": null      }    }  ],  \"created\": 1745847448,  \"model\": \"qwen-turbo\",  \"object\": \"chat.completion\",  \"service_tier\": null,  \"system_fingerprint\": null,  \"usage\": {    \"completion_tokens\": 53,    \"prompt_tokens\": 22,    \"total_tokens\": 75,    \"completion_tokens_details\": null,    \"prompt_tokens_details\": {      \"audio_tokens\": null,      \"cached_tokens\": 0    }  }}\n\n最主要的参数是 choices[0].message.content，这是大模型返回结果的头条结果 (这并不代表这个结果是概率最大的结果，如果是采样的话，那结果就是随机采集的，当然会排除小概率的序列)\n多轮对话单轮对话仅适用于简单的或者一次性的任务，现在如果你对大模型的输出不满意，你不需要从头开始要求大模型，而是直接在以上对话的基础上 (你最初的问题/任务，大模型首次答案)，追加你的反馈信息，然后以这些 message 作为历史信息，使大模型生成新的答案。\n多轮对话实际上就是不断的把大模型输出结果和新的用户输入加入对话历史，然后再将所有对话历史输入大模型，产生新的回答。因此只要维护一个对话历史，以及新 message 的加入，就可实现多轮对话。\nfrom openai import OpenAIfrom openai_client import default_clientdefault_chat_config = {\t\"model\": \"qwen-turbo\",\t\"temperature\": 0.8,\t\"max_tokens\": 120,\t\"top_p\": 0.9,\t\"frequency_penalty\": 0.5,\t\"presence_penalty\": 0.0,}class ChatBot:    # 初始化函数，指定调用大模型的超参，和最基础的 system prompt\tdef __init__(self, \t\t\t\topenai_client:OpenAI=default_client,\t\t\t\tsystem_prompt:str=\"You are a helpful assistant.\",\t\t\t\tchat_config:dict=default_chat_config\t\t\t\t):\t\t\t\tself.openai_client = openai_client\t\tself.system_prompt = system_prompt\t\tself.chat_config = chat_config\t\t\t\tself.messages = []\t\tif self.system_prompt != \"\":\t\t\tself.messages.append({\"role\": \"system\", \"content\": system_prompt})\t# 执行大模型生成，仅是将维护的对话传入大模型\tdef execute(self):\t\tcompletion = self.openai_client.chat.completions.create(\t\t\tmessages=self.messages,\t\t\t**self.chat_config\t\t)\t\treturn completion.choices[0].message.content\t# 实际调用多轮对话的函数，每次将用户输入作为新的 user message 加入 messages，然后调用执行函数获得大模型的回答，将这个回答加入 messages\tdef __call__(self, message):\t\tself.messages.append({\"role\": \"user\", \"content\": message})\t\tresult = self.execute()\t\tself.messages.append({\"role\": \"assistant\", \"content\": result})\t\treturn result\t\t# 清空 messages\tdef clear_chat_history(self):\t\tself.messages = []\t\tif self.system_prompt != \"\":\t\t\tself.messages.append({\"role\": \"system\", \"content\": self.system_prompt})if __name__ == \"__main__\":\tchatbot = ChatBot()\tprint(f\"[System Prompt]:\\n{chatbot.system_prompt}\")\twhile True:\t\tuser_input = input(\"[User]:\\n\")\t\tif user_input.lower() == \"exit\":\t\t\tbreak\t\t\t\tresponse = chatbot(user_input)\t\tprint(f\"[AI]:\\n{response}\")\n\n接下来简单用下多轮对话，主要验证大模型是否能记得对话历史，这里以我提前声明我的名字再提问大模型的方式验证。\n[System Prompt]:You are a helpful assistant.[User]:I am john, what's your name?[AI]:Hello John! My name is Qwen. Nice to meet you! How can I assist you today?[User]:What's my name?[AI]:I'm sorry, but I don't have access to any personal information about you, including your name. You mentioned that you are John, but I don't have any way of verifying that. Is there anything else you'd like to share or ask? 😊\n\n\n\n简单agentagent 的流程是在多轮对话的基础上，加上大模型执行工具的能力，这个是由我们控制，大模型只是产生调用工具的超参。\nagent 的流程参考了 ReAct 思路，需要在动作之前思考，然后执行动作 (在我的 prompt 里动作分为了正常的 action 和 answer)。如果不用 ReAct，那 agent 会不断执行 action 最后 answer。\n问题 --&gt; 调用工具 --&gt; 是否可以回答(非必要显式判断) --&gt; 回答\t\t\tA             |\t\t\t|-------------|\n\n\n\n代码如下:\nimport jsonimport refrom chat_bot import ChatBotfrom typing import Callable, Dictdef calculator(expression: str) -&gt; str:\ttry:\t\treturn str(eval(expression))\texcept Exception as e:\t\treturn f\"Calculator Error: {str(e)}\"def reverse_text(text: str) -&gt; str:\treturn text[::-1]class Agent:\tdef __init__(\t\tself,\t\tchatbot: ChatBot,\t\ttools: Dict[str, Callable] = None,\t):\t\tself.chatbot = chatbot\t\tself.tools = tools or {\t\t\t\"calculator\": calculator,\t\t\t\"reverse_text\": reverse_text,\t\t}\t\tself.chatbot.clear_chat_history()\t\tself.chatbot.messages.append({\t\t\t\"role\": \"system\",\t\t\t\"content\": self._build_agent_prompt()\t\t})\t    # 生成system prompt (包含工具的说明、每次输出的说明、agent 执行流程)\tdef _build_agent_prompt(self):\t\ttool_descriptions = \"\\n\".join([\t\t\t\"- calculator: evaluates math expressions. Params: {'expression': str}\",\t\t\t\"- reverse_text: reverses a string. Params: {'text': str}\",\t\t])\t\treturn f\"\"\"You are a structured reasoning agent. You can use the following tools:{tool_descriptions}You run in a loop of Reasoning, Action, and Execution. If you have enough information to answer the question, you should do so. Otherwise, you will carefully oberve, reason, and use one of the tools to get the information you need. 1. Reason and Action: In this step, you will reason about the question and decide which tool to use and call one of the tools with the required parameters. You must reply using this format:&lt;thought&gt;your current reasoning&lt;/thought&gt;&lt;action&gt;{{\"function_name\": \"tool_name\", \"function_params\": {{\"param1\": \"...\", \"param2\": \"...\"}}}}&lt;/action&gt;After you call a tool, you will receive an observation. 2. Observation: It is a passive step where the output of the tool will be returned in the role of User. The observation will be in the format:&lt;observation&gt;tool execution output here&lt;/observation&gt;Only use one tool at a time. Always use valid JSON for &lt;action&gt;. After you receive the observation, you will reason again and decide. If you can answer the question based on existing information or you sense it is a insolvable question, then go to step 3. Otherwise, if you cannot answer the question based on existing information and you still think it is a olvable question, you will go to step 1. 3. Reason and Answer: After you receive the observation, you will reason again and decide if you can answer the question. If you can, reply with:&lt;thought&gt;your current reasoning&lt;/thought&gt;&lt;answer&gt;your final answer&lt;/answer&gt;\"\"\"\tdef __call__(self, user_input: str):        # 用户输入\t\tself.chatbot.messages.append({\"role\": \"user\", \"content\": f\"User Input: {user_input}\"})        # 死循环，当大模型输出真正答案时 break\t\twhile True:            # 大模型返回\t\t\tresponse = self.chatbot.execute()\t\t\t# print(response)            # 解析返回结果            # 我用类 html 格式包裹住相关内容\t\t\t# Check if it's the final answer\t\t\tfinal_match = re.search(r\"&lt;answer&gt;(.*?)&lt;/answer&gt;\", response, re.DOTALL)\t\t\tif final_match:                print(f'---\\n[AI final response]:\\n{response}\\n---')\t\t\t\tself.chatbot.messages.append({\"role\": \"assistant\", \"content\": response})\t\t\t\treturn final_match.group(1).strip()\t\t\t# Extract tool call\t\t\taction_match = re.search(r\"&lt;action&gt;\\n\\s*(\\{.*?\\})\\s*\\n&lt;/action&gt;\", response, re.DOTALL)\t\t\tif not action_match:\t\t\t\treturn \"Invalid format: &lt;action&gt; block not found.\"\t\t\ttry:\t\t\t\taction_dict = json.loads(action_match.group(1))\t\t\t\tfunc_name = action_dict[\"function_name\"]\t\t\t\tparams = action_dict[\"function_params\"]\t\t\t\tassert isinstance(params, dict)\t\t\texcept Exception as e:\t\t\t\treturn f\"Error parsing &lt;action&gt;: {e}\"\t\t\tif func_name not in self.tools:\t\t\t\tobservation = f\"Unknown tool: {func_name}\"\t\t\telse:\t\t\t\ttry:\t\t\t\t\tobservation = self.tools[func_name](**params)\t\t\t\t\tobservation = f\"tool execution results: \\n{observation}\"\t\t\t\texcept Exception as e:\t\t\t\t\tobservation = f\"Error during tool execution: {e}\"\t\t\t# 把大模型输出和工具调用结果加入对话历史\t\t\t# Append response and observation\t\t\tappending_new_ai_message = {\t\t\t\t\"role\": \"assistant\",\t\t\t\t\"content\": response\t\t\t}\t\t\tappending_new_tool_message = {\t\t\t\t\"role\": \"user\",\t\t\t\t\"content\": f\"&lt;observation&gt;\\n{observation}\\n&lt;/observation&gt;\"\t\t\t}                        # 输出中间结果方便检查\t\t\tprint(f'---\\n[AI intermediate response]:\\n{appending_new_ai_message[\"content\"]}\\n---')\t\t\tprint(f'---\\n[tool intermediate response]:\\n{appending_new_tool_message[\"content\"]}\\n---')\t\t\tself.chatbot.messages.append(appending_new_ai_message)\t\t\tself.chatbot.messages.append(appending_new_tool_message)if __name__ == \"__main__\":\tagent = Agent(ChatBot())\tprint(agent(\"What is the result of 12 * 7 + 3?\"))# print(agent(\"Reverse the phrase 'Hello Agent'\"))\n\n以下是对话记录，注意 [...] 是我方便区分对话角色的修饰，下面的才是真正内容。\n---[AI intermediate response]:&lt;thought&gt;To solve this problem, I need to use the calculator tool to evaluate the expression.&lt;/thought&gt;&lt;action&gt;{\"function_name\": \"calculator\", \"function_params\": {\"expression\": \"12 * 7 + 3\"}}&lt;/action&gt;------[tool intermediate response]:&lt;observation&gt;tool execution results: 87&lt;/observation&gt;------[AI final response]:&lt;thought&gt;The calculator tool returned the result of the expression 12 * 7 + 3 as 87.&lt;/thought&gt;&lt;answer&gt;The result of 12 * 7 + 3 is 87.&lt;/answer&gt;---The result of 12 * 7 + 3 is 87.\n\n在用户输出的基础上，大模型输出格式化的结构，以非 ReAct 的思路来讲，每次检查大模型输出是 action 还是 answer。\n\n如果是 action，那解析为函数名和函数超参，接下来执行这个函数，并将其结果作为 observation 返回给大模型 (这里是以 user 身份告知大模型)。这一步 (生成工具参数 + 返回工具调用结果) 可能会执行多次，知道大模型觉得可以回答问题了。\n如果是 answer，那将回答返回给用户，这就是 agent 流程的最后一步。\n\n注意 ReAct 思路只是为了在 action 之前显式的输出中间思考过程。\n","categories":["agent"]},{"title":"Langchain 入门教程 - 12.智能体 agent","url":"/2025/05/04/langchain-tutorial-series/12.Agent/","content":"Tool工具是一个接口，允许代理、链条或大语言模型与外部世界互动。\nLangChain 提供了易于使用的内置工具，并且还允许用户轻松构建自定义工具。\n你可以在下面的链接中找到集成到 LangChain 中的工具列表。\n\n集成到 LangChain 中的工具列表\n\n内置工具 Built-in tools你可以使用 LangChain 提供的预定义工具和工具包。\n工具指的是单一的实用工具，而工具包将多个工具组合成一个单元供使用。\n你可以在下面的链接中找到相关的工具。\n注意\n\nLangChain 工具/工具包\n\nweb 检索工具\nfrom langchain_community.utilities import GoogleSerperAPIWrappersearch = GoogleSerperAPIWrapper()search.run(\"Obama's first name?\")\n\n图片生成工具\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import ChatOpenAI# Initialize the ChatOpenAI modelllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)# Define a prompt template for DALL-E image generationprompt = PromptTemplate.from_template(    \"Generate a detailed IMAGE GENERATION prompt for DALL-E based on the following description. \"    \"Return only the prompt, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the prompt\"    \"Output should be less than 1000 characters. Write in English only.\"    \"Image Description: \\n{image_desc}\",)# Create a chain connecting the prompt, LLM, and output parserchain = prompt | llm | StrOutputParser()# Execute the chainimage_prompt = chain.invoke(    {\"image_desc\": \"A Neo-Classicism painting satirizing people looking at their smartphones.\"})# Output the image promptprint(image_prompt)\n\n几乎所有 tool 都是需要 api key 的\nPython REPL 工具此工具提供一个类，用于在 REPL (Read-Eval-Print Loop) 环境中执行 Python 代码。\n\nPythonREPLTool\n\n描述\n\n提供一个 Python shell 环境。\n执行有效的 Python 命令作为输入。\n使用 print(...) 函数查看结果。\n\n主要特点\n\nsanitize_input：可选项，用于清理输入（默认：True）\npython_repl：PythonREPL 的实例（默认：在全局作用域中执行）\n\n使用方法\n\n创建 PythonREPLTool 的实例。\n使用 run、arun 或 invoke 方法执行 Python 代码。\n\n输入清理\n\n从输入字符串中移除不必要的空格、反引号、关键字 “python” 和其他多余的元素。\n\nfrom langchain_experimental.tools import PythonREPLTool# Creates a tool for executing Python code.python_tool = PythonREPLTool()# Executes Python code and returns the results.print(python_tool.invoke(\"print(100 + 200)\"))\n\n下面是请求大语言模型编写 Python 代码并返回结果的示例。\n工作流程概述\n\n请求大语言模型为特定任务编写 Python 代码。\n执行生成的代码以获取结果。\n输出结果。\n\nfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnableLambdapython_tool = PythonREPLTool()# A function that executes Python code, outputs intermediate steps, and returns the tool execution results.def print_and_execute(code, debug=True):    if debug:        print(\"CODE:\")        print(code)    return python_tool.invoke(code)# A prompt requesting Python code to be written.prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are Raymond Hetting, an expert python programmer, well versed in meta-programming and elegant, concise and short but well documented code. You follow the PEP8 style guide. \"            \"Return only the code, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the code.\",        ),        (\"human\", \"{input}\"),    ])# Create LLM model.llm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)# Create a chain using the prompt and the LLM model.chain = prompt | llm | StrOutputParser() | RunnableLambda(print_and_execute)# Outputting the results.print(chain.invoke(\"Write code to generate Powerball numbers.\"))\n\n自定义工具除了 LangChain 提供的内置工具外，你还可以定义和使用自己的自定义工具。\n为此，可以使用 langchain.tools 模块提供的 @tool 装饰器将一个函数转换为工具。\n@tool 装饰器: 这个装饰器允许你将一个函数转换为工具。它提供了各种选项来定制工具的行为。\n使用方法\n\n在函数上方应用 @tool 装饰器。\n根据需要设置装饰器参数。\n\n使用这个装饰器，你可以轻松地将常规 Python 函数转换为强大的工具，从而实现自动化文档生成和灵活的接口创建。\nfrom langchain.tools import tool# Convert a function into a tool using a decorator.@tooldef add_numbers(a: int, b: int) -&gt; int:    \"\"\"Add two numbers\"\"\"    return a + b@tooldef multiply_numbers(a: int, b: int) -&gt; int:    \"\"\"Multiply two numbers\"\"\"    return a * b# Execute tool.print(add_numbers.invoke({\"a\": 3, \"b\": 4}))print(multiply_numbers.invoke({\"a\": 3, \"b\": 4}))\n\n创建一个用于 Google 新闻文章搜索的自定义工具定义 GoogleNews 类，该类将作为一个工具，用于搜索 Google 新闻文章。\n注意\n\n不需要 API 密钥（因为它使用 RSS 源）。\n\n此工具用于搜索由 news.google.com 提供的新闻文章。\n描述\n\n使用 Google 新闻搜索 API 来检索最新的新闻。\n允许基于关键词搜索新闻。\n\n主要参数\n\nk (int)：返回的最大搜索结果数（默认：5）。\n\n# hl: 语言, gl: 区域, ceid: 区域和语言代码url = f\"{self.base_url}?hl=en&amp;gl=US&amp;ceid=US:en\" \n\n在代码中，你可以通过修改语言 (hl)、区域 (gl) 和区域与语言代码 (ceid) 来调整搜索结果的语言和区域。\n注意\n将提供的代码保存为 google_news.py，然后你可以在其他文件中使用 from google_news import GoogleNews 进行导入。\nimport feedparserfrom urllib.parse import quotefrom typing import List, Dict, Optionalclass GoogleNews:    \"\"\"    This is a class for searching Google News and returning the results.    \"\"\"    def __init__(self):        \"\"\"        Initializes the GoogleNews class.        Sets the base_url attribute.        \"\"\"        self.base_url = \"https://news.google.com/rss\"    def _fetch_news(self, url: str, k: int = 3) -&gt; List[Dict[str, str]]:        \"\"\"        Fetches news from the given URL.        Args:            url (str): The URL to fetch the news from.            k (int): The maximum number of news articles to fetch (default: 3).        Returns:            List[Dict[str, str]]: A list of dictionaries containing news titles and links.        \"\"\"        news_data = feedparser.parse(url)        return [            {\"title\": entry.title, \"link\": entry.link}            for entry in news_data.entries[:k]        ]    def _collect_news(self, news_list: List[Dict[str, str]]) -&gt; List[Dict[str, str]]:        \"\"\"        Formats and returns the list of news articles.        Args:            news_list (List[Dict[str, str]]): A list of dictionaries containing news information.        Returns:            List[Dict[str, str]]: A list of dictionaries containing URLs and content.        \"\"\"        if not news_list:            print(\"No news available for the given keyword.\")            return []        result = []        for news in news_list:            result.append({\"url\": news[\"link\"], \"content\": news[\"title\"]})        return result    def search_latest(self, k: int = 3) -&gt; List[Dict[str, str]]:        \"\"\"        Searches for the latest news.        Args:            k (int): The maximum number of news articles to search for (default: 3).        Returns:            List[Dict[str, str]]: A list of dictionaries containing URLs and content.        \"\"\"        #url = f\"{self.base_url}?hl=ko&amp;gl=KR&amp;ceid=KR:ko\"        url = f\"{self.base_url}?hl=en&amp;gl=US&amp;ceid=US:en\" # hl: 언어, gl: 지역, ceid: 지역 및 언어 코드        news_list = self._fetch_news(url, k)        return self._collect_news(news_list)    def search_by_keyword(        self, keyword: Optional[str] = None, k: int = 3    ) -&gt; List[Dict[str, str]]:        \"\"\"        Searches for news using a keyword.          Args:            keyword (Optional[str]): The keyword to search for (default: None).            k (int): The maximum number of news articles to search for (default: 3).        Returns:            List[Dict[str, str]]: A list of dictionaries containing URLs and content.        \"\"\"        if keyword:            encoded_keyword = quote(keyword)            url = f\"{self.base_url}/search?q={encoded_keyword}\"        else:            url = f\"{self.base_url}?hl=en&amp;gl=US&amp;ceid=US:en\"        news_list = self._fetch_news(url, k)        return self._collect_news(news_list)google_tool = GoogleNews()\n\n\ngoogle_tool.search_by_keyword(\"AI Investment\")\n\n地址是 google 的, 得翻墙, 以下是示例结果  \n'https://news.google.com/rss/articles/CBMimAFBVV95cUxPNkFrLURMdEZWOV9zdmRrTUhNbVFkdWswZWx2Qmh4cTJlMmFIdmpsQ3doaVluenA3TEJaT0U3RWVmanl3TTQ5V3RfS3kyYVpydEloNWZXbjBmSF85MGR5cjNFSFI5eFhtTGdIVlNXX3UxNmxwMnVIb2NkTXA5WFVZR2hKLUw5RU9iT3k1Zno2UG10N2h1b2g5Sw?oc 'content': 'Nvidia Calls China\\'s DeepSeek an \"Excellent AI Advancement\": Should Investors Press the Buy Button? - The Motley Fool'},{'url': 'https://news.google.com/rss/articles/CBMikwFBVV95cUxPd2ZnMnNwSWo2ZGhVSUJuNHd5S1Y3WUNWSkM4a0h5aHZQWU8tdzdlaW9pb25RUnI2cEwyZGtTemo5VUgwTDNHLVppNkw2MXdsbTRnb0UteHhtaHgxV043ZE9ZeG5aLUlCTzBGSHc1TFJzaHJsZENObzMxdTlvaEcyaG9vSjlRSTFWYXJEelF6RkRETnc?oc=5', 'content': 'How DeepSeek is upending AI innovation and investment after sending tech leaders reeling - New York Post'},{'url': 'https://news.google.com/rss/articles/CBMivwFBVV95cUxNUGdjLVE5dFpLaVZOcFY1djBRQXBLeTNZalptNmstNXlWRkpvX1U2aTJ5cDNiS3RNT2JzeGI1SnlzTXIyS2dWcEdieDB4R1kxSEo2eXUydlRkVWlzOGdUTnVCQ2NwNjNjaFpCdVpxQkphZXYxLU9BaXhBWmdVYWVjQnY1N3Q1aUtqaER5LV9WVlNWZ3BXMk5WR0gwWnlIU3RIazJZelZJQUM1ek12ZDFodEg1eDFaRm56eTR5UEh3VQ?oc=5', 'content': 'DeepSeek Marks The End Of The First Phase Of The AI Investment Boom - Forbes'}]\n\n\n\n再用 tool 装饰器\nfrom langchain.tools import toolfrom typing import List, Dict# Create a tool for searching news by keyword@tooldef search_keyword(query: str) -&gt; List[Dict[str, str]]:    \"\"\"Look up news by keyword\"\"\"    print(query)    news_tool = GoogleNews()    return news_tool.search_by_keyword(query, k=5)\t# Execution Resultssearch_keyword.invoke({\"query\": \"LangChain AI\"})\n\nbind_toolsbind_tools 是 LangChain 中的一个强大功能，用于将自定义工具与大语言模型 (LLMs) 集成，从而实现增强的 AI 工作流。\n接下来展示如何创建、绑定工具、解析并执行输出，并将它们集成到 AgentExecutor 中。\n\nimport requestsfrom bs4 import BeautifulSoupfrom langchain_core.tools import tool# Define the tools@tooldef get_word_length(word: str) -&gt; int:    \"\"\"Return the length of the given text\"\"\"    return len(word)@tooldef add_function(a: float, b: float) -&gt; float:    \"\"\"Add two numbers together\"\"\"    return a + b\ttools = [get_word_length, add_function]\n\n现在，让我们使用 bind_tools 函数将定义的工具与特定的大语言模型 (LLM) 关联起来。\nfrom langchain_openai import ChatOpenAI# Create a modelllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-3B-Instruct',\ttemperature=0.2,)# Tool bindingllm_with_tools = llm.bind_tools(tools)\n\n这是绑定函数的说明\nimport jsonprint(json.dumps(llm_with_tools.kwargs, indent=2))\n\n\nllm_with_tools.invoke(    \"What is the length of the given text 'LangChain OpenTutorial'?\")\n\n结果存储在 tool_calls 中。让我们打印 tool_calls。\n[注意]\n\nname 表示工具的名称。\nargs 包含传递给工具的参数。\n\nfrom pprint import pprint# Execution resultret = llm_with_tools.invoke(    \"What is the length of the given text 'LangChain OpenTutorial'?\")pprint(ret.__dict__)print(20*'-')pprint(ret.tool_calls)\n\ntool 输出解析 JsonOutputToolsParser接下来，我们将把 llm_with_tools 与 JsonOutputToolsParser 连接起来，以解析 tool_calls 并查看结果。\n\ntype 表示工具的类型。\nargs 包含传递给工具的参数。\n\nfrom langchain_core.output_parsers.openai_tools import JsonOutputToolsParser# Tool Binding + Tool Parserchain = llm_with_tools | JsonOutputToolsParser(tools=tools)# Execution Resulttool_call_results = chain.invoke(    \"What is the length of the given text 'LangChain OpenTutorial'?\")print(tool_call_results)\n\nexecute_tool_calls 函数识别合适的工具，传递相应的 args，然后执行该工具。\ndef execute_tool_calls(tool_call_results):    \"\"\"    Function to execute the tool call results.    :param tool_call_results: List of the tool call results    :param tools: List of available tools    \"\"\"    # Iterate over the list of the tool call results    for tool_call_result in tool_call_results:        # Tool name (function name)        tool_name = tool_call_result[\"type\"]        # Tool arguments        tool_args = tool_call_result[\"args\"]        # Find the tool that matches the name and execute it        # Use the next() function to find the first matching tool        matching_tool = next((tool for tool in tools if tool.name == tool_name), None)        if matching_tool:            # Execute the tool            result = matching_tool.invoke(tool_args)            print(                f\"[Executed Tool] {tool_name} [Args] {tool_args}\\n[Execution Result] {result}\"            )        else:            print(f\"Warning: Unable to find the tool corresponding to {tool_name}.\")# Execute the tool callsexecute_tool_calls(tool_call_results)\n\n将工具与解析器绑定以执行这次，我们将工具绑定、解析结果和执行工具调用的整个过程合并为一个步骤。\n\nllm_with_tools：绑定工具的LLM模型。\nJsonOutputToolsParser：处理工具调用结果的解析器。\nexecute_tool_calls：执行工具调用结果的函数。\n\n[流程摘要]\n\n将工具绑定到模型。\n解析工具调用的结果。\n执行工具调用的结果。\n\nfrom langchain_core.output_parsers.openai_tools import JsonOutputToolsParser# bind_tools + Parser + Executionchain = llm_with_tools | JsonOutputToolsParser(tools=tools) | execute_tool_calls\n\n\nchain.invoke(\"What is the length of the given text 'LangChain OpenTutorial'?\")\n\n\n# Execution Result 2chain.invoke(\"114.5 + 121.2\")# Double checkprint(114.5 + 121.2)\n\n将工具与Agent和AgentExecutor绑定bind_tools 提供可以由模型使用的工具（schemas）。\nAgentExecutor 创建一个执行循环，用于执行诸如调用LLM、路由到合适的工具、执行工具以及重新调用模型等任务。\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai import ChatOpenAI# Create an Agent promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are very powerful assistant, but don't know current events\",        ),        (\"user\", \"{input}\"),        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),    ])# Create a modelllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-3B-Instruct',\ttemperature=0.2,)\n\n\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent# Use the tools defined previouslytools = [get_word_length, add_function]# Create an Agentagent = create_tool_calling_agent(llm, tools, prompt)# Create an AgentExecutoragent_executor = AgentExecutor(    agent=agent,    tools=tools,    verbose=True,    handle_parsing_errors=True,)\n\n\n# Execute the Agentresult = agent_executor.invoke(    {\"input\": \"What is the length of the given text 'LangChain OpenTutorial'?\"})# Execution Resultprint(result[\"output\"])\n\n\n# Execute the Agentresult = agent_executor.invoke({\"input\": \"Calculate the result of 114.5 + 121.2\"})# Execution Resultprint(result[\"output\"])\n\n小模型的性能还是会存在解析和 reasoning 方面的错误\nTool Calling Agent在 LangChain 中，工具调用（tool calling）允许模型检测何时调用一个或多个工具，以及需要将什么输入传递给这些工具。\n在进行 API 调用时，你可以定义工具，并智能地引导模型生成结构化对象，例如 JSON，这些对象包含调用这些工具所需的参数。\n工具 API 的目标是提供比标准的文本生成或聊天 API 更加可靠的有效和有用的工具调用生成。\n你可以创建代理（agents），这些代理会迭代地调用工具并接收结果，直到通过整合这些结构化输出解决查询，并将多个工具绑定到一个工具调用的聊天模型，让模型选择调用哪些工具。\n这代表了一个更加通用的版本，它是为 OpenAI 的特定工具调用风格设计的 OpenAI 工具代理的扩展。\n这个代理使用 LangChain 的 ToolCall 接口，支持比 OpenAI 更广泛的提供者实现，包括 Anthropic、Google Gemini 和 Mistral 等。\n\n创建工具LangChain 允许你定义自定义工具，供你的代理与之交互。你可以创建用于搜索新闻或执行 Python 代码的工具。\n@tool 装饰器用于创建工具：\n\nTavilySearchResults 是一个用于搜索新闻的工具。\nPythonREPL 是一个用于执行 Python 代码的工具。\n\nfrom langchain.tools import toolfrom typing import List, Dict, Annotatedfrom langchain_community.tools import TavilySearchResultsfrom langchain_experimental.utilities import PythonREPL# Creating tool for searching news@tooldef search_news(query: str) -&gt; List[Dict[str, str]]:    \"\"\"Search news by input keyword using Tavily Search API\"\"\"    news_tool = TavilySearchResults(        max_results=3,        include_answer=True,        include_raw_content=True,        include_images=True,        # search_depth=\"advanced\",        # include_domains = [],        # exclude_domains = []    )    return news_tool.invoke(query, k=3)# Creating tool for executing python code@tooldef python_repl_tool(    code: Annotated[str, \"The python code to execute to generate your chart.\"],):    \"\"\"Use this tool to execute Python code. If you want to see the output of a value,    you should print it using print(...). This output is visible to the user.\"\"\"    result = \"\"    try:        result = PythonREPL().run(code)    except BaseException as e:        print(f\"Failed to execute. Error: {repr(e)}\")    finally:        return resultprint(f\"Tool name: {search_news.name}\")print(f\"Tool description: {search_news.description}\")print(f\"Tool args: {search_news.args}\")print('-'*20)print(f\"Tool name: {python_repl_tool.name}\")print(f\"Tool description: {python_repl_tool.description}\")print(f\"Tool args: {python_repl_tool.args}\")\n\nTool name: search_news\nTool description: Search news by input keyword using Tavily Search API\nTool args: {'query': {'title': 'Query', 'type': 'string'}}\n--------------------\nTool name: python_repl_tool\nTool description: Use this tool to execute Python code. If you want to see the output of a value,\n    you should print it using print(...). This output is visible to the user.\nTool args: {'code': {'description': 'The python code to execute to generate your chart.', 'title': 'Code', 'type': 'string'}}\n\nsearch_news('2024')\n\n\n\n\n[{'url': 'https://en.wikipedia.org/wiki/2024_in_the_United_States',\n  'content': \"In the Senate, at least six seats, those of Senators Tom Carper from Delaware, Mike Braun from Indiana, Ben Cardin from Maryland, Debbie Stabenow from Michigan, Mitt Romney from Utah, and Joe Manchin from West Virginia, will be open contests; the seat of the late Dianne Feinstein is also expected to be an open contest with Feinstein's immediate replacement, Laphonza Butler, expected to serve on an interim basis.[1][2][3]\\nConcerning state governments, 11 states and two territories will hold gubernatorial elections, and most states and territories will hold elections for their legislatures. Contents\\n2024 in the United States\\nThe following is a list of predicted and scheduled events of the year 2024 in the United States, that have not yet occurred.\\n With former president Donald Trump's declaration to run for the office again, the election may possibly be a rematch of the 2020 election, although the June 2023 indictment of Donald Trump may have a significant impact on Trump's presidential campaign. In the federal government, the offices of the president, vice president, all 435 seats of the House of Representatives, and roughly one third of the Senate. ←\\n→\\nElections[edit]\\nThe US general elections will be held on November 5 of this year.\"},\n {'url': 'https://abcnews.go.com/Entertainment/abc-news-year-2024-back-years-major-news/story?id=116448091',\n  'content': 'ABC News\\' \\'The Year: 2024\\' looks back at this year\\'s major news and entertainment events - ABC News ABC News ABC News\\' \\'The Year: 2024\\' looks back at this year\\'s major news and entertainment events As the world gears up for 2025, it leaves behind a year of war, political shifts, pop culture moments, sporting triumphs, lost stars and more. ABC News was there to chronicle every moment and will look back at this year\\'s defining events in a two-hour special, \"The Year: 2024,\" which airs Thursday, Dec. 26 at 9 p.m. ET, and streams afterwards on Hulu. The special also explores how the love lives of some of our favorite stars evolved this year. ABC News Live'},\n {'url': 'https://en.wikipedia.org/wiki/2024',\n  'content': 'May 8 – In North Macedonian elections, the right-wing party VMRO-DPMNE wins in a landslide in the parliamentary elections, while its presidential candidate Gordana Siljanovska-Davkova is elected as the first female president of the country in the second round of the presidential election.[88][89] July 13 – While campaigning for the 2024 United States presidential election, former President Donald Trump is shot in the right ear in an assassination attempt at a rally he held near Butler, Pennsylvania.[139] July 28 – 2024 Venezuelan presidential election: Incumbent President Nicolás Maduro declares victory against opposition candidate Edmundo González Urrutia amid alleged irregularities, causing numerous South American states to refuse to acknowledge the results or suspend diplomatic relations with the Maduro government and sparking nationwide protests.[151]'}]\n\n# Creating toolstools = [search_news, python_repl_tool]\n\n构建代理提示\nchat_history：此变量存储对话历史记录，如果你的代理支持多轮对话，则使用此变量。（否则，可以省略此项。）\nagent_scratchpad：此变量作为临时存储，用于存放中间变量。\ninput：此变量代表用户的输入。\n\nfrom langchain_core.prompts import ChatPromptTemplate# Creating prompt# Prompt is a text that describes the task the model should perform. (input the name and role of the tool)prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. \"            \"Make sure to use the `search_news` tool for searching keyword related news.\",        ),        (\"placeholder\", \"{chat_history}\"),        (\"human\", \"{input}\"),        (\"placeholder\", \"{agent_scratchpad}\"),    ])\n\n创建代理使用 create_tool_calling_agent 函数定义一个代理。\nfrom langchain_openai import ChatOpenAIfrom langchain.agents import create_tool_calling_agent# Creating LLMllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-3B-Instruct',\ttemperature=0.2,)# Creating Agentagent = create_tool_calling_agent(llm, tools, prompt)\n\nAgentExecutorAgentExecutor 是一个用于管理使用工具的代理的类。\n关键属性\n\nagent：负责创建计划并在执行循环的每个步骤中确定行动的底层代理。\ntools：包含代理被授权使用的所有有效工具的列表。\nreturn_intermediate_steps：布尔标志，决定是否返回代理在执行过程中所采取的中间步骤以及最终输出。\nmax_iterations：代理在执行循环终止之前可以采取的最大步骤数。\nmax_execution_time：执行循环允许运行的最长时间。\nearly_stopping_method：定义当代理未返回 AgentFinish 时如何处理的方式。（”force” 或 “generate”）\n\"force\"：返回一个字符串，表示执行循环由于达到时间或迭代限制而被停止。\n\"generate\"：调用代理的 LLM 链一次，根据之前的步骤生成最终答案。\n\n\nhandle_parsing_errors：指定如何处理解析错误。（可以设置为 True、False，或提供自定义错误处理函数。）\ntrim_intermediate_steps：修剪中间步骤的方法。（可以设置为 -1 以保留所有步骤，或提供自定义修剪函数。）\n\n关键方法\n\ninvoke：执行代理。\nstream：流式传输达到最终输出所需的步骤。\n\n关键特性\n\n工具验证：确保工具与代理兼容。\n执行控制：设置最大迭代次数和执行时间限制来管理代理行为。\n错误处理：提供多种处理输出解析错误的选项。\n中间步骤管理：允许修剪中间步骤或返回调试选项。\n异步支持：支持异步执行和结果的流式传输。\n\n优化建议\n\n设置适当的 max_iterations 和 max_execution_time 值来管理执行时间。\n使用 trim_intermediate_steps 来优化内存使用。\n对于复杂任务，使用 stream 方法来逐步监控结果。\n\nfrom langchain.agents import AgentExecutor# Create AgentExecutoragent_executor = AgentExecutor(    agent=agent,    tools=tools,    verbose=True,    max_iterations=10,    max_execution_time=10,    handle_parsing_errors=True,)# Run AgentExecutorresult = agent_executor.invoke({\"input\": \"Search news about AI Agent in 2025.\"})print(\"Agent execution result:\")print(result[\"output\"])\n\n    ​    \u001b[1m&gt; Entering new AgentExecutor chain...\u001b[0m​    \u001b[32;1m\u001b[1;3m​    Invoking: `search_news` with `{'query': 'AI Agent 2025'}`​    ​    \u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://hai.stanford.edu/news/predictions-ai-2025-collaborative-agents-ai-skepticism-and-new-risks', 'content': 'According to leading experts from Stanford Institute for Human-Centered AI, one major trend is the rise of collaborative AI systems where multiple specialized agents work together, with humans providing high-level guidance. I expect to see more focus on multimodal AI models in education, including in processing speech and images. AI Agents Work Together In 2025, we will see a significant shift from relying on individual AI models to using systems where multiple AI agents of diverse expertise work together. As an example, we recently introduced the\\xa0Virtual Lab, where a professor AI agent leads a team of AI scientist agents (e.g., AI chemist, AI biologist) to tackle challenging, open-ended research, with a human researcher providing high-level feedback. We will experience an emerging paradigm of research around how humans work together with AI agents.'}, {'url': 'https://www.forbes.com/sites/lutzfinger/2025/01/05/ai-agents-in-2025-what-enterprise-leaders-need-to-know/', 'content': 'AI Agents In 2025: What Enterprise Leaders Need To Know AI Agents In 2025: What Enterprise Leaders Need To Know AI Agents for the Enterprise will be the focus of 2025 To see what AI agents can do in 2025, let’s consider a simple example: an email-answering tool. Let’s improve our tool by building AI agents within a workflow. The Workflow of AI Agents: More Than Generative AI AI models can be connected or \"chained\" to build workflows where the output of one model becomes the input for the next. AI Agent Workflows: Input - Orchestration - Control - Actions - Synthesizing 2025 - AI Agents for the Enterprise Follow me here on Forbes or on LinkedIn for more of my 2025 AI predictions.'}, {'url': 'https://www.godofprompt.ai/blog/ai-agents-you-cant-miss', 'content': 'Explore 10+ AI agents that are reshaping industries in 2025. From ChatGPT to DeepSeek-R1, discover how AI is becoming more intelligent, efficient, and essential for businesses and individuals alike.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some relevant news articles about AI Agents in 2025:​    ​    1. [According to leading experts from Stanford Institute for Human-Centered AI, one major trend is the rise of collaborative AI systems where multiple specialized agents work together, with humans providing high-level guidance.](https://hai.stanford.edu/news/predictions-ai-2025-collaborative-agents-ai-skepticism-and-new-risks)​        2. [AI Agents In 2025: What Enterprise Leaders Need To Know](https://www.forbes.com/sites/lutzfinger/2025/01/05/ai-agents-in-2025-what-enterprise-leaders-need-to-know/) - This article discusses AI Agents for the Enterprise, focusing on AI models being connected or \"chained\" to build workflows where the output of one model becomes the input for the next. It also mentions AI Agent Workflows: Input - Orchestration - Control - Actions - Synthesizing.        3. [Explore 10+ AI agents that are reshaping industries in 2025. From ChatGPT to DeepSeek-R1, discover how AI is becoming more intelligent, efficient, and essential for businesses and individuals alike.](https://www.godofprompt.ai/blog/ai-agents-you-cant-miss)        These articles provide insights into the future of AI Agents, including their collaborative nature, their potential impact on enterprises, and the development of AI Agent Workflows.\u001b[0m        \u001b[1m&gt; Finished chain.\u001b[0m    Agent execution result:    Here are some relevant news articles about AI Agents in 2025:        1. [According to leading experts from Stanford Institute for Human-Centered AI, one major trend is the rise of collaborative AI systems where multiple specialized agents work together, with humans providing high-level guidance.](https://hai.stanford.edu/news/predictions-ai-2025-collaborative-agents-ai-skepticism-and-new-risks)        2. [AI Agents In 2025: What Enterprise Leaders Need To Know](https://www.forbes.com/sites/lutzfinger/2025/01/05/ai-agents-in-2025-what-enterprise-leaders-need-to-know/) - This article discusses AI Agents for the Enterprise, focusing on AI models being connected or \"chained\" to build workflows where the output of one model becomes the input for the next. It also mentions AI Agent Workflows: Input - Orchestration - Control - Actions - Synthesizing.        3. [Explore 10+ AI agents that are reshaping industries in 2025. From ChatGPT to DeepSeek-R1, discover how AI is becoming more intelligent, efficient, and essential for businesses and individuals alike.](https://www.godofprompt.ai/blog/ai-agents-you-cant-miss)        These articles provide insights into the future of AI Agents, including their collaborative nature, their potential impact on enterprises, and the development of AI Agent Workflows.\n\n使用 Stream 输出检查逐步结果我们将使用 AgentExecutor 的 stream() 方法来流式传输代理的中间步骤。\nstream() 的输出在 (Action, Observation) 对之间交替，最终如果目标达成，将以代理的答案结束。\n流程如下所示：\n\nAction 输出\nObservation 输出\nAction 输出\nObservation 输出\n\n…（继续直到目标达成）…\n然后，代理将在目标达成后得出最终答案。\n以下表格总结了你将在输出中遇到的内容：\n\n\n\n输出\n描述\n\n\n\nAction\nactions：表示 AgentAction 或其子类。messages：与动作调用对应的聊天消息。\n\n\nObservation\nsteps：记录代理的工作，包括当前的动作和其观察结果。messages：包含函数调用结果（即观察结果）的聊天消息。\n\n\nFinal Answer\noutput：表示 AgentFinish 信号。messages：包含最终输出的聊天消息。\n\n\nfrom langchain.agents import AgentExecutor# Create AgentExecutoragent_executor = AgentExecutor(    agent=agent,    tools=tools,    verbose=False,    handle_parsing_errors=True,)# Run in streaming moderesult = agent_executor.stream({\"input\": \"Search news about AI Agent in 2025.\"})for step in result:    # Print intermediate steps    print(step)    print(\"===\" * 20)\n\n{'actions': [ToolAgentAction(tool='search_news', tool_input={'query': 'AI Agent 2025'}, log=\"\\nInvoking: `search_news` with `{'query': 'AI Agent 2025'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'function': {'arguments': '{\"query\": \"AI Agent 2025\"}', 'name': 'search_news'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'Qwen2.5-3B-Instruct'}, id='run-a877dfea-5a20-4970-96da-0f3483298f7e', tool_calls=[{'name': 'search_news', 'args': {'query': 'AI Agent 2025'}, 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'search_news', 'args': '{\"query\": \"AI Agent 2025\"}', 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='chatcmpl-tool-cf8525019f5847519566061e0e6647c6')], 'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'function': {'arguments': '{\"query\": \"AI Agent 2025\"}', 'name': 'search_news'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'Qwen2.5-3B-Instruct'}, id='run-a877dfea-5a20-4970-96da-0f3483298f7e', tool_calls=[{'name': 'search_news', 'args': {'query': 'AI Agent 2025'}, 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'search_news', 'args': '{\"query\": \"AI Agent 2025\"}', 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'index': 0, 'type': 'tool_call_chunk'}])]}============================================================{'steps': [AgentStep(action=ToolAgentAction(tool='search_news', tool_input={'query': 'AI Agent 2025'}, log=\"\\nInvoking: `search_news` with `{'query': 'AI Agent 2025'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'function': {'arguments': '{\"query\": \"AI Agent 2025\"}', 'name': 'search_news'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'Qwen2.5-3B-Instruct'}, id='run-a877dfea-5a20-4970-96da-0f3483298f7e', tool_calls=[{'name': 'search_news', 'args': {'query': 'AI Agent 2025'}, 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'search_news', 'args': '{\"query\": \"AI Agent 2025\"}', 'id': 'chatcmpl-tool-cf8525019f5847519566061e0e6647c6', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='chatcmpl-tool-cf8525019f5847519566061e0e6647c6'), observation=[{'url': 'https://hai.stanford.edu/news/predictions-ai-2025-collaborative-agents-ai-skepticism-and-new-risks', 'content': 'According to leading experts from Stanford Institute for Human-Centered AI, one major trend is the rise of collaborative AI systems where multiple specialized agents work together, with humans providing high-level guidance. I expect to see more focus on multimodal AI models in education, including in processing speech and images. AI Agents Work Together In 2025, we will see a significant shift from relying on individual AI models to using systems where multiple AI agents of diverse expertise work together. As an example, we recently introduced the\\xa0Virtual Lab, where a professor AI agent leads a team of AI scientist agents (e.g., AI chemist, AI biologist) to tackle challenging, open-ended research, with a human researcher providing high-level feedback. We will experience an emerging paradigm of research around how humans work together with AI agents.'}, {'url': 'https://www.forbes.com/sites/lutzfinger/2025/01/05/ai-agents-in-2025-what-enterprise-leaders-need-to-know/', 'content': 'AI Agents In 2025: What Enterprise Leaders Need To Know AI Agents In 2025: What Enterprise Leaders Need To Know AI Agents for the Enterprise will be the focus of 2025 To see what AI agents can do in 2025, let’s consider a simple example: an email-answering tool. Let’s improve our tool by building AI agents within a workflow. The Workflow of AI Agents: More Than Generative AI AI models can be connected or \"chained\" to build workflows where the output of one model becomes the input for the next. AI Agent Workflows: Input - Orchestration - Control - Actions - Synthesizing 2025 - AI Agents for the Enterprise Follow me here on Forbes or on LinkedIn for more of my 2025 AI predictions.'}, {'url': 'https://www.godofprompt.ai/blog/ai-agents-you-cant-miss', 'content': 'Explore 10+ AI agents that are reshaping industries in 2025. From ChatGPT to DeepSeek-R1, discover how AI is becoming more intelligent, efficient, and essential for businesses and individuals alike.'}])], 'messages': [FunctionMessage(content='[{\"url\": \"https://hai.stanford.edu/news/predictions-ai-2025-collaborative-agents-ai-skepticism-and-new-risks\", \"content\": \"According to leading experts from Stanford Institute for Human-Centered AI, one major trend is the rise of collaborative AI systems where multiple specialized agents work together, with humans providing high-level guidance. I expect to see more focus on multimodal AI models in education, including in processing speech and images. AI Agents Work Together In 2025, we will see a significant shift from relying on individual AI models to using systems where multiple AI agents of diverse expertise work together. As an example, we recently introduced the\\xa0Virtual Lab, where a professor AI agent leads a team of AI scientist agents (e.g., AI chemist, AI biologist) to tackle challenging, open-ended research, with a human researcher providing high-level feedback. We will experience an emerging paradigm of research around how humans work together with AI agents.\"}, {\"url\": \"https://www.forbes.com/sites/lutzfinger/2025/01/05/ai-agents-in-2025-what-enterprise-leaders-need-to-know/\", \"content\": \"AI Agents In 2025: What Enterprise Leaders Need To Know AI Agents In 2025: What Enterprise Leaders Need To Know AI Agents for the Enterprise will be the focus of 2025 To see what AI agents can do in 2025, let’s consider a simple example: an email-answering tool. Let’s improve our tool by building AI agents within a workflow. The Workflow of AI Agents: More Than Generative AI AI models can be connected or \\\\\"chained\\\\\" to build workflows where the output of one model becomes the input for the next. AI Agent Workflows: Input - Orchestration - Control - Actions - Synthesizing 2025 - AI Agents for the Enterprise Follow me here on Forbes or on LinkedIn for more of my 2025 AI predictions.\"}, {\"url\": \"https://www.godofprompt.ai/blog/ai-agents-you-cant-miss\", \"content\": \"Explore 10+ AI agents that are reshaping industries in 2025. From ChatGPT to DeepSeek-R1, discover how AI is becoming more intelligent, efficient, and essential for businesses and individuals alike.\"}]', additional_kwargs={}, response_metadata={}, name='search_news')]}============================================================{'output': 'Here are some relevant news articles about AI Agents in 2025:\\n\\n1. [According to leading experts from Stanford Institute for Human-Centered AI, one major trend is the rise of collaborative AI systems where multiple specialized agents work together, with humans providing high-level guidance.](https://hai.stanford.edu/news/predictions-ai-2025-collaborative-agents-ai-skepticism-and-new-risks)\\n\\n2. [AI Agents In 2025: What Enterprise Leaders Need To Know](https://www.forbes.com/sites/lutzfinger/2025/01/05/ai-agents-in-2025-what-enterprise-leaders-need-to-know/) - This article discusses AI Agents for the Enterprise, focusing on how AI models can be connected or \"chained\" to build workflows where the output of one model becomes the input for the next.\\n\\n3. [Explore 10+ AI agents that are reshaping industries in 2025. From ChatGPT to DeepSeek-R1, discover how AI is becoming more intelligent, efficient, and essential for businesses and individuals alike.](https://www.godofprompt.ai/blog/ai-agents-you-cant-miss)\\n\\nThese articles provide insights into the expected trends and developments in AI Agents for both research and enterprise applications in the year 2025.', 'messages': [AIMessage(content='Here are some relevant news articles about AI Agents in 2025:\\n\\n1. [According to leading experts from Stanford Institute for Human-Centered AI, one major trend is the rise of collaborative AI systems where multiple specialized agents work together, with humans providing high-level guidance.](https://hai.stanford.edu/news/predictions-ai-2025-collaborative-agents-ai-skepticism-and-new-risks)\\n\\n2. [AI Agents In 2025: What Enterprise Leaders Need To Know](https://www.forbes.com/sites/lutzfinger/2025/01/05/ai-agents-in-2025-what-enterprise-leaders-need-to-know/) - This article discusses AI Agents for the Enterprise, focusing on how AI models can be connected or \"chained\" to build workflows where the output of one model becomes the input for the next.\\n\\n3. [Explore 10+ AI agents that are reshaping industries in 2025. From ChatGPT to DeepSeek-R1, discover how AI is becoming more intelligent, efficient, and essential for businesses and individuals alike.](https://www.godofprompt.ai/blog/ai-agents-you-cant-miss)\\n\\nThese articles provide insights into the expected trends and developments in AI Agents for both research and enterprise applications in the year 2025.', additional_kwargs={}, response_metadata={})]}============================================================\n\n使用用户定义的函数自定义中间步骤输出你可以定义以下 3 个函数来自定义中间步骤的输出：\n\ntool_callback：此函数处理工具调用生成的输出。\nobservation_callback：此函数处理观察数据输出。\nresult_callback：此函数允许你处理最终答案的输出。\n\nfrom typing import Dict, Any# Create AgentStreamParser classclass AgentStreamParser:    def __init__(self):        pass    def tool_callback(self, tool: Dict[str, Any]) -&gt; None:        print(\"\\n=== Tool Called ===\")        print(f\"Tool: {tool.get('tool')}\")        print(f\"Input: {tool.get('tool_input')}\")        print(\"==================\\n\")    def observation_callback(self, step: Dict[str, Any]) -&gt; None:        print(\"\\n=== Observation ===\")        observation_data = step[\"steps\"][0].observation        print(f\"Observation: {observation_data}\")        print(\"===================\\n\")    def result_callback(self, result: str) -&gt; None:        print(\"\\n=== Final Answer ===\")        print(result)        print(\"====================\\n\")    def process_agent_steps(self, step: Dict[str, Any]) -&gt; None:        if \"actions\" in step:            for action in step[\"actions\"]:                self.tool_callback(                    {\"tool\": action.tool, \"tool_input\": action.tool_input}                )        elif \"output\" in step:            self.result_callback(step[\"output\"])        else:            self.observation_callback(step)# Create AgentStreamParser instanceagent_stream_parser = AgentStreamParser()\n\n\n# Run in streaming moderesult = agent_executor.stream({\"input\": \"Generate a array from 0 to 1 with the stride of 0.1 using numpy.\"})# result = agent_executor.stream({\"input\": \"Search news about AI Agent in 2025.\"})for step in result:    agent_stream_parser.process_agent_steps(step)\n\n\n    === Tool Called ===    Tool: numpy_array_generator    Input: {'start': 1, 'step': 0.1}    ==================​    ​    === Observation ===​    Observation: numpy_array_generator is not a valid tool, try one of [search_news, python_repl_tool].​    ===================​    ​    === Tool Called ===​    Tool: python_repl_tool​    Input: {'code': 'import numpy as np\\nnp.arange(0, 1, 0.1)'}​    ==================​    ​    === Observation ===​    Observation: ​    ===================​    ​    === Tool Called ===​    Tool: python_repl_tool​    Input: {'code': 'import numpy as np\\nnp.arange(0, 1, 0.1)'}​    ==================​    ​    === Observation ===​    Observation: ​    ===================​    ​    === Tool Called ===​    Tool: python_repl_tool​    Input: {'code': 'import numpy as np\\nnp.arange(0, 1, 0.1)'}​    ==================​    ​    === Observation ===​    Observation: ​    ===================​    ​    === Final Answer ===​    The numpy array from 0 to 1 with a stride of 0.1 has been successfully generated. Here it is:​    ​    ```​    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])​    ```​        Is there anything else I can assist you with?    ====================\n\n\n与之前的对话历史进行代理通信为了记住过去的对话，你可以将 AgentExecutor 包装在 RunnableWithMessageHistory 中。\n有关 RunnableWithMessageHistory 的更多细节，请参阅以下链接。\n参考\n\nLangChain Python API Reference &gt; langchain: 0.3.14 &gt; core &gt; runnables &gt; langchain_core.runnables.history &gt; RunnableWithMessageHistory\n\nfrom langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistory# Create a dictionary to store session_idstore = {}# Function to get session history based on session_iddef get_session_history(session_ids):    if session_ids not in store:  # If session_id is not in store        # Create a new ChatMessageHistory object and store it in store        store[session_ids] = ChatMessageHistory()    return store[session_ids]  # Return session history for the corresponding session_id# Create an agent with chat message historyagent_with_chat_history = RunnableWithMessageHistory(    agent_executor,    # Chat session_id    get_session_history,    # The key for the question input in the prompt: \"input\"    input_messages_key=\"input\",    # The key for the message input in the prompt: \"chat_history\"    history_messages_key=\"chat_history\",)\n\n\n# Request streaming output for the queryresponse = agent_with_chat_history.stream(    {\"input\": \"Hello! My name is Teddy!\"},    # Set session_id    config={\"configurable\": {\"session_id\": \"abc123\"}},)# Check the outputfor step in response:    agent_stream_parser.process_agent_steps(step)\n\n=== Final Answer ===Hello Teddy! It's nice to meet you. How can I assist you today? Do you have any specific questions or topics you'd like to explore?====================\n\n\n\n# Request streaming output for the queryresponse = agent_with_chat_history.stream(    {\"input\": \"What is my name?\"},    # Set session_id    config={\"configurable\": {\"session_id\": \"abc123\"}},)# Check the outputfor step in response:    agent_stream_parser.process_agent_steps(step)\n\n=== Final Answer ===It seems like you've already provided your name as Teddy. If you have any other questions or need information about something else, feel free to ask!====================\n\n\n\n# Request streaming output for the queryresponse = agent_with_chat_history.stream(    {        \"input\": \"My email address is teddy@teddynote.com. The company name is TeddyNote Co., Ltd.\"    },    # Set session_id    config={\"configurable\": {\"session_id\": \"abc123\"}},)# Check the outputfor step in response:    agent_stream_parser.process_agent_steps(step)\n\n    === Tool Called ===    Tool: search_news    Input: {'query': 'TeddyNote Co., Ltd.'}    ==================​    ​    === Observation ===​    Observation: [{'url': 'https://www.youtube.com/@teddynote', 'content': '데이터 분석, 머신러닝, 딥러닝, LLM 에 대한 내용을 다룹니다. 연구보다는 개발에 관심이 많습니다 🙇\\u200d♂️🔥 \"테디노트의 RAG 비법노트\" 랭체인'}, {'url': 'https://github.com/teddynote', 'content': 'By company size. Enterprises Small and medium teams Startups Nonprofits By use case. DevSecOps DevOps CI/CD View all use cases By industry ... teddynote.github.io teddynote.github.io Public. Forked from mmistakes/minimal-mistakes. 📐 Jekyll theme for building a personal site, blog, project documentation, or portfolio.'}, {'url': 'https://github.com/teddylee777', 'content': 'Jupyter Notebook\\n1\\n4\\nConv2d and MaxPool2d Calculator for PyTorch\\nPython\\n18\\n1\\nStreamlit 튜토리얼 😁\\nJupyter Notebook\\n13\\n12\\n주가 종목 패턴 발굴기\\nJupyter Notebook\\n14\\n12\\n586\\ncontributions\\nin the last year\\nContribution activity\\nJanuary 2024\\nSeeing something unexpected? Teddy Lee\\nteddylee777\\nAchievements\\nAchievements\\nHighlights\\nBlock or report teddylee777\\nPrevent this user from interacting with your repositories and sending you notifications.\\n Jupyter Notebook\\n58\\n16\\nForked from lovedlim/tensorflow\\n텐서플로 도서 예제 파일입니다.\\n Samsung Electronics\\n테디노트 Blog\\n테디노트 YouTube\\n@teddynote\\nLinkedIn\\n💻 (This repository is intented for helping whom are interested in machine learning study)\\nJupyter Notebook\\n2.3k\\n789\\n머신러닝/딥러닝(PyTorch, TensorFlow) 전용 도커입니다.'}]​    ===================​    ​    === Final Answer ===​    Here are some recent news and information related to your company, TeddyNote Co., Ltd.:​    ​    1. **TeddyNote Co., Ltd. on YouTube**: They have a YouTube channel where they discuss topics related to data analysis, machine learning, deep learning, and Large Language Models (LLM). They seem to have a focus on development rather than research. You can check out their channel [here](https://www.youtube.com/@teddynote).​        2. **TeddyNote Co., Ltd. on GitHub**:        - **Company Size**: They cater to small and medium-sized teams, startups, and nonprofits.       - **Use Cases**: They support DevSecOps and DevOps, CI/CD.       - **Public Repository**: You can view their public repository [here](https://github.com/teddynote).       - **Personal Site**: They have a personal site and blog available at [teddynote.github.io](https://teddynote.github.io/).       - **Contributions**: They have contributed to various projects, including a Jupyter Notebook for a Conv2d and MaxPool2d Calculator for PyTorch, a Streamlit tutorial, and a stock price pattern analysis tool. You can see their contributions [here](https://github.com/teddylee777).        3. **TeddyLee777 on GitHub**: This is likely a personal GitHub account associated with TeddyNote Co., Ltd. They have contributed to various projects, including a TensorFlow book example repository and a Docker image for machine learning study.        If you need more detailed information or have any specific questions about these resources, feel free to ask!    ====================\n\n\n\n# Request streaming output for the queryresponse = agent_with_chat_history.stream(    {        \"input\": \"Search the latest news and write it as the body of the email. \"        \"The recipient is `Ms. Sally` and the sender is my personal information.\"        \"Write in a polite tone, and include appropriate greetings and closings at the beginning and end of the email.\"    },    # Set session_id    config={\"configurable\": {\"session_id\": \"abc123\"}},)# Check the outputfor step in response:    agent_stream_parser.process_agent_steps(step)\n\n    === Tool Called ===    Tool: search_news    Input: {'query': 'TeddyNote Co., Ltd latest news'}    ==================​    ​    === Tool Called ===​    Tool: python_repl_tool​    Input: {'code': 'import email; from email.mime.multipart import MIMEMultipart; from email.mime.text import MIMEText; msg = MIMEMultipart(); msg[\\'From\\'] = \\'teddy@teddynote.com\\'; msg[\\'To\\'] = \\'sally@example.com\\'; msg[\\'Subject\\'] = \\'Latest News from TeddyNote Co., Ltd\\'; body = \"\"\"Here are the latest news and updates from TeddyNote Co., Ltd.:\\n\\n1. **TeddyNote Co., Ltd. on YouTube**: They have a YouTube channel where they discuss topics related to data analysis, machine learning, deep learning, and Large Language Models (LLM). They seem to have a focus on development rather than research. You can check out their channel [here](https://www.youtube.com/@teddynote).\\n\\n2. **TeddyNote Co., Ltd. on GitHub**: \\n   - **Company Size**: They cater to small and medium-sized teams, startups, and nonprofits.\\n   - **Use Cases**: They support DevSecOps and DevOps, CI/CD.\\n   - **Public Repository**: You can view their public repository [here](https://github.com/teddynote).\\n   - **Personal Site**: They have a personal site and blog available at [teddynote.github.io](https://teddynote.github.io/).\\n   - **Contributions**: They have contributed to various projects, including a Jupyter Notebook for a Conv2d and MaxPool2d Calculator for PyTorch, a Streamlit tutorial, and a stock price pattern analysis tool. You can see their contributions [here](https://github.com/teddylee777).\\n\\n3. **TeddyLee777 on GitHub**: This is likely a personal GitHub account associated with TeddyNote Co., Ltd. They have contributed to various projects, including a TensorFlow book example repository and a Docker image for machine learning study.\\n\\nIf you need more detailed information or have any specific questions about these resources, feel free to ask!\"\"\"; msg.attach(MIMEText(body, \\'plain\\')); return msg.as_string()'}​    ==================​    ​    === Tool Called ===​    Tool: send_email​    Input: {'to': 'sally@example.com', 'subject': 'Latest News from TeddyNote Co., Ltd', 'body': '...', 'sender': 'teddy@teddynote.com'}​    ==================​    ​    === Tool Called ===​    Tool: email_status​    Input: {'email_id': '...'}​    ==================​    ​    === Observation ===​    Observation: [{'url': 'https://www.threads.net/@teddynote', 'content': '60 Followers • 44 Threads • 데이터 &amp; AI. See the latest conversations with @teddynote.'}, {'url': 'https://github.com/teddylee777', 'content': 'Jupyter Notebook\\n1\\n4\\nConv2d and MaxPool2d Calculator for PyTorch\\nPython\\n18\\n1\\nStreamlit 튜토리얼 😁\\nJupyter Notebook\\n13\\n12\\n주가 종목 패턴 발굴기\\nJupyter Notebook\\n14\\n12\\n586\\ncontributions\\nin the last year\\nContribution activity\\nJanuary 2024\\nSeeing something unexpected? Teddy Lee\\nteddylee777\\nAchievements\\nAchievements\\nHighlights\\nBlock or report teddylee777\\nPrevent this user from interacting with your repositories and sending you notifications.\\n Jupyter Notebook\\n58\\n16\\nForked from lovedlim/tensorflow\\n텐서플로 도서 예제 파일입니다.\\n Samsung Electronics\\n테디노트 Blog\\n테디노트 YouTube\\n@teddynote\\nLinkedIn\\n💻 (This repository is intented for helping whom are interested in machine learning study)\\nJupyter Notebook\\n2.3k\\n789\\n머신러닝/딥러닝(PyTorch, TensorFlow) 전용 도커입니다.'}, {'url': 'https://langchain-opentutorial.gitbook.io/langchain-opentutorial/15-agent/03-agent', 'content': 'Best regards, Teddy teddy@teddynote.com TeddyNote Co., Ltd. --- Feel free to modify any part of the email as you see fit! &gt;&gt;&gt;&gt;&gt;'}]​    ===================​    ​    === Observation ===​    Observation: SyntaxError(\"'return' outside function\", ('&lt;string&gt;', 14, 152, None, 14, 174))​    ===================​    ​    === Observation ===​    Observation: send_email is not a valid tool, try one of [search_news, python_repl_tool].​    ===================​    ​    === Observation ===​    Observation: email_status is not a valid tool, try one of [search_news, python_repl_tool].​    ===================​    ​    === Final Answer ===​    It seems there was an issue with the previous steps. Let's proceed with creating the email body using the news and information we gathered. Here is the body of the email:​    ​    ---​        Hello Ms. Sally,        I hope this email finds you well. I wanted to share some recent news and updates from TeddyNote Co., Ltd.:        1. **TeddyNote Co., Ltd. on YouTube**: They have a YouTube channel where they discuss topics related to data analysis, machine learning, deep learning, and Large Language Models (LLM). They seem to have a focus on development rather than research. You can check out their channel [here](https://www.youtube.com/@teddynote).        2. **TeddyNote Co., Ltd. on GitHub**:       - **Company Size**: They cater to small and medium-sized teams, startups, and nonprofits.       - **Use Cases**: They support DevSecOps and DevOps, CI/CD.       - **Public Repository**: You can view their public repository [here](https://github.com/teddynote).       - **Personal Site**: They have a personal site and blog available at [teddynote.github.io](https://teddynote.github.io/).       - **Contributions**: They have contributed to various projects, including a Jupyter Notebook for a Conv2d and MaxPool2d Calculator for PyTorch, a Streamlit tutorial, and a stock price pattern analysis tool. You can see their contributions [here](https://github.com/teddylee777).        3. **TeddyLee777 on GitHub**: This is likely a personal GitHub account associated with TeddyNote Co., Ltd. They have contributed to various projects, including a TensorFlow book example repository and a Docker image for machine learning study.        If you need more detailed information or have any specific questions about these resources, feel free to ask!        Best regards,    Teddy    teddy@teddynote.com    TeddyNote Co., Ltd.        ---        Please let me know if you need any further assistance or if there are any specific details you would like to include.    ====================\n\n\nAgentic RAGAgentic RAG 扩展了传统的 RAG（检索增强生成）系统，通过结合基于代理的方法，实现更复杂的信息检索和响应生成。该系统不仅仅局限于简单的文档检索和响应生成，还允许代理利用各种工具进行更智能的信息处理。这些工具包括用于访问最新信息的 Tavily Search、执行 Python 代码的能力以及自定义功能实现，所有这些都集成在 LangChain 框架中，为信息处理和生成任务提供全面的解决方案。\n本教程演示了如何构建一个文档检索系统，使用 FAISS DB 来有效地处理和搜索 PDF 文档。以软件政策研究所的 AI Brief 为示例文档，我们将探索如何将基于 Web 的文档加载器、文本拆分器、向量存储和 OpenAI 嵌入结合起来，创建一个实际的 Agentic RAG 系统。该实现展示了如何将 Retriever 工具与各种 LangChain 组件有效结合，创建一个强大的文档搜索和响应生成管道。\n创建工具from langchain_community.tools.tavily_search import TavilySearchResults# Create a search tool instance that returns up to 6 resultssearch = TavilySearchResults(k=6)\n\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_community.embeddings import DashScopeEmbeddingsfrom langchain.document_loaders import PyPDFLoaderfrom langchain.tools.retriever import create_retriever_tool# Load and process the PDFloader = PyPDFLoader(\"data/What-is-AI.pdf\")# Create text splittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)# Split the documentsplit_docs = loader.load_and_split(text_splitter)# Create vector storeembeddings = DashScopeEmbeddings(    model=\"text-embedding-v2\",)vector = FAISS.from_documents(split_docs, embeddings)# Create retrieverretriever = vector.as_retriever()# Create retriever toolretriever_tool = create_retriever_tool(    retriever,    name=\"pdf_search\",    description=\"use this tool to search information from the PDF document\",)\n\n创建代理tools = [search, retriever_tool]\n\n\nfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain.agents import create_tool_calling_agent, AgentExecutor# Initialize LLMllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-3B-Instruct',\ttemperature=0.2,)# Define prompt templateprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. \"            \"Make sure to use the `pdf_search` tool for searching information from the PDF document. \"            \"If you can't find the information from the PDF document, use the `search` tool for searching information from the web.\",        ),        (\"placeholder\", \"{chat_history}\"),        (\"human\", \"{input}\"),        (\"placeholder\", \"{agent_scratchpad}\"),    ])# Create agentagent = create_tool_calling_agent(llm, tools, prompt)# Create agent executoragent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n\n对话历史from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistory# Create a store for session historiesstore = {}def get_session_history(session_ids):    if session_ids not in store:        store[session_ids] = ChatMessageHistory()    return store[session_ids]# Create agent with chat historyagent_with_chat_history = RunnableWithMessageHistory(    agent_executor,    get_session_history,    input_messages_key=\"input\",    history_messages_key=\"chat_history\",)\n\n\ndef process_response(response):    \"\"\"    Process and display streaming response from the agent.    Args:        response: Agent's streaming response iterator    \"\"\"    for chunk in response:        if chunk.get(\"output\"):            print(chunk[\"output\"])        elif chunk.get(\"actions\"):            for action in chunk[\"actions\"]:                print(f\"\\nTool Used: {action.tool}\")                print(f\"Tool Input: {action.tool_input}\")                if action.log:                    print(f\"Tool Log: {action.log}\")\n\n\n# Example 1: Searching in PDFresponse = agent_with_chat_history.stream(    {        \"input\": \"What information can you find about Samsung's AI model in the document?\"    },    config={\"configurable\": {\"session_id\": \"tutorial_session_1\"}},)process_response(response)\n\n    Tool Used: pdf_search    Tool Input: {'query': 'Samsung AI model'}    Tool Log:     Invoking: `pdf_search` with `{'query': 'Samsung AI model'}`​    ​    ​    ​    Tool Used: search​    Tool Input: {'query': 'Samsung AI model'}​    Tool Log: ​    Invoking: `search` with `{'query': 'Samsung AI model'}`​    responded: The provided text does not contain specific information about Samsung's AI model. It seems to be a general introduction to AI, its components, and some real-world applications. To find information about Samsung's AI model, we might need to look for more detailed or specific documents or articles. Let me try searching the web for more relevant information.​    ​    ​    ​    Tool Used: tavily_search_results_json​    Tool Input: {'query': 'Samsung AI model'}​    Tool Log: ​    Invoking: `tavily_search_results_json` with `{'query': 'Samsung AI model'}`​    responded: It appears that the 'search' tool is not available. Let's try using the 'tavily_search_results_json' tool to search the web for information about Samsung's AI model.​    ​    ​    ​    The search results provide information about Samsung's AI model, specifically Samsung Gauss 2, which is described as a new GenAI model that improves Galaxy AI performance and efficiency. Here are some key points from the search results:​    ​    1. **Samsung Gauss 2**: This model supports 9 to 14 human languages and several programming languages. Samsung claims that Balanced and Supreme models match or beat other AI models on tasks.​        2. **Galaxy S25 Series**: The Galaxy S25 series features advanced, efficient AI image processing with ProScaler11, achieving a 40% improvement in display image scaling quality. It also incorporates custom technology with Samsung’s mobile Digital Natural Image engine (mDNIe) embedded within the processor using Galaxy IP to enable greater display power efficiency.        3. **Galaxy AI**: Samsung's Galaxy AI is described as a set of generative AI tools that brings features like live translation, generative photo editing, and more. The AI features are available on newer Samsung phones, but Samsung is making efforts to support these features on older models as well.        4. **Samsung Gauss 2 on Device**: Samsung Gauss 2 is an on-device AI model, which means it processes data locally on the device rather than sending it to a cloud server.        These results suggest that Samsung Gauss 2 is a significant advancement in their AI capabilities, particularly in improving Galaxy AI performance and efficiency. If you need more detailed information, you might want to look into the specific features and capabilities of Samsung Gauss 2 in more detail.\n\n\n# Example 1: Searching in PDFresponse = agent_with_chat_history.stream(    {        \"input\": \"List the devices using ai in your previous responese.\"    },    config={\"configurable\": {\"session_id\": \"tutorial_session_1\"}},)process_response(response)\n\n    Tool Used: pdf_search    Tool Input: {'query': 'devices using ai'}    Tool Log:     Invoking: `pdf_search` with `{'query': 'devices using ai'}`​    ​    ​    Based on the information provided in the document, the devices mentioned that use AI are:​    ​    1. **Smartphones**: The document mentions that AI is available on newer Samsung phones, indicating that smartphones are one of the devices using AI.​    2. **Galaxy S25 Series**: The document describes the Galaxy S25 series as featuring advanced, efficient AI image processing, which implies that this device uses AI.​    3. **Galaxy AI**: The document states that Galaxy AI is a set of generative AI tools available on newer Samsung phones, suggesting that the Galaxy S25 series and possibly other newer Samsung devices use AI.​    4. **Smart City Initiative**: The document provides an example of a government initiative using AI for real-time video analytics, advanced forensic investigation capabilities, and comprehensive operational intelligence. While it doesn't specify which devices are used, it implies that AI is used across various devices in this context.​        Therefore, the devices using AI mentioned in the document are:    - Smartphones (Samsung)    - Galaxy S25 Series    - Galaxy AI (presumably available on newer Samsung phones)\n","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"利用 json_repair 修复大模型的 json 生成错误","url":"/2025/05/04/uncategorized/json_repair/","content":"有些 LLM 在返回格式正确的 JSON 数据时会有些问题，有时会漏掉括号，有时会在数据中添加一些单词。不至于这种错误每次都要丢弃，再次生成太浪费时间了，因此能修复错误时还是要尽量修复。这就是 json_repair 的主要目的：修复 LLM 在生成 json 数据时的错误。\n\n修复 JSON 中的语法错误\n缺少引号、逗号位置错误、未转义的字符以及不完整的键值对。\n缺少引号、格式不正确的值（true、false、null）以及修复损坏的键值结构。\n\n\n修复格式错误的 JSON 数组和对象\n通过添加必要的元素（例如逗号、括号）或默认值（null、””）来修复不完整或损坏的数组/对象。\n该库可以处理包含额外非 JSON 字符（例如注释或位置不正确的字符）的 JSON，并在保持有效结构的同时进行清理。\n\n\nJSON 缺失值自动补全\n使用合理的默认值（例如空字符串或 null）自动补全 JSON 字段中的缺失值，确保有效性。\n\n\n\n安装!pip install json-repair\n\nLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, https://mirrors.aliyun.com/pypi/simple/, https://pypi.mirrors.ustc.edu.cn/simple/\nRequirement already satisfied: json-repair in /home/guest/anaconda3/envs/xprepo/lib/python3.11/site-packages (0.41.1)\n\n使用以常见的字段内部的多余””为例\nimport jsonimport json_repairfrom json_repair import repair_jsonbad_json_string = \"\"\"```json{  \"title\": \"Daily Thoughts\",  \"mood\": \"\"relaxed\"\",  \"content\": \"Today I felt really \"productive\" and relaxed.\"}```\"\"\"a_str = bad_json_stringtry:\tobj = json.loads(a_str)except json.JSONDecodeError as e:\tprint(\"Attempting to fix the JSON string...\")\t# solution 1. fix the json string\ta_str = repair_json(a_str)\tobj = json.loads(a_str)\t# solution 1. fix the json string and load as json\t# obj = json_repair.loads(a_str)\tfinally:\tprint(json.dumps(obj, indent=2))\t\n\nAttempting to fix the JSON string...\n{\n  \"title\": \"Daily Thoughts\",\n  \"mood\": \"relaxed\",\n  \"content\": \"Today I felt really \\\"productive\\\" and relaxed.\"\n}\n\n这个库虽然能修改部分错误，但是还有些错误无法解决，会有 fixed error 报错，所以还是得用两层 try 语句包裹住。\n","categories":["碎片"]},{"title":"wandb 的使用方法和示例","url":"/2025/05/04/uncategorized/wandb/","content":"Weights &amp; Biases (wandb)使用 Weights &amp; Biases (wandb) 可以非常方便地记录训练过程中的 loss、accuracy、模型权重、学习率曲线、超参配置、版本控制、模型可视化 等内容。\n下面是使用 wandb 的完整入门步骤：\n\n1. 安装 wandbpip install wandb\n\n\n2. 注册账号并登录你需要在 https://wandb.ai 注册一个账号。注意现在只有注册成个人使用才免费\n然后运行一次登录命令（只需一次）：\nwandb login\n\n它会让你粘贴一个 token（注册账号后网页上会提供），输入后即登录成功。\n示例输出如下:\n$ wandb loginwandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.wandb: Logging into wandb.ai. (Learn how to deploy a W&amp;B server locally: https://wandb.me/wandb-server)wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=modelswandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: wandb: No netrc file found, creating one.wandb: Appending key for api.wandb.ai to your netrc file: /home/guest/.netrcwandb: Currently logged in as: **YOUR_ACCOUNT** to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n\n\n3. 基本使用方式（训练脚本中添加）以下是在 PyTorch 训练中最基本的使用方式：\nimport wandb# ① 初始化项目wandb.init(project=\"my_mnist_project\", config={    \"epochs\": 5,    \"batch_size\": 64,    \"lr\": 1e-3,})# ② 使用 wandb.config 读取超参（可选）config = wandb.config# ③ 在每步/每个 epoch 记录指标for epoch in range(config.epochs):    train_loss = ...  # your training loop    acc = ...    # 这些参数可在网页上绘制曲线，x代表你记录的周期，可以是 step 或者 epoch    wandb.log({\"loss\": train_loss, \"accuracy\": acc, \"epoch\": epoch})# ④ 保存模型（可选）torch.save(model.state_dict(), \"model.pt\")wandb.save(\"model.pt\")\n\n\n4. 执行带 wandb 的代码时的输出wandb: Currently logged in **YOUR_ACCOUNT** to https://api.wandb.ai. Use `wandb login --relogin` to force reloginwandb: Tracking run with wandb version 0.19.10wandb: Run data is saved locally in **YOUR_LOCAL_SAVE_PATH**wandb: Run `wandb offline` to turn off syncing.wandb: Syncing run royal-brook-1wandb: ⭐️ View project at **YOUR_WANDB_URL**wandb: 🚀 View run at **YOUR_WANDB_URL**---THis your executed file's output. ---wandb:                                                                                wandb: wandb: Run history:wandb:      epoch ▁▃▅▆█wandb: epoch_time █▂▂▁▁wandb:         lr █▄▂▁▁wandb:   test_acc ▁▅▇██wandb:  test_loss █▄▂▁▁wandb: train_loss █▂▁▁▁wandb: wandb: Run summary:wandb:      epoch 5wandb: epoch_time 9.86671wandb:         lr 6e-05wandb:   test_acc 0.9867wandb:  test_loss 0.03622wandb: train_loss 0.05843wandb: wandb: 🚀 View run comfy-wildflower-2 at: **YOUR_WANDB_URL**wandb: ⭐️ View project at: **YOUR_WANDB_URL**wandb: Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)wandb: Find logs at: ./wandb/run-20250503_060654-lnl2z1vr/logs\n\n\nMNIST 使用示例以下是一个自定义 Trainer 训练 MNIST 分类器的代码，wandb部分都在主函数里，尽量避免放在 Trainer 内部。\nimport osimport torchimport torch.nn as nnimport torch.optim as optimimport matplotlib.pyplot as pltimport torch.nn.functional as Ffrom torch.utils.data import Dataset, DataLoaderfrom torchvision import datasets, transformsfrom time import timeimport argparsefrom tqdm import tqdmimport wandbdef compute_accuracy(preds: torch.Tensor, target: torch.Tensor, from_logits: bool = True) -&gt; float:    \"\"\"    计算准确率。        参数:    - preds: 模型输出，形状为 [batch_size, num_classes] 或 [batch_size]（如果已经是类标）    - target: 真实标签，形状为 [batch_size]    - from_logits: 若为 True，表示 preds 是 logits，需要取 argmax；若为 False，表示 preds 已是预测标签    返回:    - 准确率（float）    \"\"\"    if from_logits:        pred_labels = torch.argmax(preds, dim=1)    else:        pred_labels = preds    correct = (pred_labels == target).sum().item()    total = target.numel()    return correct / totalclass ConvNet(nn.Module):\tdef __init__(self):\t\tsuper(ConvNet, self).__init__()\t\tself.features = nn.Sequential(\t\t\tnn.Conv2d(1, 32, 3, 1),\t\t\tnn.ReLU(),\t\t\tnn.Conv2d(32, 64, 3, 1),\t\t\tnn.ReLU(),\t\t\tnn.MaxPool2d(2),\t\t\tnn.Dropout(0.25)\t\t)\t\tself.classifier = nn.Sequential(\t\t\tnn.Linear(9216, 128),\t\t\tnn.ReLU(),\t\t\tnn.Dropout(0.5),\t\t\tnn.Linear(128, 10)\t\t)\tdef forward(self, x):\t\tx = self.features(x)\t\tx = torch.flatten(x, 1)\t\tx = self.classifier(x)\t\toutput = F.log_softmax(x, dim=1)\t\treturn output\t# 自定义Datasetclass MyDataset(Dataset):\tdef __init__(self, data):\t\tself.data = data\tdef __len__(self):\t\treturn len(self.data)\tdef __getitem__(self, idx):\t\timage, label = self.data[idx]\t\treturn image, labelclass Trainer:\tdef __init__(\t\tself,\t\tmodel: torch.nn.Module,\t\toptimizer: torch.optim.Optimizer,\t\tgpu_id: int=0,\t) -&gt; None:\t\tself.gpu_id = gpu_id\t\tself.model = model.to(gpu_id)\t\tself.optimizer = optimizer\tdef save_checkpoint(self):\t\tckp = self.model.state_dict()\t\tPATH = \"checkpoint.pt\"\t\ttorch.save(ckp, PATH)\t\tprint(f\"Training checkpoint saved at {PATH}\")\tdef _train_batch(self, source, targets):\t\tself.model.train()\t\tself.optimizer.zero_grad()\t\tsource = source.to(self.gpu_id)\t\ttargets = targets.to(self.gpu_id)\t\toutput = self.model(source)\t\tloss = F.cross_entropy(output, targets)\t\tloss.backward()\t\tself.optimizer.step()\t\tbatch_loss = loss.detach().item()\t\treturn batch_loss\tdef _test_batch(self, source, targets):\t\tself.model.eval()\t\twith torch.no_grad():\t\t\tsource = source.to(self.gpu_id)\t\t\ttargets = targets.to(self.gpu_id)\t\t\toutput = self.model(source)\t\t\tloss = F.cross_entropy(output, targets)\t\t\tbatch_loss = loss.detach().item()\t\t\taccuracy = compute_accuracy(output, targets)\t\treturn batch_loss, accuracy\tdef train_epoch(self, train_dataloader):\t\t\t\ttotal_loss = 0.0\t\tnum_batches = 0\t\tfor source, targets in tqdm(train_dataloader, desc=\"Training\", mininterval=1): \t\t\t\t\t\tloss = self._train_batch(source, targets)\t\t\ttotal_loss += loss\t\t\tnum_batches += 1\t\t\t\tavg_loss = total_loss / num_batches\t\treturn avg_loss\t\t\t\tdef test_epoch(self, test_dataloader):\t\t\t\ttotal_loss = 0.0\t\tnum_batches = 0\t\ttotal_accuracy = 0.0\t\tfor source, targets in tqdm(test_dataloader, desc=\"Testing\", mininterval=1): \t\t\t\t\t\tloss, accuracy = self._test_batch(source, targets)\t\t\ttotal_loss += loss\t\t\ttotal_accuracy += accuracy\t\t\tnum_batches += 1\t\t\t\tavg_loss = total_loss / num_batches\t\tavg_accuracy = total_accuracy / num_batches\t\treturn avg_loss, avg_accuracydef prepare_dataset():\ttransform = transforms.Compose([\t\ttransforms.ToTensor(),\t\ttransforms.Normalize((0.1307,), (0.3081,))\t])\ttrain_data = datasets.MNIST(\t\troot = './mnist',\t\ttrain=True,       # 设置True为训练数据，False为测试数据\t\ttransform = transform,\t\t# download=True  # 设置True后就自动下载，下载完成后改为False即可\t)\t  \ttrain_set = MyDataset(train_data)\t\ttest_data = datasets.MNIST(\t\troot = './mnist',\t\ttrain=False,       # 设置True为训练数据，False为测试数据\t\ttransform = transform,\t)\t  \ttest_set = MyDataset(test_data)\t  \treturn train_set, test_set\t  def prepare_dataloader(dataset: Dataset, batch_size: int):\treturn DataLoader(\t\tdataset,\t\tbatch_size=batch_size,\t\tpin_memory=True,\t\tshuffle=False,\t)def arg_parser():\tparser = argparse.ArgumentParser(description=\"MNIST Training Script\")\tparser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\tparser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch size for training\")\tparser.add_argument(\"--lr\", type=float, default=0.0005, help=\"Learning rate\")\tparser.add_argument(\"--lr_decay_step_num\", type=int, default=1, help=\"Step size for learning rate decay\")\tparser.add_argument(\"--lr_decay_factor\", type=float, default=0.5, help=\"Factor by which learning rate is decayed\")\tparser.add_argument(\"--cuda_id\", type=int, default=0, help=\"CUDA device ID to use\")\tparser.add_argument('--save_every', type=int, default=1, help='How often to save a snapshot')\treturn parser.parse_args()if __name__ == \"__main__\":\targs = arg_parser()\tprint(f\"Training arguments: {args}\")\t\tEPOCHS = args.epochs\tBATCH_SIZE = args.batch_size\tLR = args.lr\tLR_DECAY_STEP_NUM = args.lr_decay_step_num\tLR_DECAY_FACTOR = args.lr_decay_factor\tCUDA_ID = args.cuda_id\tDEVICE = torch.device(f\"cuda:{CUDA_ID}\")\tSAVE_EVERY = args.save_every\t# 初始化 wandb\twandb_run = wandb.init(\t\tproject=\"mnist\",  # 可自定义项目名称\t\tconfig={\t\t\t\"epochs\": EPOCHS,\t\t\t\"batch_size\": BATCH_SIZE,\t\t\t\"lr\": LR,\t\t\t\"lr_decay_step_num\": LR_DECAY_STEP_NUM,\t\t\t\"lr_decay_factor\": LR_DECAY_FACTOR,\t\t}\t)\t\t# prepare dataloader\ttrain_set, test_set = prepare_dataset()\tprint(f\"Train dataset size: {len(train_set)}\")\tprint(f\"Test dataset size: {len(test_set)}\")\ttrain_dataloader = prepare_dataloader(dataset=train_set, batch_size=BATCH_SIZE)\ttest_dataloader = prepare_dataloader(dataset=test_set, batch_size=BATCH_SIZE)\t\t# prepare model\tmodel = ConvNet()\toptimizer = optim.Adam(model.parameters(), lr=1e-3)\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP_NUM, gamma=LR_DECAY_FACTOR)\t# init trainer\ttrainer = Trainer(model, optimizer, CUDA_ID)\tfor epoch in range(EPOCHS):\t\tprint(f\"Epoch {epoch+1}/{EPOCHS}\")\t\tprint(f'Learning Rate: {scheduler.get_last_lr()[0]}')\t\tstart_time = time()\t\ttrain_loss = trainer.train_epoch(train_dataloader)\t\ttest_loss, test_accuracy = trainer.test_epoch(test_dataloader)\t\tprint(f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\t\t\t\tepoch_time = time() - start_time\t\tcurrent_lr = scheduler.get_last_lr()[0]\t\t\t\twandb_run.log({\t\t\t\"epoch\": epoch + 1,\t\t\t\"train_loss\": train_loss,\t\t\t\"test_loss\": test_loss,\t\t\t\"test_acc\": test_accuracy,\t\t\t\"lr\": current_lr,\t\t\t\"epoch_time\": epoch_time\t\t})\t\tscheduler.step()\t\tif (epoch + 1) % SAVE_EVERY == 0:\t\t\ttrainer.save_checkpoint()\twandb_run.finish()\n\n\n\n完整的 wandb 输出如下: \nTraining arguments: Namespace(epochs=5, batch_size=512, lr=0.0005, lr_decay_step_num=1, lr_decay_factor=0.5, cuda_id=0, save_every=1)wandb: Currently logged in as: **PERSONAL_INFO** to https://api.wandb.ai. Use `wandb login --relogin` to force reloginwandb: Tracking run with wandb version 0.19.10wandb: Run data is saved locally in **PERSONAL_INFO**wandb: Run `wandb offline` to turn off syncing.wandb: Syncing run proud-oath-4wandb: ⭐️ View project at **PERSONAL_INFO**wandb: 🚀 View run at **PERSONAL_INFO**Train dataset size: 60000Test dataset size: 10000Epoch 1/5Learning Rate: 0.001Training: 100%|████████████████████████████████████████████| 118/118 [00:08&lt;00:00, 13.34it/s]Testing: 100%|███████████████████████████████████████████████| 20/20 [00:01&lt;00:00, 14.14it/s]Train Loss: 0.3594, Test Loss: 0.0741, Test Accuracy: 0.9763Training checkpoint saved at checkpoint.ptEpoch 2/5Learning Rate: 0.0005Training: 100%|████████████████████████████████████████████| 118/118 [00:08&lt;00:00, 13.74it/s]Testing: 100%|███████████████████████████████████████████████| 20/20 [00:01&lt;00:00, 14.53it/s]Train Loss: 0.1085, Test Loss: 0.0536, Test Accuracy: 0.9831Training checkpoint saved at checkpoint.ptEpoch 3/5Learning Rate: 0.00025Training: 100%|████████████████████████████████████████████| 118/118 [00:08&lt;00:00, 13.90it/s]Testing: 100%|███████████████████████████████████████████████| 20/20 [00:01&lt;00:00, 14.62it/s]Train Loss: 0.0849, Test Loss: 0.0450, Test Accuracy: 0.9848Training checkpoint saved at checkpoint.ptEpoch 4/5Learning Rate: 0.000125Training: 100%|████████████████████████████████████████████| 118/118 [00:08&lt;00:00, 13.85it/s]Testing: 100%|███████████████████████████████████████████████| 20/20 [00:01&lt;00:00, 14.46it/s]Train Loss: 0.0737, Test Loss: 0.0398, Test Accuracy: 0.9870Training checkpoint saved at checkpoint.ptEpoch 5/5Learning Rate: 6.25e-05Training: 100%|████████████████████████████████████████████| 118/118 [00:08&lt;00:00, 13.54it/s]Testing: 100%|███████████████████████████████████████████████| 20/20 [00:01&lt;00:00, 14.60it/s]Train Loss: 0.0680, Test Loss: 0.0391, Test Accuracy: 0.9870Training checkpoint saved at checkpoint.ptwandb:                                                                                wandb: wandb: Run history:wandb:      epoch ▁▃▅▆█wandb: epoch_time █▃▁▂▅wandb:         lr █▄▂▁▁wandb:   test_acc ▁▅▇██wandb:  test_loss █▄▂▁▁wandb: train_loss █▂▁▁▁wandb: wandb: Run summary:wandb:      epoch 5wandb: epoch_time 10.08319wandb:         lr 6e-05wandb:   test_acc 0.98698wandb:  test_loss 0.03908wandb: train_loss 0.068wandb: wandb: 🚀 View run proud-oath-4 at: **PERSONAL_INFO**wandb: ⭐️ View project at: **PERSONAL_INFO**wandb: Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)wandb: Find logs at: ./wandb/run-20250503_063703-b6zlso17/logs\n\n","categories":["碎片"]},{"title":"三种知识蒸馏的简单CIFAR-10示例","url":"/2025/05/06/Knowledge-Distillation-series/KD_pytorch_intro/","content":"知识蒸馏是一种技术，能够将大型、计算开销大的模型中的知识转移到较小的模型中，同时保持其有效性。这使得模型可以部署在计算能力较弱的硬件上，从而实现更快速和高效的推理。\n知识蒸馏，简而言之，是让学生模型学习到类似教师模型某个阶段(即某些层)的结果，再搭配正常的与 labels 的监督学习，就可以对齐教师模型。以下是本文涉及到的不同层面的对齐方式的 loss 计算方式。\n\n\n\nLoss 类型\n分类\n简要说明\n\n\n\nBaseline（CE）\n——\n学生用常规 CrossEntropy 监督，不涉及教师。\n\n\nKL 散度（Soft Targets）\nLogits-based KD\n最常见的 Hinton KD（softmax+KL），对比教师和学生的 soft logits。\n\n\nCosine Embedding Loss\nRepresentation-based KD\n对比教师和学生的某层输出（如 embedding）的方向一致性。\n\n\nIntermediate Regressor Loss（MSE）\nFeature-based KD / Hint-based KD\n让学生的中间层对齐教师某一层（通常是 MSE 或 L2），如可特征层或者MLP 层。\n\n\nimport torchimport torch.nn as nnimport torch.optim as optimimport torchvision.transforms as transformsimport torchvision.datasets as datasets# Check if the current `accelerator &lt;https://pytorch.org/docs/stable/torch.html#accelerators&gt;`__# is available, and if not, use the CPUdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"print(f\"Using {device} device\")\n\nUsing cuda device\n\n\nCIFAR-10 是一个包含十个类别的常用图像数据集。我们的目标是为每张输入图像预测其所属的一个类别。\n\n# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.transforms_cifar = transforms.Compose([    transforms.ToTensor(),    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])# Loading the CIFAR-10 dataset:train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n\nFiles already downloaded and verifiedFiles already downloaded and verified\n\n教师模型和学生模型的独立监督学习以下两种架构都是卷积神经网络（CNN），具有不同数量的卷积层，用作特征提取器，后接一个包含10个类别的分类器。学生模型中的卷积核和参数数量较少。\n# Deeper neural network class to be used as teacher:class DeepNN(nn.Module):    def __init__(self, num_classes=10):        super(DeepNN, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(128, 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),            nn.Conv2d(64, 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(64, 32, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),        )        self.classifier = nn.Sequential(            nn.Linear(2048, 1024),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(1024, 256),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(256, num_classes)        )    def forward(self, x):        x = self.features(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        return x# Lightweight neural network class to be used as student:class LightNN(nn.Module):    def __init__(self, num_classes=10):        super(LightNN, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 16, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),            nn.Conv2d(16, 16, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),        )        self.classifier = nn.Sequential(            nn.Linear(1024, 256),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(256, num_classes)        )    def forward(self, x):        x = self.features(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        return x\n\n以下是基础模型的训练和测试函数，loss 为 logit 与 label 之间的交叉熵，metric 也是 accuracy。\ndef train(model, train_loader, epochs, learning_rate, device):    criterion = nn.CrossEntropyLoss()    optimizer = optim.Adam(model.parameters(), lr=learning_rate)    model.train()    for epoch in range(epochs):        running_loss = 0.0        for inputs, labels in train_loader:            # inputs: A collection of batch_size images            # labels: A vector of dimensionality batch_size with integers denoting class of each image            inputs, labels = inputs.to(device), labels.to(device)            optimizer.zero_grad()            outputs = model(inputs)            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes            # labels: The actual labels of the images. Vector of dimensionality batch_size            loss = criterion(outputs, labels)            loss.backward()            optimizer.step()            running_loss += loss.item()        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")def test(model, test_loader, device):    model.to(device)    model.eval()    correct = 0    total = 0    with torch.no_grad():        for inputs, labels in test_loader:            inputs, labels = inputs.to(device), labels.to(device)            outputs = model(inputs)            _, predicted = torch.max(outputs.data, 1)            total += labels.size(0)            correct += (predicted == labels).sum().item()    accuracy = 100 * correct / total    print(f\"Test Accuracy: {accuracy:.2f}%\")    return accuracy\n\n设置 torch 随机种子，以方便结果复现\ntorch.manual_seed(42)nn_deep = DeepNN(num_classes=10).to(device)train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)test_accuracy_deep = test(nn_deep, test_loader, device)\n\nEpoch 1/10, Loss: 1.4504150144584345Epoch 2/10, Loss: 0.9319331475231044Epoch 3/10, Loss: 0.7057780996917764Epoch 4/10, Loss: 0.5456663286289596Epoch 5/10, Loss: 0.40601869922159883Epoch 6/10, Loss: 0.28496988056718237Epoch 7/10, Loss: 0.20859538298814803Epoch 8/10, Loss: 0.15263370982826213Epoch 9/10, Loss: 0.1255439391069095Epoch 10/10, Loss: 0.10921698193901869Test Accuracy: 75.68%\n\n\n# Instantiate the lightweight network:torch.manual_seed(42)nn_light = LightNN(num_classes=10).to(device)torch.manual_seed(42)new_nn_light = LightNN(num_classes=10).to(device)# Print the norm of the first layer of the initial lightweight modelprint(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())# Print the norm of the first layer of the new lightweight modelprint(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())\n\nNorm of 1st layer of nn_light: 2.327361822128296Norm of 1st layer of new_nn_light: 2.327361822128296\n\n\n这里我们输出第一次的 norm 值，只是为了确保两个同种的不同模型初始化完全一样，这样一个直接监督学习，另一个知识蒸馏。\ntotal_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))print(f\"DeepNN parameters: {total_params_deep}\")total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))print(f\"LightNN parameters: {total_params_light}\")\n\nDeepNN parameters: 2,495,914LightNN parameters: 267,738\n\n\n学生模型参数量仅为教师模型的 1/10。\ntrain(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)test_accuracy_light_ce = test(nn_light, test_loader, device)\n\nEpoch 1/10, Loss: 1.470865885315039Epoch 2/10, Loss: 1.1615625789098423Epoch 3/10, Loss: 1.0311961670970673Epoch 4/10, Loss: 0.9303452948780011Epoch 5/10, Loss: 0.8560442997671455Epoch 6/10, Loss: 0.790915090104808Epoch 7/10, Loss: 0.727434201466153Epoch 8/10, Loss: 0.672618583721273Epoch 9/10, Loss: 0.6193401737286307Epoch 10/10, Loss: 0.5703270701343751Test Accuracy: 70.50%\n\n\nprint(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")\n\nTeacher accuracy: 75.68%Student accuracy: 70.50%\n\n\nLogits-based KD这里的 soft target 指的就是教师模型的 logit，相比 hard target (label) 是没有那么绝对的，所以称为 soft target。我们计算教师模型和学生模型的 logit 的 KL 散度以对齐两者的 logit。\n\n假设：\n\n 是教师模型的 logits\n 是学生模型的 logits\n 是温度系数（Temperature）\n 是 ground-truth 标签（硬标签）\n\n\nSoft Loss（蒸馏项）：KL 散度\n\n教师的 softmax logits 用温度  平滑，学生也用相同温度。\n\n\n\n用 log_softmax + softmax 配合 KLDivLoss 实现。\n 越大，分布越平滑，能体现非主类的概率差异。\n\n\nHard Loss（监督项）：CrossEntropy\n\n这是常规的分类损失，基于 ground-truth。\n\n\n总 Loss\n\n综合上述两项：\n\n\n 是权重系数（常取 0.5 ~ 0.9）\n 是补偿因子，因为 KL 散度在梯度反传时会除以 ，所以前向传播要乘回来。\n\ndef train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):    ce_loss = nn.CrossEntropyLoss()    optimizer = optim.Adam(student.parameters(), lr=learning_rate)    teacher.eval()  # Teacher set to evaluation mode    student.train() # Student to train mode    for epoch in range(epochs):        running_loss = 0.0        for inputs, labels in train_loader:            inputs, labels = inputs.to(device), labels.to(device)            optimizer.zero_grad()            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights            with torch.no_grad():                teacher_logits = teacher(inputs)            # Forward pass with the student model            student_logits = student(inputs)            #Soften the student logits by applying softmax first and log() second            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"            # KL(P || Q) = sum P * (log P - log Q)            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)            # Calculate the true label loss            label_loss = ce_loss(student_logits, labels)            # Weighted sum of the two losses            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss            loss.backward()            optimizer.step()            running_loss += loss.item()        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)# Compare the student test accuracy with and without the teacher, after distillationprint(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n\nEpoch 1/10, Loss: 2.4518108270357333Epoch 2/10, Loss: 1.9292995130924313Epoch 3/10, Loss: 1.7038319656611098Epoch 4/10, Loss: 1.5407711703454137Epoch 5/10, Loss: 1.410411078606725Epoch 6/10, Loss: 1.2932921629732528Epoch 7/10, Loss: 1.1938754214959986Epoch 8/10, Loss: 1.1060839129225981Epoch 9/10, Loss: 1.0279690643100787Epoch 10/10, Loss: 0.9521754057815922Test Accuracy: 71.17%Teacher accuracy: 75.68%Student accuracy without teacher: 70.50%Student accuracy with CE + KD: 71.17%\n\nRepresentation-based KD\n在神经网络中，除了主要目标之外，加入额外的损失函数是一种常见且简单的做法，有助于实现更好的泛化效果。现在我们尝试为学生模型添加一个目标，但这次关注的不是输出层，而是其隐状态。我们的目标是通过引入一个简单的 loss ，将教师模型的表示信息传递给学生模型。该 loss 的最小化意味着：在传递给分类器之前，展平后的 embedding 向量随着 loss 的降低变得更加相似。\n这种做法主要是想学习到类似教师模型的 embedding，既然 embedding 也很大程度影响了 MLP，相似的 embedding 也有利于得到更好的分类结果。\n在以下模型设计中，为了计算 embedding，我们修改模型，在输出 logit 时也将 embedding 输出。这样的做法很正常，例如 transformer 通常也会把 embedding 输出，而它是为了利用已经计算好的 KV value。\nclass ModifiedDeepNNCosine(nn.Module):    def __init__(self, num_classes=10):        super(ModifiedDeepNNCosine, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(128, 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),            nn.Conv2d(64, 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(64, 32, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),        )        self.classifier = nn.Sequential(            nn.Linear(2048, 1024),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(1024, 256),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(256, num_classes)        )    def forward(self, x):        x = self.features(x)        flattened_conv_output = torch.flatten(x, 1)        x = self.classifier(flattened_conv_output)        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)        return x, flattened_conv_output_after_pooling# Create a similar student class where we return a tuple. We do not apply pooling after flattening.class ModifiedLightNNCosine(nn.Module):    def __init__(self, num_classes=10):        super(ModifiedLightNNCosine, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 16, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),            nn.Conv2d(16, 16, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),        )        self.classifier = nn.Sequential(            nn.Linear(1024, 256),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(256, num_classes)        )    def forward(self, x):        x = self.features(x)        flattened_conv_output = torch.flatten(x, 1)        x = self.classifier(flattened_conv_output)        return x, flattened_conv_output# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instancemodified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)modified_nn_deep.load_state_dict(nn_deep.state_dict())# Once again ensure the norm of the first layer is the same for both networksprint(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.torch.manual_seed(42)modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())\n\nNorm of 1st layer for deep_nn: 7.297417163848877Norm of 1st layer for modified_deep_nn: 7.297417163848877Norm of 1st layer: 2.327361822128296\n\n在这里，embedding 大小完全一样，这样两者可以直接计算 loss，不需要首先将两者大小对齐。\n# Create a sample input tensorsample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32# Pass the input through the studentlogits, hidden_representation = modified_nn_light(sample_input)# Print the shapes of the tensorsprint(\"Student logits shape:\", logits.shape) # batch_size x total_classesprint(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size# Pass the input through the teacherlogits, hidden_representation = modified_nn_deep(sample_input)# Print the shapes of the tensorsprint(\"Teacher logits shape:\", logits.shape) # batch_size x total_classesprint(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n\nStudent logits shape: torch.Size([128, 10])Student hidden representation shape: torch.Size([128, 1024])Teacher logits shape: torch.Size([128, 10])Teacher hidden representation shape: torch.Size([128, 1024])\n\n我们接下来把 KL 散度换成 cosine embedding loss\n\n给定两个 embedding,  和 $$x_2和他们是否属于同一类的y，如果y=1$，两者 embedding 应该尽量相似，两者的 cosine 值应该尽量高，夹角最后为 0 。否则，cosine 应该尽量小，我们并不硬性要求，只要小于 Margin 值就可。\ndef train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):    ce_loss = nn.CrossEntropyLoss()    cosine_loss = nn.CosineEmbeddingLoss()    optimizer = optim.Adam(student.parameters(), lr=learning_rate)    teacher.to(device)    student.to(device)    teacher.eval()  # Teacher set to evaluation mode    student.train() # Student to train mode    for epoch in range(epochs):        running_loss = 0.0        for inputs, labels in train_loader:            inputs, labels = inputs.to(device), labels.to(device)            optimizer.zero_grad()            # Forward pass with the teacher model and keep only the hidden representation            with torch.no_grad():                _, teacher_hidden_representation = teacher(inputs)            # Forward pass with the student model            student_logits, student_hidden_representation = student(inputs)            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))            # Calculate the true label loss            label_loss = ce_loss(student_logits, labels)            # Weighted sum of the two losses            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss            loss.backward()            optimizer.step()            running_loss += loss.item()        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n\n\ndef test_multiple_outputs(model, test_loader, device):    model.to(device)    model.eval()    correct = 0    total = 0    with torch.no_grad():        for inputs, labels in test_loader:            inputs, labels = inputs.to(device), labels.to(device)            outputs, _ = model(inputs) # Disregard the second tensor of the tuple            _, predicted = torch.max(outputs.data, 1)            total += labels.size(0)            correct += (predicted == labels).sum().item()    accuracy = 100 * correct / total    print(f\"Test Accuracy: {accuracy:.2f}%\")    return accuracy\n\n\n# Train and test the lightweight network with cross entropy losstrain_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)\n\nEpoch 1/10, Loss: 1.2930794695149297Epoch 2/10, Loss: 1.0589119400209783Epoch 3/10, Loss: 0.9568290230258346Epoch 4/10, Loss: 0.8834206243915022Epoch 5/10, Loss: 0.827963415466611Epoch 6/10, Loss: 0.7844577648145769Epoch 7/10, Loss: 0.7424624330552337Epoch 8/10, Loss: 0.7057774653825004Epoch 9/10, Loss: 0.6661281107026903Epoch 10/10, Loss: 0.6384436357814027Test Accuracy: 70.53%\n\nFeature-based KD\n教师模型使用32个卷积核，学生模型使用16个卷积核，两者输出大小不一样，因此不可以直接对齐两者。因此我们将加入一个可训练的层，用于将学生模型的特征图转换为与教师模型特征图相同的形状。在实际操作中，我们会修改轻量级模型类，使其在一个中间 regressor 之后返回隐状态，该回归器用于匹配卷积特征图的尺寸；同时修改教师模型类，使其返回最后一个卷积层的输出。\n# Pass the sample input only from the convolutional feature extractorconvolutional_fe_output_student = nn_light.features(sample_input)convolutional_fe_output_teacher = nn_deep.features(sample_input)# Print their shapesprint(\"Student's feature extractor output shape: \", convolutional_fe_output_student.shape)print(\"Teacher's feature extractor output shape: \", convolutional_fe_output_teacher.shape)\n\nStudent's feature extractor output shape:  torch.Size([128, 16, 8, 8])Teacher's feature extractor output shape:  torch.Size([128, 32, 8, 8])\n\n\nclass ModifiedDeepNNRegressor(nn.Module):    def __init__(self, num_classes=10):        super(ModifiedDeepNNRegressor, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(128, 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),            nn.Conv2d(64, 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(64, 32, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),        )        self.classifier = nn.Sequential(            nn.Linear(2048, 1024),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(1024, 256),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(256, num_classes)        )    def forward(self, x):        x = self.features(x)        conv_feature_map = x        x = torch.flatten(x, 1)        x = self.classifier(x)        return x, conv_feature_mapclass ModifiedLightNNRegressor(nn.Module):    def __init__(self, num_classes=10):        super(ModifiedLightNNRegressor, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 16, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),            nn.Conv2d(16, 16, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2),        )        # Include an extra regressor (in our case linear)        self.regressor = nn.Sequential(            nn.Conv2d(16, 32, kernel_size=3, padding=1)        )        self.classifier = nn.Sequential(            nn.Linear(1024, 256),            nn.ReLU(),            nn.Dropout(0.1),            nn.Linear(256, num_classes)        )    def forward(self, x):        x = self.features(x)        regressor_output = self.regressor(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        return x, regressor_output\n\n接下来，我们也不用 cosine embedding loss，改为直接让两者的 MSE loss，使得两者尽量相同，而之前的 cosine embedding loss 会尽量让 embedding 的角度尽量相同。\ndef train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):    ce_loss = nn.CrossEntropyLoss()    mse_loss = nn.MSELoss()    optimizer = optim.Adam(student.parameters(), lr=learning_rate)    teacher.to(device)    student.to(device)    teacher.eval()  # Teacher set to evaluation mode    student.train() # Student to train mode    for epoch in range(epochs):        running_loss = 0.0        for inputs, labels in train_loader:            inputs, labels = inputs.to(device), labels.to(device)            optimizer.zero_grad()            # Again ignore teacher logits            with torch.no_grad():                _, teacher_feature_map = teacher(inputs)            # Forward pass with the student model            student_logits, regressor_feature_map = student(inputs)            # Calculate the loss            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)            # Calculate the true label loss            label_loss = ce_loss(student_logits, labels)            # Weighted sum of the two losses            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss            loss.backward()            optimizer.step()            running_loss += loss.item()        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")# Notice how our test function remains the same here with the one we used in our previous case. We only care about the actual outputs because we measure accuracy.# Initialize a ModifiedLightNNRegressortorch.manual_seed(42)modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=10).to(device)# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instancemodified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())# Train and test once againtrain_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, train_loader=train_loader, epochs=10, learning_rate=0.001, feature_map_weight=0.25, ce_loss_weight=0.75, device=device)test_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_nn_light_reg, test_loader, device)\n\nEpoch 1/10, Loss: 1.2503477350220351Epoch 2/10, Loss: 0.9886605646604162Epoch 3/10, Loss: 0.8816004340605967Epoch 4/10, Loss: 0.7988157702224029Epoch 5/10, Loss: 0.7375963549784688Epoch 6/10, Loss: 0.6859350952955768Epoch 7/10, Loss: 0.6347175088837324Epoch 8/10, Loss: 0.5938715356237748Epoch 9/10, Loss: 0.556459863015148Epoch 10/10, Loss: 0.519292121576836Test Accuracy: 70.37%\n","categories":["知识蒸馏"],"tags":["知识蒸馏","CIFAR-10"]},{"title":"MNIST 手写数字分类 单卡实现","url":"/2025/05/04/torch-distributed-series/1.MNIST/","content":"本笔记本演示了训练一个卷积神经网络（CNN）来对 MNIST 数据集中的手写数字进行分类的过程。工作流程包括：\n\n数据准备：加载和预处理 MNIST 数据集。\n模型定义：使用 PyTorch 构建 CNN 模型。\n模型训练：在 MNIST 训练数据集上训练模型。\n模型评估：在 MNIST 测试数据集上测试模型并评估其性能。\n可视化：展示样本图像及其对应的标签。\n\n参考 pytorch 官方示例 https://github.com/pytorch/examples/blob/main/mnist/main.py 。\n至于为什么选择 MNIST 分类任务, 因为它就是深度学习里的 Hello World. \nimport torchimport torch.nn as nnimport torch.optim as optimimport matplotlib.pyplot as pltimport torch.nn.functional as Ffrom torchvision import datasets, transformsfrom time import time\n\n在深度学习里，真正必要的超参数，大致是下面这些：\n\n学习率（learning rate）  \n\n最最核心的超参数。  \n决定每次参数更新的步幅大小。\n学习率不合适，训练几乎一定失败。\n\n\n优化器（optimizer）  \n\n比如 SGD、Adam、AdamW 等。\n不同优化器，收敛速度、最终效果差异很大。\n有时也需要设置优化器内部超参（比如 Adam 的 ）。\n\n\n批大小（batch size）  \n\n多少样本合成一批送进模型训练。\n影响训练稳定性、收敛速度、硬件占用。\n\n\n训练轮次（epoch） 或 最大步数（max steps）  \n\n总共训练多久。\n如果训练不够长，模型欠拟合；太久则过拟合或资源浪费。\n\n\n损失函数（loss function）  \n\n明确训练目标，比如分类用 CrossEntropyLoss，回归用 MSELoss。\n不同任务必须选对损失。\n\n\n\n超参设置我们设置些最基础的超参: epoch, batch size, device, lr\nEPOCHS = 5BATCH_SIZE = 512LR = 0.001LR_DECAY_STEP_NUM = 1LR_DECAY_FACTOR = 0.5DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n数据构建直接用库函数生成 dataset 和 dataloader, 前者其实只是拿来生成 dataloader\ntransform = transforms.Compose([    transforms.ToTensor(),    transforms.Normalize((0.1307,), (0.3081,))])train_data = datasets.MNIST(    root = './mnist',    train=True,       # 设置True为训练数据，False为测试数据    transform = transform,    # download=True  # 设置True后就自动下载，下载完成后改为False即可)train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)test_data = datasets.MNIST(    root = './mnist',    train=False,       # 设置True为训练数据，False为测试数据    transform = transform,)test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)# plot one exampleprint(f'dataset: input shape: {train_data.data.size()}, label shape: {train_data.targets.size()}')print(f'dataloader iter: input shape: {next(iter(train_loader))[0].size()}, label shape: {next(iter(train_loader))[1].size()}')plt.imshow(train_data.data[0].numpy(), cmap='gray')plt.title(f'Label: {train_data.targets[0]}')plt.show()\n\n输出结果:\ndataset: input shape: torch.Size([60000, 28, 28]), label shape: torch.Size([60000])dataloader iter: input shape: torch.Size([512, 1, 28, 28]), label shape: torch.Size([512])\n\n\n\n\n\n网络设计简单的 ConvNet, 几层 CNN + MLP。初始化新模型后，先将其放到 DEVICE 上\nclass ConvNet(nn.Module):    \"\"\"    A neural network model for MNIST digit classification.    This model is designed to classify images from the MNIST dataset, which     consists of grayscale images of handwritten digits (0-9). The network     architecture includes convolutional layers for feature extraction,     followed by fully connected layers for classification.    Attributes:        features (nn.Sequential): A sequential container of convolutional             layers, activation functions, pooling, and dropout for feature             extraction.        classifier (nn.Sequential): A sequential container of fully connected             layers, activation functions, and dropout for classification.    Methods:        forward(x):            Defines the forward pass of the network. Takes an input tensor             `x`, processes it through the feature extractor and classifier,             and returns the log-softmax probabilities for each class.    \"\"\"    def __init__(self):        super(ConvNet, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(1, 32, 3, 1),            nn.ReLU(),            nn.Conv2d(32, 64, 3, 1),            nn.ReLU(),            nn.MaxPool2d(2),            nn.Dropout(0.25)        )        self.classifier = nn.Sequential(            nn.Linear(9216, 128),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(128, 10)        )    def forward(self, x):        x = self.features(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        output = F.log_softmax(x, dim=1)        return output\n\n训练和评估函数将训练和评估函数分别封装为函数,使主循环更简洁\ndef train(model, device, train_loader, optimizer):    model.train()    for batch_idx, (data, target) in enumerate(train_loader):        data, target = data.to(device), target.to(device)        optimizer.zero_grad()        output = model(data)        loss = F.nll_loss(output, target)        loss.backward()        optimizer.step()        if (batch_idx + 1) % 30 == 0:             print('Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(                batch_idx * len(data), len(train_loader.dataset),                100. * batch_idx / len(train_loader), loss.item()))            def test(model, device, test_loader):    model.eval()    test_loss = 0    correct = 0    with torch.no_grad():        for data, target in test_loader:            data, target = data.to(device), target.to(device)            output = model(data)            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标            correct += pred.eq(target.view_as(pred)).sum().item()    test_loss /= len(test_loader.dataset)    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(        test_loss, correct, len(test_loader.dataset),        100. * correct / len(test_loader.dataset)))\n\n主训练循环model = ConvNet().to(DEVICE)optimizer = optim.Adam(model.parameters(), lr=LR)scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP_NUM, gamma=LR_DECAY_FACTOR)start_time = time()  # Record the start timefor epoch in range(EPOCHS):    epoch_start_time = time()  # Record the start time of the current epoch    print(f'Epoch {epoch}/{EPOCHS}')    print(f'Learning Rate: {scheduler.get_last_lr()[0]}')    train(model, DEVICE, train_loader, optimizer)    test(model, DEVICE, test_loader)    scheduler.step()    epoch_end_time = time()  # Record the end time of the current epoch    print(f\"Time for epoch {epoch}: {epoch_end_time - epoch_start_time:.2f} seconds\")end_time = time()  # Record the end timeprint(f\"Total training time: {end_time - start_time:.2f} seconds\")+-----------------------------------------------------------------------------------------+| Processes:                                                                              ||  GPU   GI   CI        PID   Type   Process name                              GPU Memory ||        ID   ID                                                               Usage      ||=========================================================================================||    0   N/A  N/A   1795609      C   ...st/anaconda3/envs/xprepo/bin/python        448MiB ||    0   N/A  N/A   1814253      C   ...st/anaconda3/envs/xprepo/bin/python       1036MiB ||    7   N/A  N/A   4167010      C   ...guest/anaconda3/envs/QDM/bin/python      19416MiB |+-----------------------------------------------------------------------------------------+\n\n0 卡的占用 1484 MB\n完整代码import torchimport torch.nn as nnimport torch.optim as optimimport matplotlib.pyplot as pltimport torch.nn.functional as Ffrom torchvision import datasets, transformsfrom time import timeimport argparseclass ConvNet(nn.Module):    def __init__(self):        super(ConvNet, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(1, 32, 3, 1),            nn.ReLU(),            nn.Conv2d(32, 64, 3, 1),            nn.ReLU(),            nn.MaxPool2d(2),            nn.Dropout(0.25)        )        self.classifier = nn.Sequential(            nn.Linear(9216, 128),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(128, 10)        )    def forward(self, x):        x = self.features(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        output = F.log_softmax(x, dim=1)        return outputdef arg_parser():    parser = argparse.ArgumentParser(description=\"MNIST Training Script\")    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch size for training\")    parser.add_argument(\"--lr\", type=float, default=0.0005, help=\"Learning rate\")    parser.add_argument(\"--lr_decay_step_num\", type=int, default=1, help=\"Step size for learning rate decay\")    parser.add_argument(\"--lr_decay_factor\", type=float, default=0.5, help=\"Factor by which learning rate is decayed\")    parser.add_argument(\"--cuda_id\", type=int, default=0, help=\"CUDA device ID to use\")    return parser.parse_args()def prepare_data(batch_size):    transform = transforms.Compose([        transforms.ToTensor(),        transforms.Normalize((0.1307,), (0.3081,))    ])    train_data = datasets.MNIST(        root = './mnist',        train=True,       # 设置True为训练数据，False为测试数据        transform = transform,        # download=True  # 设置True后就自动下载，下载完成后改为False即可    )    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)    test_data = datasets.MNIST(        root = './mnist',        train=False,       # 设置True为训练数据，False为测试数据        transform = transform,    )    test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)    return train_loader, test_loaderdef train(model, device, train_loader, optimizer):    model.train()    for batch_idx, (data, target) in enumerate(train_loader):        data, target = data.to(device), target.to(device)        optimizer.zero_grad()        output = model(data)        loss = F.nll_loss(output, target)        loss.backward()        optimizer.step()        if (batch_idx + 1) % 30 == 0:             print('Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(                batch_idx * len(data), len(train_loader.dataset),                100. * batch_idx / len(train_loader), loss.item()))            def test(model, device, test_loader):    model.eval()    test_loss = 0    correct = 0    with torch.no_grad():        for data, target in test_loader:            data, target = data.to(device), target.to(device)            output = model(data)            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标            correct += pred.eq(target.view_as(pred)).sum().item()    test_loss /= len(test_loader.dataset)    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(        test_loss, correct, len(test_loader.dataset),        100. * correct / len(test_loader.dataset)))def train_mnist_classification():    args = arg_parser()    print(args)    EPOCHS = args.epochs    BATCH_SIZE = args.batch_size    LR = args.lr    LR_DECAY_STEP_NUM = args.lr_decay_step_num    LR_DECAY_FACTOR = args.lr_decay_factor    CUDA_ID = args.cuda_id    DEVICE = torch.device(f\"cuda:{CUDA_ID}\")    train_loader, test_loader = prepare_data(BATCH_SIZE)    model = ConvNet().to(DEVICE)    optimizer = optim.Adam(model.parameters(), lr=LR)    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP_NUM, gamma=LR_DECAY_FACTOR)    start_time = time()  # Record the start time    for epoch in range(EPOCHS):        epoch_start_time = time()  # Record the start time of the current epoch        print(f'Epoch {epoch}/{EPOCHS}')        print(f'Learning Rate: {scheduler.get_last_lr()[0]}')        train(model, DEVICE, train_loader, optimizer)        test(model, DEVICE, test_loader)        scheduler.step()        epoch_end_time = time()  # Record the end time of the current epoch        print(f\"Time for epoch {epoch}: {epoch_end_time - epoch_start_time:.2f} seconds\")    end_time = time()  # Record the end time    print(f\"Total training time: {end_time - start_time:.2f} seconds\")if __name__ == \"__main__\":    train_mnist_classification()","categories":["分布式"],"tags":["分布式","MNIST"]},{"title":"AMP - Automatic Mixed Precision","url":"/2025/05/09/torch-distributed-series/autocast_mixed_precision/","content":"torch.amp 提供了用于混合精度训练的便捷方法，其中一部分操作使用 torch.float32（即 float）数据类型，另一部分操作则使用较低精度的浮点类型（lower_precision_fp），如 torch.float16（half）或 torch.bfloat16。某些操作（例如线性层和卷积）在低精度浮点下运行速度更快，而其他操作（如归约操作）通常需要 float32 提供的更大动态范围。混合精度训练的目标是为每个操作匹配最合适的数据类型，从而在保证数值稳定性的同时提升训练性能。\n模型的精度对存储的影响import torchfrom IPython.display import Imagefrom transformers import AutoModelForCausalLM\n\n\nmodel_path = '../data_collection/Qwen2.5-0.5B-Instruct/'# all float32model = AutoModelForCausalLM.from_pretrained(model_path)print(round(model.get_memory_footprint() / (1024**2)))# all float16model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)print(round(model.get_memory_footprint() / (1024**2)))\n\n\nfp32 和 fp16 加载大小分别为 1885 MB 和 942 MB，参数精度降低就可以更好的利用显存。\n# Creates some tensors in default dtype (here assumed to be float32)a_float32 = torch.rand((8, 8), device=\"cuda\")b_float32 = torch.rand((8, 8), device=\"cuda\")c_float32 = torch.rand((8, 8), device=\"cuda\")d_float32 = torch.rand((8, 8), device=\"cuda\")with torch.amp.autocast('cuda'):    # torch.mm is on autocast's list of ops that should run in float16.    # Inputs are float32, but the op runs in float16 and produces float16 output.    # No manual casts are required.    e_float16 = torch.mm(a_float32, b_float32)    print('in autocast', e_float16.dtype, e_float16.device)    # Also handles mixed input types    f_float16 = torch.mm(d_float32, e_float16)    print('in autocast', f_float16.dtype, e_float16.device)# After exiting autocast, calls f_float16.float() to use with d_float32g_float32 = torch.mm(d_float32, f_float16.float())print('out autocast', g_float32.dtype, g_float32.device)\n\n\n\n\nin autocast torch.float16 cuda:0in autocast torch.float16 cuda:0out autocast torch.float32 cuda:0\n在使用 torch.amp.autocast() 时，自动切换 fp32 至 fp16\nfp32 and fp16\n\nSign(符号位): 1 位，0表示整数；1表示负数。\nExponent(指数位)：5位，简单地来说就是表示整数部分，范围为00001(1)到11110(30)，正常来说整数范围就是 ，但其实为了指数位能够表示负数，引入了一个偏置值，偏置值是一个固定的数，它被加到实际的指数上，在二进制16位浮点数中，偏置值是 15。这个偏置值确保了指数位可以表示从-14到+15的范围即 ，注：当指数位都为00000和11111时，它表示的是一种特殊情况，在IEEE 754标准中叫做非规范化情况。\nFraction/Mantissa(尾数位)：10位，简单地来说就是表示小数部分，存储的尾数位数为10位，但其隐含了首位的1，实际的尾数精度为11位，这里的隐含位可能有点难以理解，简单通俗来说，假设尾数部分为1001000000，为默认在其前面加一个1，最后变成1.1001000000然后换成10进制就是 \n\n正式的计算为\n(-1)^sign × 1.fraction × 2^(exponent - bias)\n\n而非正规数（subnormal numbers）的指数部分为 0，此时没有前导 1：\n(-1)^sign × 0.fraction × 2^(1 - bias)\n\nfp32 的指数位更多，表示的范围更大，gradient （weight update）的计算需要将其 scale 避免 fp16 的浮点数下溢 (由于数值太小，低于当前类型所能表示的最小的值，计算机就只好把尾数位向右移，空出第一个二进制位)。\nfp32 精度和范围更好，fp16 is fast and memory-efficient: \n\n更快的 compute throughout （8x）\n更高的 memory throughout (2x)\n更小的显存占用 (1/2x)\n\n所以综合两者是更好的策略。\npara = torch.tensor([1.], dtype=torch.float32)update = torch.tensor([.0001], dtype=torch.float32)print(para + update) # tensor([1.0001])para = torch.tensor([1.], dtype=torch.float16)update = torch.tensor([.0001], dtype=torch.float16)print(para + update) # tensor([1.], dtype=torch.float16)\n\n例如 1. + 0.0001 会导致末位超出范围，而导致被抹去。\n混合精度训练\n因此 fp16 被用于计算要求没那么高的计算上，而计算要求高的(如loss经常会被reduce到一个值，微调时较小的梯度需要更大的表示范围)则用 fp32。\n\n前向时用 fp16 计算，但为了让梯度更新效果更好，将 fp16 的梯度改为 fp32，fp32 的优化器更新后更新 fp32 的模型副本，然后再转换为 fp16 的模型。\n\n这是混合精度训练文章 https://arxiv.org/pdf/1710.03740.pdf 的示意图，FWD 和 BWD 都是 fp16，只有再更新权重时会用 fp32。\n混合精度下的 GPU memory 占用， 个模型参数（fp16）\n\nParameters：\nGradients：\nOptimizer states (Adam, all is fp32) : \nParameters copy：\nMomentum：\nVariance:：\n\n\n\nloss scaling\n![](images/autocast_mixed_precision/Screenshot 2025-05-09 224209.png)\n从图中看出梯度的分布范围，大部分都在 fp16 能表示的范围外面，准确点是小于极小值。这部分如果强行用 fp16 表示，那只能为 0 。\n\nNote that much of the FP16 representable range was left unused, while many values were below the minimum representable range and became zeros. Scaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.\n\n\nOne efficient way to shift the gradient values into FP16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation. By chain rule back-propagation ensures that all the gradient values are scaled by the same amount. This requires no extra operationsduring back-propagation and keeps the relevant gradient values from becoming zeros.\n\n因此一个方法就是将梯度值 scale 到 fp16 能表达的范围内，只要在 BWD 前把 loss 乘以一个值就可以，梯度都会相同的 scale 。将梯度更新前再反向 scale，当然在 amp 中不需要手动设置。\ni.e., 前向传播计算 loss 时将其放大，使得对应的梯度也被同样的放大，能够正常的被 fp16 表示，在更新梯度前再除以 scale。\nmodel = Net().cuda()optimizer = optim.SGD(model.parameters(), ...)for input, target in data:    optimizer.zero_grad()    # Enables autocasting for the forward pass (model + loss)    with torch.autocast(device_type=\"cuda\"):        output = model(input)        loss = loss_fn(output, target)    # Exits the context manager before backward()    loss.backward()    optimizer.step()\n","categories":["分布式"],"tags":["分布式"]},{"title":"NCCL 通信模式 简单示例","url":"/2025/05/05/torch-distributed-series/nccl_communication/","content":"接下来简单介绍几种 NCCL (NVIDIA Collective Communications Library) 的通信方式，不涉及原理，只是简单展示下效果。\n\nscatter\ngather\nbroadcast\nreduce\nall reduce\nall gather\nreduce scatter\n\nfrom IPython.display import Imageimport loggingimport torchimport torch.distributed as dist\n\npytorch 分布式相关api\ntorch.distributed.init_process_group() ，初始化进程组，必须先用这条命令才能使用 torch.distrubuted 相关操作。\ntorch.distributed.get_rank()，可以获得当前进程的 rank；\ntorch.distributed.get_world_size()，可以获得进程组的进程数量。\ntorch.distributed.barrier()，同步进程组内的所有进程，阻塞所有进程直到所有进程都执行到操作。\n\n节点获取信息def main():\tdist.init_process_group(\"nccl\")\trank = dist.get_rank()\tworld_size = dist.get_world_size()\tlogging.info(f'world size: {world_size}, rank: {rank}')\tdist.destroy_process_group()\n\n命令: torchrun –nproc_per_node 2 torch_nccl_test.py输出结果为\nINFO:root:world size: 2, rank: 0INFO:root:world size: 2, rank: 1\n\nscatterImage(url='https://pytorch.org/tutorials/_images/scatter.png', width=400)\n\n\n\n\n\n\n\n\n\ndef dist_scatter():\tdist.barrier()\trank = dist.get_rank()\tworld_size = dist.get_world_size()\tif rank == 0:\t\tlogging.info(f\"rank: {rank} is scattering data\")\ttensor = torch.zeros(world_size)\tbefore_tensor = tensor.clone()\tif dist.get_rank() == 0:\t\t# Assumes world_size of 2.\t\t# Only tensors, all of which must be the same size.\t\tt_ones = torch.ones(world_size)\t\tt_fives = torch.ones(world_size) * 5\t\t# [[1, 1], [5, 5]]\t\tscatter_list = [t_ones, t_fives]\telse:\t\tscatter_list = None\tdist.scatter(tensor, scatter_list, src=0)\tlogging.info(f\"scatter, rank: {rank}, before scatter: {repr(before_tensor)} after scatter: {repr(tensor)}\")\tdist.barrier()\n\nscatter 的用法就是从某个节点把数据分散到所有节点，包括自己。scatter_list 本身两个数组，在指定 src=0 (source)(由 rank 0 来分散数据)时，scatter_list数据被分别发送给 rank 0 和 rank 1，最终赋值到 tensor 上。\nINFO:root:rank: 0 is scattering dataINFO:root:scatter, rank: 1, before scatter: tensor([0., 0.], device='cuda:1') after scatter: tensor([5., 5.], device='cuda:1')INFO:root:scatter, rank: 0, before scatter: tensor([0., 0.], device='cuda:0') after scatter: tensor([1., 1.], device='cuda:0')\n\ngatherImage(url='https://pytorch.org/tutorials/_images/gather.png', width=400)\n\n\n\n\n\n\n\n\n\ndef dist_gather():\tdist.barrier()\trank = dist.get_rank()\tworld_size = dist.get_world_size()\ttensor = torch.tensor([rank], dtype=torch.float32)\tbefore_tensor = tensor.clone()\t\tgather_list = [torch.zeros(1) for _ in range(world_size)] if rank == 0 else None\tdist.gather(tensor, gather_list, dst=0)\t\tlogging.info(f\"gather, rank: {rank}, before gather: {repr(before_tensor)} after gather: {repr(gather_list)}\")\tdist.barrier()\n\ngather 的作用是 scatter 相反作用的，让所有 rank 上的 tensor 收集到 rank 为 dst (destination) 的卡上\nINFO:root:gather, rank: 0, before gather: tensor([0.], device='cuda:0') after gather: [tensor([0.], device='cuda:0'), tensor([1.], device='cuda:0')]INFO:root:gather, rank: 1, before gather: tensor([1.], device='cuda:1') after gather: None\n\nbroadcastImage(url='https://pytorch.org/tutorials/_images/broadcast.png', width=400)\n\n\n\n\n\n\ndef dist_broadcast():\tdist.barrier()\trank = dist.get_rank()\tworld_size = dist.get_world_size()\tsrc_rank = 0\ttensor = torch.tensor(rank)\tbefore_tensor = tensor.clone()\tdist.broadcast(tensor, src=src_rank)\tlogging.info(f\"broadcast, rank: {rank}, before broadcast tensor: {repr(before_tensor)} after broadcast tensor: {repr(tensor)}\")\tdist.barrier()\n\nbroadcast 的作用就是把 rank 为 src_rank 的 tensor 广播到其他 rank 上。\nINFO:root:broadcast, rank: 1, before broadcast tensor: tensor(1, device='cuda:1') after broadcast tensor: tensor(0, device='cuda:1')INFO:root:broadcast, rank: 2, before broadcast tensor: tensor(2, device='cuda:2') after broadcast tensor: tensor(0, device='cuda:2')INFO:root:broadcast, rank: 3, before broadcast tensor: tensor(3, device='cuda:3') after broadcast tensor: tensor(0, device='cuda:3')INFO:root:broadcast, rank: 0, before broadcast tensor: tensor(0, device='cuda:0') after broadcast tensor: tensor(0, device='cuda:0')\n\nreduceImage(url='https://pytorch.org/tutorials/_images/reduce.png', width=400)\n\n\n\n\n\n\nfrom torch.distributed import ReduceOpdef dist_reduce():\tdist.barrier()\trank = dist.get_rank()\tworld_size = dist.get_world_size()\ttensor = torch.tensor([rank], dtype=torch.float32)\tbefore_tensor = tensor.clone()\tdist.reduce(tensor, op=ReduceOp.SUM, dst=0)\t\tlogging.info(f\"reduce, rank: {rank}, before reduce: {repr(before_tensor)} after reduce: {repr(tensor)}\")\tdist.barrier()\n\nreduce 的作用和 gather 类似，都是把所有卡上数据集合到某个卡上，但不会组合为 list，会直接对这些数据进行结合式的计算。\nINFO:root:reduce, rank: 1, before reduce: tensor([1.], device='cuda:1') after reduce: tensor([1.], device='cuda:1')INFO:root:reduce, rank: 0, before reduce: tensor([0.], device='cuda:0') after reduce: tensor([6.], device='cuda:0')INFO:root:reduce, rank: 2, before reduce: tensor([2.], device='cuda:2') after reduce: tensor([2.], device='cuda:2')INFO:root:reduce, rank: 3, before reduce: tensor([3.], device='cuda:3') after reduce: tensor([3.], device='cuda:3')\nrank 0 上的 tensor 值为 0+1+2+3 = 6\nall-reduceImage(url='https://pytorch.org/tutorials/_images/all_reduce.png', width=400)\n\n\n\n\n\n\ndef dist_allreduce():\tprint_rank_0(\"all_reduce:\")\tdist.barrier()\trank = dist.get_rank()\t# world_size = torch.distributed.get_world_size()\ttensor = torch.tensor([rank], dtype=torch.float32)\tinput_tensor = tensor.clone()\tdist.all_reduce(tensor)\tlogging.info(f\"all_reduce, rank: {rank}, before allreduce tensor: {repr(input_tensor)}, after allreduce tensor: {repr(tensor)}\")\tdist.barrier()\n\nall_reduce 相当于 reduce + broadcast，all 体现在所有 rank 都要执行所有操作，可以视为 reduce + broadcast，实际应该是所有 rank 都执行 reduce。\nINFO:root:all_reduce, rank: 0, before allreduce tensor: tensor([0.], device='cuda:0'), after allreduce tensor: tensor([6.], device='cuda:0')INFO:root:all_reduce, rank: 2, before allreduce tensor: tensor([2.], device='cuda:2'), after allreduce tensor: tensor([6.], device='cuda:2')INFO:root:all_reduce, rank: 1, before allreduce tensor: tensor([1.], device='cuda:1'), after allreduce tensor: tensor([6.], device='cuda:1')INFO:root:all_reduce, rank: 3, before allreduce tensor: tensor([3.], device='cuda:3'), after allreduce tensor: tensor([6.], device='cuda:3')\n\nall gatherImage(url='https://pytorch.org/tutorials/_images/all_gather.png', width=400)\n\n\n\n\n\n\ndef dist_allgather():\tdist.barrier()\trank = dist.get_rank()\tworld_size = dist.get_world_size()\tinput_tensor = torch.tensor(rank)\ttensor_list = [torch.zeros(1, dtype=torch.int64) for _ in range(world_size)]\tdist.all_gather(tensor_list, input_tensor)\tlogging.info(f\"allgather, rank: {rank}, input_tensor: {repr(input_tensor)}, output tensor_list: {tensor_list}\")\tdist.barrier()\n\nall_gather 也是类似所有 rank 执行 gather\nINFO:root:allgather, rank: 0, input_tensor: tensor(0, device='cuda:0'), output tensor_list: [tensor([0], device='cuda:0'), tensor([1], device='cuda:0')]INFO:root:allgather, rank: 1, input_tensor: tensor(1, device='cuda:1'), output tensor_list: [tensor([0], device='cuda:1'), tensor([1], device='cuda:1')]\n\nreduce-scatterImage(url='https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/_images/reducescatter.png', width=400)\n\n\n\n\n\n\ndef dist_reducescatter():\tdist.barrier()\trank = dist.get_rank()\tworld_size = dist.get_world_size()\toutput = torch.empty(1, dtype=torch.int64)\tinput_list = [torch.tensor(rank*2+1), torch.tensor(rank*2+2)]\tdist.reduce_scatter(output, input_list, op=ReduceOp.SUM)\tdist.barrier()\tlogging.info(f\"reduce_scatter, rank: {rank}, input_list: {input_list}, tensor: {repr(output)}\")\tdist.barrier()\n\nreduce_scatter 是每个 rank 上都有完整的数据，但 reduce 后再 scatter 到所有 rank 上。\nINFO:root:reduce_scatter, rank: 0, input_list: [tensor(1, device='cuda:0'), tensor(2, device='cuda:0')], tensor: tensor([4], device='cuda:0')INFO:root:reduce_scatter, rank: 1, input_list: [tensor(3, device='cuda:1'), tensor(4, device='cuda:1')], tensor: tensor([6], device='cuda:1')\nrank 0 上是 [1,2], rank 1 上是 [3,4]， 执行 reduce 效果是 [4,6], 再加上 scatter 效果变成了 rank 0 上是 4, rank 1 上是 6。\n","categories":["分布式"],"tags":["分布式"]},{"title":"MNIST 手写数字分类 Data Parallel (DP)","url":"/2025/05/04/torch-distributed-series/2.MNIST_DP/","content":"数据并行 vs. 模型并行\n\n数据并行：模型拷贝（per device），数据 split/chunk（batch 上）\n\nthe module is replicated on each device, and each replica handles a portion of the input. \nDuring the backwards pass, gradients from each replica are summed into the original module.\n\n\n模型并行：数据拷贝（per device），模型 split/chunk（显然是单卡放不下模型的情况下）\n\n\nDP 说的直白点，就是把通常意义上的 batch 切分到各个卡上， 在主卡的控制下实现前向传播和反向传播。\n官方文档: https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html\n参数:\n\nmodule, 需要 DP 的 torch.nn.module, i.e., 你的 model\ndevice_ids=None, 参与训练的 GPU 有哪些，device_ids=gpus\noutput_device=None, 用于汇总梯度的 GPU 是哪个，output_device=gpus[0]\ndim=0, 数据切分的维度, 一般就是第一维度 batch 维度来切分, dim = 0 [30, xxx] -&gt; [10, …], [10, …], [10, …] on 3 GPUs\n\n\nThe parallelized module must have its parameters and buffers on device_ids[0] before running(forward/backward) this DataParallel module. 模型参数必须先缓存在给定卡中的第一张卡上。\n\n如果我只执行了 nn.DataParallel, 没有执行 model.to(device) device 必须是选中的第一张卡, 否则会报错 RuntimeError: module must have its parameters and buffers on device cuda:4 (device_ids[0]) but found one of them on device: cpu. PS: 你没先缓存到正确的卡上\nclass ConvNet(nn.Module):        def __init__(self):        super(ConvNet, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(1, 32, 3, 1),            nn.ReLU(),            nn.Conv2d(32, 64, 3, 1),            nn.ReLU(),            nn.MaxPool2d(2),            nn.Dropout(0.25)        )        self.classifier = nn.Sequential(            nn.Linear(9216, 128),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(128, 10)        )    def forward(self, input):        x = self.features(input)        x = torch.flatten(x, 1)        x = self.classifier(x)        output = F.log_softmax(x, dim=1)        # 输出 forward 变量形状，主要关注 batch 大小        print(f\"[Inside]: input shape: {input.size()}, label shape: {output.size()}\")        return output\n\n单卡 forwarddevice = torch.device(f\"cuda:4\" if torch.cuda.is_available() else \"cpu\")model = ConvNet()model = model.to(device)for batch_idx, (data, target) in enumerate(train_loader):    data, target = data.to(device), target.to(device)    print(f\"[Outside]: input shape: {data.size()}, label shape: {target.size()}\")    output = model(data)    break\n\nDP forwardCUDA_DEVICE_IDS = [4,5]device = torch.device(f\"cuda:{CUDA_DEVICE_IDS[0]}\" if torch.cuda.is_available() else \"cpu\")model = ConvNet()model = model.to(device)model = nn.DataParallel(model, device_ids=CUDA_DEVICE_IDS)for batch_idx, (data, target) in enumerate(train_loader):    data, target = data.to(device), target.to(device)    output = model(data)    print(f\"[Outside]: input shape: {data.size()}, label shape: {target.size()}, output shape: {output.size()}\")    break\n\nforward 参数对比 (单卡和DP-2卡)\n\n单卡[Outside]: input shape: torch.Size([512, 1, 28, 28]), label shape: torch.Size([512])[Inside]: input shape: torch.Size([512, 1, 28, 28]), label shape: torch.Size([512, 10])DP-2卡[Inside]: input shape: torch.Size([256, 1, 28, 28]), label shape: torch.Size([256, 10])[Inside]: input shape: torch.Size([256, 1, 28, 28]), label shape: torch.Size([256, 10])[Outside]: input shape: torch.Size([512, 1, 28, 28]), label shape: torch.Size([512]), output shape: torch.Size([512, 10])\n\n用DP情况下虽然循环里每次的 batch 大小还是一样的, 但模型 forward 确实将 batch / len(device_ids), 原来 512 的 batch 变为 256, 两个卡上各自有一个模型分别跑了 1 / len(device_ids) 的数据。\n单卡+-----------------------------------------+------------------------+----------------------+|   4  NVIDIA GeForce RTX 4090        On  |   00000000:81:00.0 Off |                  Off || 46%   33C    P8             14W /  450W |     719MiB /  24564MiB |      0%      Default ||                                         |                        |                  N/A |+-----------------------------------------+------------------------+----------------------+|   5  NVIDIA GeForce RTX 4090        On  |   00000000:A1:00.0 Off |                  Off || 43%   32C    P8             20W /  450W |       4MiB /  24564MiB |      0%      Default ||                                         |                        |                  N/A |+-----------------------------------------+------------------------+----------------------+DP-2卡+-----------------------------------------+------------------------+----------------------+|   4  NVIDIA GeForce RTX 4090        On  |   00000000:81:00.0 Off |                  Off || 45%   34C    P2             55W /  450W |     717MiB /  24564MiB |      0%      Default ||                                         |                        |                  N/A |+-----------------------------------------+------------------------+----------------------+|   5  NVIDIA GeForce RTX 4090        On  |   00000000:A1:00.0 Off |                  Off || 43%   33C    P2             57W /  450W |     719MiB /  24564MiB |      0%      Default ||                                         |                        |                  N/A |+-----------------------------------------+------------------------+----------------------+\n每个卡上都有一份模型参数和 batch / len(device_ids) 的数据\nDP 训练实际只是增加了 model = nn.DataParallel(model, device_ids=CUDA_DEVICE_IDS), 后续 forward 和 backpropagation 都不需要改变\nCUDA_DEVICE_IDS = [4,5]DEVICE = torch.device(f\"cuda:{CUDA_DEVICE_IDS[0]}\" if torch.cuda.is_available() else \"cpu\")model = ConvNet()model = model.to(DEVICE)model = nn.DataParallel(model, device_ids=CUDA_DEVICE_IDS)optimizer = optim.Adam(model.parameters(), lr=LR)scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP_NUM, gamma=LR_DECAY_FACTOR)start_time = time()  # Record the start timefor epoch in range(EPOCHS):    epoch_start_time = time()  # Record the start time of the current epoch    print(f'Epoch {epoch}/{EPOCHS}')    print(f'Learning Rate: {scheduler.get_last_lr()[0]}')    train(model, DEVICE, train_loader, optimizer)    test(model, DEVICE, test_loader)    scheduler.step()    epoch_end_time = time()  # Record the end time of the current epoch    print(f\"Time for epoch {epoch}: {epoch_end_time - epoch_start_time:.2f} seconds\")end_time = time()  # Record the end timeprint(f\"Total training time: {end_time - start_time:.2f} seconds\")\n\n看下 nn.DataParallel 的内部 forward 函数, 有几行代码显示了大致流程\ninputs, module_kwargs = self.scatter(inputs, kwargs, self.device_ids) # 分散数据 inputsreplicas = self.replicate(self.module, self.device_ids[: len(inputs)]) # 复制模型 replicasoutputs = self.parallel_apply(replicas, inputs, module_kwargs) # 并行计算 outputsreturn self.gather(outputs, self.output_device) # 合并结果 gather\n\n模型输出（outputs）来自多个子 GPU，但会 在主卡上 gather，因为 torch.nn.DataParallel 的默认行为是把所有子 GPU 的输出，gather 回主 GPU（device[0]）。\n为什么要 gather, 因为 label （targets）通常在主卡上，所以为了计算 loss，需要把输出也 gather 到主卡，才能和 labels 对应。PS：计算损失是要求参数在同一个 device 上。\nloss.backward() 触发 autograd，它会根据 gather 的结构把 grad_output 自动 scatter 回子卡，每张子卡用自己的输出执行 backward。最终每个 gpu 的 gradient 都还要进行统一的更新，将梯度聚合再下方梯度，即 all-reduce。\n实现 DP 的一种经典编程框架叫 “参数服务器” parameter server，在这个框架里，计算 GPU 称为 Worker，**梯度聚合 GPU 称为 Server。**在实际应用中，为了尽量减少通讯量，一般可选择一个 Worker 同时作为 Server。比如可把梯度全发到 GPU0 上做聚合。DP 的通信瓶颈在于 server 的通信开销，sever 没法一次性立马接受所有数据，所以当 worker 无法传输数据且已经计算完成时，它就只能摸鱼。\n!!! 注意：DP 已经不被推荐了\n完整代码import torchimport torch.nn as nnimport torch.optim as optimimport matplotlib.pyplot as pltimport torch.nn.functional as Ffrom torchvision import datasets, transformsfrom time import timeimport argparseclass ConvNet(nn.Module):    def __init__(self):        super(ConvNet, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(1, 32, 3, 1),            nn.ReLU(),            nn.Conv2d(32, 64, 3, 1),            nn.ReLU(),            nn.MaxPool2d(2),            nn.Dropout(0.25)        )        self.classifier = nn.Sequential(            nn.Linear(9216, 128),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(128, 10)        )    def forward(self, x):        x = self.features(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        output = F.log_softmax(x, dim=1)        return outputdef arg_parser():    parser = argparse.ArgumentParser(description=\"MNIST Training Script\")    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch size for training\")    parser.add_argument(\"--lr\", type=float, default=0.0005, help=\"Learning rate\")    parser.add_argument(\"--lr_decay_step_num\", type=int, default=1, help=\"Step size for learning rate decay\")    parser.add_argument(\"--lr_decay_factor\", type=float, default=0.5, help=\"Factor by which learning rate is decayed\")    parser.add_argument(\"--cuda_ids\", type=int, nargs='+', default=[0,1], help=\"List of CUDA device IDs to use\")    return parser.parse_args()def prepare_data(batch_size):    transform = transforms.Compose([        transforms.ToTensor(),        transforms.Normalize((0.1307,), (0.3081,))    ])    train_data = datasets.MNIST(        root = './mnist',        train=True,       # 设置True为训练数据，False为测试数据        transform = transform,        # download=True  # 设置True后就自动下载，下载完成后改为False即可    )    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)    test_data = datasets.MNIST(        root = './mnist',        train=False,       # 设置True为训练数据，False为测试数据        transform = transform,    )    test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)    return train_loader, test_loaderdef train(model, device, train_loader, optimizer):    model.train()    for batch_idx, (data, target) in enumerate(train_loader):        data, target = data.to(device), target.to(device)        optimizer.zero_grad()        output = model(data)        loss = F.nll_loss(output, target)        loss.backward()        optimizer.step()        if (batch_idx + 1) % 30 == 0:             print('Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(                batch_idx * len(data), len(train_loader.dataset),                100. * batch_idx / len(train_loader), loss.item()))            def test(model, device, test_loader):    model.eval()    test_loss = 0    correct = 0    with torch.no_grad():        for data, target in test_loader:            data, target = data.to(device), target.to(device)            output = model(data)            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标            correct += pred.eq(target.view_as(pred)).sum().item()    test_loss /= len(test_loader.dataset)    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(        test_loss, correct, len(test_loader.dataset),        100. * correct / len(test_loader.dataset)))def train_mnist_classification():    args = arg_parser()    print(args)    EPOCHS = args.epochs    BATCH_SIZE = args.batch_size    LR = args.lr    LR_DECAY_STEP_NUM = args.lr_decay_step_num    LR_DECAY_FACTOR = args.lr_decay_factor    CUDA_DEVICE_IDS = args.cuda_ids    DEVICE = torch.device(f\"cuda:{CUDA_DEVICE_IDS[0]}\")    train_loader, test_loader = prepare_data(BATCH_SIZE)    model = ConvNet().to(DEVICE)    model = nn.DataParallel(model, device_ids=CUDA_DEVICE_IDS)    optimizer = optim.Adam(model.parameters(), lr=LR)    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP_NUM, gamma=LR_DECAY_FACTOR)    start_time = time()  # Record the start time    for epoch in range(EPOCHS):        epoch_start_time = time()  # Record the start time of the current epoch        print(f'Epoch {epoch}/{EPOCHS}')        print(f'Learning Rate: {scheduler.get_last_lr()[0]}')        train(model, DEVICE, train_loader, optimizer)        test(model, DEVICE, test_loader)        scheduler.step()        epoch_end_time = time()  # Record the end time of the current epoch        print(f\"Time for epoch {epoch}: {epoch_end_time - epoch_start_time:.2f} seconds\")    end_time = time()  # Record the end time    print(f\"Total training time: {end_time - start_time:.2f} seconds\")if __name__ == \"__main__\":    train_mnist_classification()","categories":["分布式"],"tags":["分布式","MNIST"]},{"title":"MNIST 手写数字分类 Distributed Data Parallel (DDP)","url":"/2025/05/04/torch-distributed-series/3.MNIST_DDP/","content":"\nThe difference between DistributedDataParallel and DataParallel is: DistributedDataParallel uses multiprocessing where a process is created for each GPU, while DataParallel uses multithreading.By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\n\n\nDDP vs DP 的并发模式\n\n\nDDP使用的是多进程multiprocessing\n\n每个 GPU 对应一个独立的 Python 进程。\n各 GPU/进程之间通过通信（比如 NCCL）同步梯度。\n进程之间可以并行,每个进程独占一个 GPU，自由度高、效率高。\n\n\nDP 使用的是多线程（multithreading）\n\n一个 Python 主进程控制多个线程，每个线程对应一个 GPU 上的模型副本。\n所有线程共享同一个 Python 解释器（主进程中的 GIL 环境）。\n在多线程环境下，同一时刻只能有一个线程执行 Python 字节码\n\n\n\n\nGIL 的性能问题\n\nPython 有个限制叫 GIL（Global Interpreter Lock）：\n\n在 Python 中，进程之间可以并行，线程之间只能并发。\n在多线程环境下，同一时刻只能有一个线程执行 Python 字节码。\n这意味着虽然多个线程运行在不同 GPU 上，但只要你涉及到 Python 层的逻辑（如 forward 调度、数据调度），就会被 GIL 限制，造成瓶颈。\n\nDDP 的多进程模式就天然绕开了 GIL，每个进程有独立的 Python 解释器和 GIL，不会互相争抢锁。所以执行速度更快、效率更高、更适合大模型和多 GPU 并行。\n\nTo use DistributedDataParallel on a host with N GPUs, you should spawn up N processes, ensuring that each process exclusively works on a single GPU from 0 to N-1.\n\n总结下，DDP 用多进程给每个 GPU 配一个独立的进程，这样就不用多个线程去抢 Python 的 GIL，避免了 DataParallel 由于多线程带来的性能开销。\n分布式数据并行时，模型（model parameters）/优化器（optimizer states）每张卡都会拷贝一份（replicas），在整个训练过程中 DDP 始终在卡间维持着模型参数和优化器状态的同步一致性；\nDDP 将 batch input，通过 DistributedSampler split &amp; 分发到不同的 gpus 上，此时虽然模型/optimizer 相同，但因为数据输入不同，导致 loss 不同，反向传播时计算到的梯度也会不同，如何保证卡间，model/optimizer 的同步一致性，之前 DP 用的 parameter server，而它的问题就在于通信压力都在 server，所以 DDP 对这方面的改进是 ring all-reduce algorithm，将 Server 上的通讯压力均衡转到各个 Worker 上\n注意有两个核心概念:\n\nAll to one：reduce \none to All：broadcast\n\n\n\n\n方法名\n通信结构\n通信路径\n数据流向\n聚合策略\n通信瓶颈位置\n通信效率\n适合场景\n\n\n\nParameter Server\n中心化（星型）\n所有 Worker ⇄ PS\n上传全部梯度 → 聚合 → 下发参数\nPS 聚合\nPS 带宽和计算压力\n❌ 低（集中式瓶颈）\n小规模训练，原型实验\n\n\nTree All-Reduce\n层次化（树型）\n节点间按树结构上传/下传\n层层上传聚合 → 再层层广播\n层次加和 &amp; 广播\n上层节点（树根）\n✅ 中（ 轮次）\n多机多卡，合理拓扑连接\n\n\nBroadcast + Reduce\n两阶段（集中）\n所有 → 主节点（reduce） → 所有\n所有上传 → 中心聚合 → 广播下发\n单节点聚合\n主节点\n❌ 低\n小规模单机多卡\n\n\nRing All-Reduce\n环形（对称）\n相邻节点之间点对点传输\n均匀传递/聚合，每轮处理一块数据\n分块加和 &amp; 拼接\n无集中瓶颈\n✅✅ 高（带宽最优）\n大规模 GPU 并行，主流方案\n\n\nParameter Server（PS）和 Broadcast + Reduce 在通信机制上本质相似，区别只在于:\n\nPS 是显式设计了专门的“参数服务器”角色；\nBroadcast + Reduce 是“隐式指定”某个节点承担聚合与广播任务。\n\nring all-reduce:\n\nReduce-scatter:首先将 gradient 分为 n 块，在第 i 轮 (0&lt;= i &lt; n-1)，每个 gpu j 把 第 (i+j) % n 块的数据传给下一个 gpu (j+1 % n)，即每个 gpu 都把自己一个块给下一个做加法，在 n 轮结束后，每个 gpu 上都有一个块是完整的聚合了所有不同 gpu 的 gradient。\nAll-gather: 将每个 gpu 上的完整聚合后的 gradient 依次传给下一个 gpu，再传递 n-1 次就使所有 gpu 的每块 gradient 都是完整聚合的数据。\n\n虽然传递的数据量还是和 PS 一样，但传输压力平均到每个 gpu 上，不需要单个 worker 承担明显大的压力。\n\n\n\n概念/参数名\n中文含义\n含义解释\n示例（2节点 × 每节点4GPU）\n\n\n\nworld\n全局进程空间\n指整个分布式系统中参与训练的所有进程总和\n2 节点 × 4 GPU = 8 个进程\n\n\nworld_size\n全局进程数\nworld 中的进程总数，参与通信、同步、梯度聚合的总 worker 数\n8\n\n\nrank\n全局进程编号\n当前进程在 world 中的唯一编号，范围是 '_' allowed only in math mode[0, \\text{world_size} - 1]\n第1节点是 03，第2节点是 47\n\n\nnode\n物理节点/机器\n实际的服务器或物理机，每个节点运行多个进程，通常对应一台机器\n2台服务器（假设每台4 GPU）\n\n\nnode_rank\n节点编号\n当前节点在所有节点中的编号，通常用于标识不同机器\n第1台是 0，第2台是 1\n\n\nlocal_rank\n本地GPU编号\n当前进程在所在节点上的 GPU 编号，绑定 cuda(local_rank)\n每台机器上分别为 0~3\n\n\n简洁点，world 代表所有服务器上的 gpu，rank 代表 world 视角下的 gpu 编号；node 代表某个具体的服务器，node_rank 代表 world 视角下的 node 编号，local_rank 代表 node 视角下的 gpu 编号。\n引入 DDP 相关库import osimport torchimport torch.nn.functional as Ffrom torch.utils.data import Dataset, DataLoader# 以下是分布式相关的import torch.multiprocessing as mpfrom torch.utils.data.distributed import DistributedSampler # 分发数据集from torch.nn.parallel import DistributedDataParallel as DDP # 用 DDP 封装 module 以支持分布式训练from torch.distributed import init_process_group, destroy_process_group # 初始化和销毁进程组，一个 process 代表一个 gpu 进程\n\nddp 对原始代码的修改\n\n\n参数\n作用说明\n\n\n\nMASTER_ADDR\n指定 主节点（rank=0 所在节点）的 IP 地址或主机名，作为所有进程连接的“服务器”\n\n\nMASTER_PORT\n指定主节点上用于通信监听的端口号，所有进程都通过这个端口进行连接与协调\n\n\n为什么只需要指定主节点的地址和端口？所有进程必须“集合”在一起组成一个通信组（process group）；这个过程需要一个 协调者，就像组织会议需要一个人发出会议链接一样；PyTorch DDP 把这个协调角色交给 rank == 0 的进程（主节点）；其它进程只需要“知道去哪找这个协调者”就能完成初始化。\n主节点负责协调组网，在 DDP 初始化时，所有节点主动连接主节点，每个节点都会告知主节点自己的地址和端口，主节点收集所有其他进程的网络信息，构建全局通信拓扑，将通信配置信息广播回每个进程，包括每个 rank 要连接哪些 peer，这样每个进程就可以进行后续的双向传输，而不再依赖主节点作为中转。\n\n\n\n主节点（rank 0）\n工作节点（rank 1,2,…）\n\n\n\n在 MASTER_PORT 启动一个监听服务（如 TCP server）\n主动连接 MASTER_ADDR:MASTER_PORT\n\n\n监听并接受连接，记录加入者信息\n与主节点握手，注册自己的 rank、地址等\n\n\n构建通信拓扑，如 Ring 或 NCCL 分组等\n一旦接入，就获得组网配置，与其他 worker 点对点通信\n\n\nddp 初始化和销毁进程def ddp_setup(rank, world_size):    \"\"\"    Args:        rank: Unique identifier of each process        world_size: Total number of processes    \"\"\"    # rank 0 process    os.environ[\"MASTER_ADDR\"] = \"localhost\"    os.environ[\"MASTER_PORT\"] = \"12355\"    # nccl：NVIDIA Collective Communication Library     # 分布式情况下的，gpus 间通信    torch.cuda.set_device(rank)    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\nDDP 会在每个 GPU 上运行一个进程，每个进程中都有一套完全相同的 Trainer 副本（包括 model 和 optimizer），各个进程之间通过一个进程池进行通信。\nddp 包装 model训练函数不需要多大的修改，使用 DistributedDataParallel 包装模型，这样模型才能在各个进程间同步参数。包装后 model 变成了一个 DDP 对象，要访问其参数得这样写 self.model.module.state_dict()\n运行过程中单独控制某个进程进行某些操作，比如要想保存 ckpt，由于每张卡里都有完整的模型参数，所以只需要控制一个进程保存即可。需要注意的是：使用 DDP 改写的代码会在每个 GPU 上各自运行，因此需要在程序中获取当前 GPU 的 rank（gpu_id），这样才能对针对性地控制各个 GPU 的行为。\nclass Trainer:    def __init__(        self,        model: torch.nn.Module,        train_data: DataLoader,        optimizer: torch.optim.Optimizer,        gpu_id: int,        save_every: int,     ) -&gt; None:        self.gpu_id = gpu_id        self.model = model.to(gpu_id)        self.train_data = train_data         self.optimizer = optimizer        self.save_every = save_every        self.model = DDP(model, device_ids=[gpu_id])    # model 要用 DDP 包装一下    def _run_batch(self, source, targets):        self.optimizer.zero_grad()        output = self.model(source)        loss = F.cross_entropy(output, targets)        loss.backward()        self.optimizer.step()    def _run_epoch(self, epoch):        b_sz = len(next(iter(self.train_data))[0])        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")        self.train_dataloader.sampler.set_epoch(epoch) # 注意需要在各 epoch 入口调用该 sampler 对象的 set_epoch() 方法，否则每个 epoch 加载的样本顺序都不变        for source, targets in self.train_data:            source = source.to(self.gpu_id)            targets = targets.to(self.gpu_id)            self._run_batch(source, targets)    def _save_checkpoint(self, epoch):        ckp = self.model.state_dict()        PATH = \"checkpoint.pt\"        torch.save(ckp, PATH)        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")    def train(self, max_epochs: int):        for epoch in range(max_epochs):            self._run_epoch(epoch)            if self.gpu_id == 0 and epoch % self.save_every == 0:                self._save_checkpoint(epoch)\n\n在程序入口初始化进程池；在程序出口销毁进程池\ndef main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):    # 初始化进程池    ddp_setup(rank, world_size)    # 进行训练    dataset, model, optimizer = load_train_objs()    train_data = prepare_dataloader(dataset, batch_size)    trainer = Trainer(model, train_data, optimizer, rank, save_every)    trainer.train(total_epochs)       # 销毁进程池    destroy_process_group()\n\nDistributedSampler构造 Dataloader 时使用 DistributedSampler 作为 sampler，这个采样器可以自动将数量为 batch_size 的数据分发到各个GPU上，并保证数据不重叠。理解是可以是这样的，但实际是根据 rank 让每个 gpu 能索引到的数据不一样，每个 gpu 上也是有重复的 Dataloader 的，但每个gpu 上 rank 设置不同，Dataloader sample 先根据 shuffle 打乱顺序，再控制不同 rank 能索引到的数据，以实现类似分发的效果。\nRank 0 sees: [4, 7, 3, 0, 6] Rank 1 sees: [1, 5, 9, 8, 2]\ndef prepare_dataloader(dataset: Dataset, batch_size: int):    return DataLoader(        dataset,        batch_size=batch_size,        pin_memory=True,        shuffle=False,                      # 设置了新的 sampler，参数 shuffle 要设置为 False         sampler=DistributedSampler(dataset) # 这个 sampler 自动将数据分块后送个各个 GPU，它能避免数据重叠    )\n\nset_epoch(epoch) 用于设置当前训练 epoch，以确保在分布式训练中 每个进程对数据的打乱顺序一致，从而保证每个 rank 分到的数据是互不重叠且可复现的。\n当 DistributedSampler 的 shuffle=True 时，它在每个 epoch 会用 torch.Generator().manual_seed(seed) 生成新的随机索引顺序。但：\n\n如果不调用 set_epoch()，每个进程将使用相同的默认种子；\n会导致每个 epoch 每个进程打乱后的样本索引相同 → 重复取样，每个 epoch 的训练数据都一样 → 训练不正确！\n\n你确实可以不手动设置 rank 和 world_size，因为 DistributedSampler 会自动从环境变量中获取它们。如果你不传入 rank 和 num_replicas，PyTorch 会调用：\n\ntorch.distributed.get_world_size()  # 获取 world_size\ntorch.distributed.get_rank()        # 获取当前进程 rank\n\nimport torchfrom torch.utils.data import Dataset, DataLoader, DistributedSampler# 自定义一个简单的数据集：返回 [0, 1, ..., n-1]class RangeDataset(Dataset):    def __init__(self, n):        self.data = list(range(n))    def __len__(self):        return len(self.data)    def __getitem__(self, idx):        return self.data[idx]# 模拟两张卡（进程）下的样本访问情况，并支持 set_epochdef simulate_distributed_sampler(n=10, world_size=2, num_epochs=2):    dataset = RangeDataset(n)    for epoch in range(num_epochs):        print(f\"\\nEpoch {epoch}\")        for rank in range(world_size):            # 设置 shuffle=True 并调用 set_epoch            sampler = DistributedSampler(                dataset,                num_replicas=world_size,                rank=rank,                shuffle=True,            )            sampler.set_epoch(epoch)  # 关键：确保每轮不同但在所有 rank 一致            dataloader = DataLoader(dataset, batch_size=1, sampler=sampler)            data_seen = [batch[0].item() for batch in dataloader]            print(f\"Rank {rank} sees: {data_seen}\")simulate_distributed_sampler(n=10, world_size=2, num_epochs=2)\n\nEpoch 0Rank 0 sees: [4, 7, 3, 0, 6]Rank 1 sees: [1, 5, 9, 8, 2]Epoch 1Rank 0 sees: [5, 1, 0, 9, 7]Rank 1 sees: [6, 2, 8, 3, 4]\n\nmultiprocessing.spawn 创建多卡进程使用 torch.multiprocessing.spawn 方法将代码分发到各个 GPU 的进程中执行。在当前机器上启动 nprocs=world_size 个子进程，每个进程执行一次 main() 函数，并由 mp.spawn 自动赋值第一个参数（目的是执行 nprocs 个进程，第一个参数为 0 ~ nprocs-1）。\ndef start_process(i):    # Each process is assigned a file to write tracebacks to.  We    # use the file being non-empty to indicate an exception    # occurred (vs an expected shutdown).  Note: this previously    # used a multiprocessing.Queue but that can be prone to    # deadlocks, so we went with a simpler solution for a one-shot    # message between processes.    tf = tempfile.NamedTemporaryFile(        prefix=\"pytorch-errorfile-\", suffix=\".pickle\", delete=False    )    tf.close()    os.unlink(tf.name)    process = mp.Process(        target=_wrap,        args=(fn, i, args, tf.name),        daemon=daemon,    )    process.start()    return i, process, tf.nameif not start_parallel:    for i in range(nprocs):        idx, process, tf_name = start_process(i)        error_files[idx] = tf_name        processes[idx] = process\n\n可以执行执行以下代码，它展现了 mp 创建进程的效果\nimport torch.multiprocessing as mpdef run(rank, message):    print(f\"[Rank {rank}] Received message: {message}\")if __name__ == \"__main__\":    world_size = 4  # 启动 4 个进程（模拟 4 个GPU）    mp.spawn(        fn=run,        args=(\"hello world\",),   # 注意是 tuple 格式        nprocs=world_size,        join=True    )\n\n效果为:\n[Rank 0] Received message: hello world[Rank 3] Received message: hello world[Rank 2] Received message: hello world[Rank 1] Received message: hello world# 利用 mp.spawn，在整个 distribution group 的 nprocs 个 GPU 上生成进程来执行 fn 方法，并能设置要传入 fn 的参数 args# 注意不需要传入 fn 的 rank 参数，它由 mp.spawn 自动分配import multiprocessing as mpworld_size = torch.cuda.device_count()mp.spawn(    fn=main,     args=(world_size, args.save_every, args.total_epochs, args.batch_size),     nprocs=world_size)!CUDA_VISIBLE_DEIVES=0,1 python mnist_ddp.py\n\ntorchruntorchrun 是 PyTorch 官方推荐的分布式训练启动工具，它用来 自动管理多进程启动、环境变量传递和通信初始化，替代早期的 torch.distributed.launch 工具。\n\n它帮你在每个 GPU 上自动启动一个训练进程；\n它设置好 DDP 所需的环境变量（如 RANK, WORLD_SIZE, LOCAL_RANK, MASTER_ADDR 等）；\n它会自动将这些参数传递给你的脚本中的 torch.distributed.init_process_group()。\n\ntorchrun == python -m torch.distributed.launch –use-env\n\n\n\n参数名\n类型\n说明\n\n\n\n--nproc_per_node\nint\n每台机器上启动的进程数（默认值为 1）\n\n\n--nnodes\nint\n总节点（机器）数\n\n\n--node_rank\nint\n当前节点编号（范围：0 ~ nnodes-1）\n\n\n--rdzv_backend\nstr\nrendezvous 后端（默认 c10d，一般不改）\n\n\n--rdzv_endpoint\nstr\nrendezvous 主地址和端口，格式如 \"localhost:29500\"\n\n\n--rdzv_id\nstr\n作业唯一标识，默认 \"default\"\n\n\n--rdzv_conf\nstr\n可选的 kv 参数，用逗号分隔，如 \"key1=val1,key2=val2\"\n\n\n--max_restarts\nint\n失败时最多重启次数（默认 3）\n\n\n--monitor_interval\nfloat\nmonitor 进程检查的间隔（秒）\n\n\n--run_path\nstr\n若脚本是模块路径形式，比如 my_module.train，则用此代替 script\n\n\n--tee\nstr\n控制日志输出，可选值为 \"stdout\" 或 \"stderr\"\n\n\n--log_dir\nstr\n日志输出目录（默认当前目录）\n\n\n--redirects\nstr\n重定向日志，可选：all, none, rank，如 all:stdout\n\n\n--no_python\nflag\n若已是 Python 脚本（不用再次 python 调用），可加这个 flag\n\n\n以上的 rendezvous 是每个进程通过 rendezvous 找到主节点，然后加入。之后的通信阶段用 backend, 即 NCCL，在 init_process_group 设置。\n最常见的几个参数的用法是\ntorchrun \\  --nproc_per_node=4 \\  --nnodes=1 \\  --node_rank=0 \\  --rdzv_endpoint=localhost:29500 \\  your_script.py\n\n对比下是否使用 torchrun 时的行为差别\n两种 DDP 启动模式的关键区别\n\n\n\n对比项\n不使用 torchrun（手动）\n使用 torchrun（推荐方式）\n\n\n\n启动方式\n使用 mp.spawn(fn, ...)\n使用 torchrun --nproc_per_node=N\n\n\nrank, world_size 设置方式\n手动传入（通过 spawn 的参数）\n自动由 torchrun 设置环境变量\n\n\n主节点地址 / 端口\n你必须手动设置 MASTER_ADDR/PORT\ntorchrun 会自动设置这些环境变量\n\n\n是否需控制进程数量\n手动使用 spawn 创建\n自动由 torchrun 创建\n\n\n是否读取环境变量\n❌ 默认不会\n✅ 自动从环境变量中读取（如 RANK, LOCAL_RANK）\n\n\n脚本能否直接运行（python train.py）\n❌ 通常不行，需要多进程协调\n✅ 支持直接 torchrun train.py 运行\n\n\n是否适用于多机\n❌ 手动处理跨节点逻辑\n✅ 内建 --nnodes, --node_rank, 可跨机运行\n\n\n\ninit_process_group() 的行为\n\n\n\n情况\n说明\n\n\n\n手动传 rank 和 world_size\n常用于 mp.spawn 场景（你在代码里传了参数）\n\n\n不传，内部读取环境变量\n如果你用的是 torchrun，环境变量如 RANK、WORLD_SIZE 自动设置了\n\n\n不传又没用 torchrun\n❌ 报错：因为 init_process_group 找不到必要的通信信息\n\n\n\n当你运行：\ntorchrun --nproc_per_node=4 --rdzv_endpoint=localhost:29500 train.py\n\n它在后台自动设置了以下环境变量（对每个进程）：\nRANK=0         # 每个进程唯一编号WORLD_SIZE=4   # 总进程数LOCAL_RANK=0   # 当前进程在本节点内的编号MASTER_ADDR=localhostMASTER_PORT=29500\n\n而 init_process_group(backend=\"nccl\") 会自动从这些环境变量中解析配置，无需你显式传入。\n非 torchrun 完整代码import osimport torchimport torch.nn as nnimport torch.optim as optimimport matplotlib.pyplot as pltimport torch.nn.functional as Ffrom torch.utils.data import Dataset, DataLoaderfrom torchvision import datasets, transformsfrom time import timeimport argparse# 对 python 多进程的一个 pytorch 包装import torch.multiprocessing as mp# 用于收集一些用于汇总的数据import torch.distributed as dist# 这个 sampler 可以把采样的数据分散到各个 CPU 上                                      from torch.utils.data.distributed import DistributedSampler     # 实现分布式数据并行的核心类        from torch.nn.parallel import DistributedDataParallel as DDP         # DDP 在每个 GPU 上运行一个进程，其中都有一套完全相同的 Trainer 副本（包括model和optimizer）# 各个进程之间通过一个进程池进行通信，这两个方法来初始化和销毁进程池from torch.distributed import init_process_group, destroy_process_group def ddp_setup(rank, world_size):    \"\"\"    setup the distribution process group    Args:        rank: Unique identifier of each process        world_size: Total number of processes    \"\"\"    # MASTER Node（运行 rank0 进程，多机多卡时的主机）用来协调各个 Node 的所有进程之间的通信    os.environ[\"MASTER_ADDR\"] = \"localhost\" # 由于这里是单机实验所以直接写 localhost    os.environ[\"MASTER_PORT\"] = \"12355\"     # 任意空闲端口    init_process_group(        backend=\"nccl\",                     # Nvidia CUDA CPU 用这个 \"nccl\"        rank=rank,                                  world_size=world_size    )    torch.cuda.set_device(rank)    class ConvNet(nn.Module):    def __init__(self):        super(ConvNet, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(1, 32, 3, 1),            nn.ReLU(),            nn.Conv2d(32, 64, 3, 1),            nn.ReLU(),            nn.MaxPool2d(2),            nn.Dropout(0.25)        )        self.classifier = nn.Sequential(            nn.Linear(9216, 128),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(128, 10)        )    def forward(self, x):        x = self.features(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        output = F.log_softmax(x, dim=1)        return output    # 自定义Datasetclass MyDataset(Dataset):    def __init__(self, data):        self.data = data    def __len__(self):        return len(self.data)    def __getitem__(self, idx):        image, label = self.data[idx]        return image, labelclass Trainer:    def __init__(        self,        model: torch.nn.Module,        train_data: DataLoader,        optimizer: torch.optim.Optimizer,        gpu_id: int,        save_every: int,    ) -&gt; None:        self.gpu_id = gpu_id        self.model = model.to(gpu_id)        self.train_data = train_data        self.optimizer = optimizer        self.save_every = save_every                    # 指定保存 ckpt 的周期        self.model = DDP(model, device_ids=[gpu_id])    # model 要用 DDP 包装一下    def _run_batch(self, source, targets):        self.optimizer.zero_grad()        output = self.model(source)        loss = F.cross_entropy(output, targets)        loss.backward()        self.optimizer.step()        # 分布式同步 loss        reduced_loss = loss.detach()        dist.all_reduce(reduced_loss, op=dist.ReduceOp.SUM)        reduced_loss /= dist.get_world_size()        return reduced_loss.item()    def _run_epoch(self, epoch):        b_sz = len(next(iter(self.train_data))[0])        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")        self.train_data.sampler.set_epoch(epoch)        # 在各个 epoch 入口调用 DistributedSampler 的 set_epoch 方法是很重要的，这样才能打乱每个 epoch 的样本顺序        total_loss = 0.0        num_batches = 0        for source, targets in self.train_data:             source = source.to(self.gpu_id)            targets = targets.to(self.gpu_id)            loss = self._run_batch(source, targets)            total_loss += loss            num_batches += 1                avg_loss = total_loss / num_batches        if self.gpu_id == 0:            print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Avg Loss: {avg_loss:.4f}\")    def _save_checkpoint(self, epoch):        ckp = self.model.module.state_dict()            # 由于多了一层 DDP 包装，通过 .module 获取原始参数         PATH = \"checkpoint.pt\"        torch.save(ckp, PATH)        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")    def train(self, max_epochs: int):        for epoch in range(max_epochs):            self._run_epoch(epoch)            # 各个 GPU 上都在跑一样的训练进程，这里指定 rank0 进程保存 ckpt 以免重复保存            if self.gpu_id == 0 and epoch % self.save_every == 0:                self._save_checkpoint(epoch)def prepare_dataset():    transform = transforms.Compose([        transforms.ToTensor(),        transforms.Normalize((0.1307,), (0.3081,))    ])    train_data = datasets.MNIST(        root = './mnist',        train=True,       # 设置True为训练数据，False为测试数据        transform = transform,        # download=True  # 设置True后就自动下载，下载完成后改为False即可    )          train_set = MyDataset(train_data)        test_data = datasets.MNIST(        root = './mnist',        train=False,       # 设置True为训练数据，False为测试数据        transform = transform,    )          test_set = MyDataset(test_data)          return train_set, test_set      def load_train_objs():    train_set, test_set = prepare_dataset()  # load your dataset    model = ConvNet()  # load your model        optimizer = optim.Adam(model.parameters(), lr=1e-3)    return train_set, test_set, model, optimizerdef prepare_dataloader(dataset: Dataset, batch_size: int):    return DataLoader(        dataset,        batch_size=batch_size,        pin_memory=True,        shuffle=False,                      # 设置了新的 sampler，参数 shuffle 要设置为 False         sampler=DistributedSampler(dataset) # 这个 sampler 自动将数据分块后送个各个 GPU，它能避免数据重叠    )def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):    # 初始化进程池, 仅是单个进程 gpu rank 的初始化    ddp_setup(rank, world_size)    # 进行训练    train_set, test_set, model, optimizer = load_train_objs()    print(f\"Train dataset size: {len(train_set)}\")    train_data = prepare_dataloader(train_set, batch_size)    trainer = Trainer(model, train_data, optimizer, rank, save_every)    trainer.train(total_epochs)    # 销毁进程池    destroy_process_group()def arg_parser():    parser = argparse.ArgumentParser(description='MNIST distributed training job')    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch size for training\")    parser.add_argument('--save_every', type=int, default=1, help='How often to save a snapshot')    return parser.parse_args()r\"\"\"README执行命令: CUDA_VISIBLE_DEVICES=0,1 python mnist_ddp.py # 用 2 卡训练注意训练数据是60K条, 训练时输出:[GPU0] Epoch 0 | Batchsize: 512 | Steps: 59[GPU1] Epoch 0 | Batchsize: 512 | Steps: 59512 * 59 = 30208 ~= 30K排除掉有些 batch_size 不足的情况, 59个 batch 就是 30K, 两个 gpu 平分了数据\"\"\"if __name__ == \"__main__\":    args = arg_parser()    print(f\"Training arguments: {args}\")        world_size = torch.cuda.device_count()    print(f\"Using {world_size} GPUs for training\")        # 利用 mp.spawn，在整个 distribution group 的 nprocs 个 GPU 上生成进程来执行 fn 方法，并能设置要传入 fn 的参数 args    # 注意不需要 fn 的 rank 参数，它由 mp.spawn 自动分配    mp.spawn(        fn=main,         args=(world_size, args.save_every, args.epochs, args.batch_size),         nprocs=world_size    )\n\n启动代码\nCUDA_VISIBLE_DEVICES=0,1 python mnist_ddp.py\n\ntorchrun 完整代码import osimport torchimport torch.nn as nnimport torch.optim as optimimport matplotlib.pyplot as pltimport torch.nn.functional as Ffrom torch.utils.data import Dataset, DataLoaderfrom torchvision import datasets, transformsfrom time import timeimport argparse# 对 python 多进程的一个 pytorch 包装import torch.multiprocessing as mp# 用于收集一些用于汇总的数据import torch.distributed as dist# 这个 sampler 可以把采样的数据分散到各个 CPU 上                                      from torch.utils.data.distributed import DistributedSampler     # 实现分布式数据并行的核心类        from torch.nn.parallel import DistributedDataParallel as DDP         # DDP 在每个 GPU 上运行一个进程，其中都有一套完全相同的 Trainer 副本（包括model和optimizer）# 各个进程之间通过一个进程池进行通信，这两个方法来初始化和销毁进程池from torch.distributed import init_process_group, destroy_process_group def ddp_setup():    \"\"\"    setup the distribution process group    Args:        rank: Unique identifier of each process        world_size: Total number of processes    \"\"\"    # 用torchrun 后台自动设置的环境变量    init_process_group(backend=\"nccl\")    torch.cuda.set_device(int(os.environ['LOCAL_RANK']))    class ConvNet(nn.Module):    def __init__(self):        super(ConvNet, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(1, 32, 3, 1),            nn.ReLU(),            nn.Conv2d(32, 64, 3, 1),            nn.ReLU(),            nn.MaxPool2d(2),            nn.Dropout(0.25)        )        self.classifier = nn.Sequential(            nn.Linear(9216, 128),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(128, 10)        )    def forward(self, x):        x = self.features(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        output = F.log_softmax(x, dim=1)        return output    # 自定义Datasetclass MyDataset(Dataset):    def __init__(self, data):        self.data = data    def __len__(self):        return len(self.data)    def __getitem__(self, idx):        image, label = self.data[idx]        return image, labelclass Trainer:    def __init__(        self,        model: torch.nn.Module,        train_data: DataLoader,        optimizer: torch.optim.Optimizer,        save_every: int,    ) -&gt; None:        self.gpu_id = int(os.environ['LOCAL_RANK']) # gpu_id 由 torchrun 自动设置        self.model = model.to(self.gpu_id)        self.train_data = train_data        self.optimizer = optimizer        self.save_every = save_every                    # 指定保存 ckpt 的周期        self.model = DDP(model, device_ids=[self.gpu_id])    # model 要用 DDP 包装一下    def _run_batch(self, source, targets):        self.optimizer.zero_grad()        output = self.model(source)        loss = F.cross_entropy(output, targets)        loss.backward()        self.optimizer.step()        # 分布式同步 loss        reduced_loss = loss.detach()        dist.all_reduce(reduced_loss, op=dist.ReduceOp.SUM)        reduced_loss /= dist.get_world_size()        return reduced_loss.item()    def _run_epoch(self, epoch):        b_sz = len(next(iter(self.train_data))[0])        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")        self.train_data.sampler.set_epoch(epoch)        total_loss = 0.0        num_batches = 0        for source, targets in self.train_data:             source = source.to(self.gpu_id)            targets = targets.to(self.gpu_id)            loss = self._run_batch(source, targets)            total_loss += loss            num_batches += 1                avg_loss = total_loss / num_batches        if self.gpu_id == 0:            print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Avg Loss: {avg_loss:.4f}\")    def _save_checkpoint(self, epoch):        ckp = self.model.module.state_dict()            # 由于多了一层 DDP 包装，通过 .module 获取原始参数         PATH = \"checkpoint.pt\"        torch.save(ckp, PATH)        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")    def train(self, max_epochs: int):        for epoch in range(max_epochs):            self._run_epoch(epoch)            # 各个 GPU 上都在跑一样的训练进程，这里指定 rank0 进程保存 ckpt 以免重复保存            if self.gpu_id == 0 and epoch % self.save_every == 0:                self._save_checkpoint(epoch)def prepare_dataset():    transform = transforms.Compose([        transforms.ToTensor(),        transforms.Normalize((0.1307,), (0.3081,))    ])    train_data = datasets.MNIST(        root = './mnist',        train=True,       # 设置True为训练数据，False为测试数据        transform = transform,        # download=True  # 设置True后就自动下载，下载完成后改为False即可    )          train_set = MyDataset(train_data)        test_data = datasets.MNIST(        root = './mnist',        train=False,       # 设置True为训练数据，False为测试数据        transform = transform,    )          test_set = MyDataset(test_data)          return train_set, test_set      def load_train_objs():    train_set, test_set = prepare_dataset()  # load your dataset    model = ConvNet()  # load your model        optimizer = optim.Adam(model.parameters(), lr=1e-3)    return train_set, test_set, model, optimizerdef prepare_dataloader(dataset: Dataset, batch_size: int):    return DataLoader(        dataset,        batch_size=batch_size,        pin_memory=True,        shuffle=False,                      # 设置了新的 sampler，参数 shuffle 要设置为 False         sampler=DistributedSampler(dataset) # 这个 sampler 自动将数据分块后送个各个 GPU，它能避免数据重叠    )def main(save_every: int, total_epochs: int, batch_size: int):    # 初始化进程池, 仅是单个进程 gpu rank 的初始化    ddp_setup()    # 进行训练    train_set, test_set, model, optimizer = load_train_objs()    print(f\"Train dataset size: {len(train_set)}\")    train_data = prepare_dataloader(train_set, batch_size)    trainer = Trainer(model, train_data, optimizer, save_every)    trainer.train(total_epochs)    # 销毁进程池    destroy_process_group()def arg_parser():    parser = argparse.ArgumentParser(description='MNIST distributed training job')    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch size for training\")    parser.add_argument('--save_every', type=int, default=1, help='How often to save a snapshot')    return parser.parse_args()r\"\"\"README执行命令: CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 mnist_ddp_torchrun.py\"\"\"if __name__ == \"__main__\":    args = arg_parser()    print(f\"Training arguments: {args}\")        world_size = torch.cuda.device_count()    print(f\"Using {world_size} GPUs for training\")    main(        args.save_every,         args.epochs,         args.batch_size    )       \n\n启动代码\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 mnist_ddp_torchrun.py\n\n\n\n","categories":["分布式"],"tags":["分布式","MNIST"]},{"title":"FSDP - Fully Sharded Data Parallel","url":"/2025/05/09/torch-distributed-series/FSDP/","content":"FSDP 是 DDP 的一种改进，它晚于 deepspeed 出现，它其实就是 zero-3: 把权重、梯度、优化器全部 sharding 存储。理解起来不难，结合下 NCCL 通信就可以知道他干什么和怎么实现的。记住，FSDP的两个关键点，sharding 和 data parallel。\nsharding\n在深度学习中，shard（分片）指的是把一个整体的数据结构或模型参数拆成多个部分，分别分布到不同设备或进程中处理或存储。通俗点，Sharding 就是“切块+分发”。\n\nSharding 的思想就是万物都可以分裂开，只要用的时候再复原，而且用完继续分裂开。这里的分裂不是常规意义上的分裂，更类似与分布式存储，每个节点都存储部分内容。FSDP 的前任是 DDP，DDP 的前提条件是每个 gpu 上都有一个模型副本，但随着模型不断变大，模型很难在单张 gpu 上部署，而且所有 gpu 都保存了大量重复的参数，也不够有效。\n\n模型并行是一个解决方法:\n\n将整个模型按层划分为多个连续的阶段(stage)，每个阶段由一个设备负责计算。\n在每个训练迭代开始时，第一个设备获取一个 batch 的输入数据，并执行前向计算。\n第一个设备将计算出的中间激活值(activations)传递给第二个阶段的设备。\n第二个设备接收到激活值后，基于这个输入继续执行前向计算，并将结果传递给下一个阶段，如此类推。\n直到最后一个阶段完成前向计算，得到最终的输出。\n基于输出，计算损失函数，并执行反向传播。\n每个阶段在反向传播时，计算本阶段所需的梯度，并将上游梯度传递给前一个阶段。\n所有阶段计算完成后，将各自的梯度汇总，更新对应的模型参数。\n将更新后的模型参数分发到对应的设备，准备进行下一个 batch 的训练迭代。\n\n但他还是以 layer 层面切分模型的，现在如果有超大的 layer，必然存在放不了一个超大的 layer 或者剩下的空间不足以放下一个 layer。\n虽然模型并行和 sharding 都是只在多 gpu 上保留一份模型参数，但 sharding 直接分裂开 tensor，粒度更细，使得其更灵活。\nShard parameters, gradients, and optimizer states across all data-parallel processes\n\nP: Parameters\nG: Gradients\nOS: Optimizer states\n\n这是 deepspeed 介绍他们三种模式的图片\n随着我们从 Optimizer 到 Gradients 到 Parameters 不断 sharding，每个 gpu 上的显存占用直线降低，而通讯压力仅有极小的成本上升。\nsharding 的重要基础就是 NCCL 的通信机制，例如以下的通信方式：\n\nall-gather,\nall-reduce,\nbroadcast,\nreduce,\nreduce-scatter\nas well as point-to-point send and receive\n\n\nThe NVIDIA Collective Communication Library (NCCL) implements multi-GPU and multi-node communication primitives optimized for NVIDIA GPUs and Networking. \n\n在 DDP 中我们就用了 all-reduce 的概念，即每个 gpu 上都计算了分配给它的 mini batch 的梯度，为了让所有卡上都维持相同的模型，or梯度更新都一致，我们用 all-reduce 让所有 gpu 上的梯度都获得了所有 gpu 上的梯度。FSDP 在此基础上，让 FWD 和 BWD 过程都利用 all-reduce 实现了参数随用随 gather，用完即丢的效果。\n\nsharding 示例\n这里以一个 Linear 层为例，现在共有 4 个节点，我们把 linear 层平均的分散到每个 gpu 上，每个 gpu 只有一部分，不足以直接计算，现在 FWD 和 BWD 时，每个节点以 all-gather 的形式将所有节点上的 shard 收集为完整的 linear 层，这样每个节点都可以基于它自己的数据计算出输出，现在这个输出可以转到下一个层的计算，也是同样的 all-gather，而已经用的层，后续不再需要，因此直接释放，还是只保留原本的 shard。\nFSDP 并不会对数据进行切分，因为它还是属于 data parallel 的范畴，即每个节点上运行部分数据，一个节点上的数据就交给这个节点计算，因此数据不会切分。\nFSDP 本质上仍是一种数据并行（DP）策略，目标是解决单卡放不下完整模型的问题，同时保留传统 DDP 的易用性与训练逻辑。不同于模型并行（MP）或流水线并行（PP）那种“切分计算”的思想，FSDP 并不切算子，而是切分参数存储（shard weight storage），从而在所有 GPU 上共同维护一个被 shard 化的模型。\n!!! 注意，FSDP 不涉及张量并行或者模型并行的概念，它只用 sharding 来存储参数，用 gather 来随时收集分散的参数，此外和正常计算一样，计算方式上没有任何改变。\ntoy model以下是一个 手动 sharding 和 gather shard 的 toy model。\nimport osimport torchimport torch.distributed as distimport torch.nn as nnfrom torch.nn.parallel import DistributedDataParallel as DDPimport torch.multiprocessing as mpdef setup(rank, world_size):\tos.environ['MASTER_ADDR'] = 'localhost'\tos.environ['MASTER_PORT'] = '12355'\tdist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\ttorch.cuda.set_device(rank)def cleanup():\tdist.destroy_process_group()def all_gather_tensor(tensor, world_size):\tgathered = [torch.zeros_like(tensor) for _ in range(world_size)]\tdist.all_gather(gathered, tensor)\tfull_tensor = torch.cat(gathered, dim=0)\treturn full_tensorclass LinearShard(nn.Module):\tdef __init__(self, in_features, out_features, rank, world_size):\t\tsuper().__init__()\t\tself.rank = rank\t\tself.world_size = world_size\t\t# 手动分 shard：每张卡只保存 out_features 的一部分\t\tself.out_per_gpu = out_features // world_size\t\tself.weight_shard = nn.Parameter(\t\t\ttorch.randn(self.out_per_gpu, in_features, device=f'cuda:{rank}')\t\t)\t\tself.bias_shard = nn.Parameter(\t\t\ttorch.randn(self.out_per_gpu, device=f'cuda:{rank}')\t\t)\tdef forward(self, x):\t\t# all_gather 所有 rank 的 weight 和 bias\t\tfull_weight = all_gather_tensor(self.weight_shard, self.world_size)\t\tfull_bias = all_gather_tensor(self.bias_shard, self.world_size)\t\tprint(f\"Rank {self.rank} gathered weight shape: {full_weight.shape}, device: {full_weight.device}\")\t\tprint(f\"Rank {self.rank} gathered bias shape: {full_bias.shape}, device: {full_bias.device}\")\t\t# 所有卡执行完整 linear（广播来的 full_weight）\t\treturn torch.nn.functional.linear(x, full_weight, full_bias)def demo(rank, world_size):\tsetup(rank, world_size)\t# 模拟 Linear(4, 8)，8维在两卡间 shard，每卡负责 4 个输出\tmodel = LinearShard(in_features=4, out_features=8, rank=rank, world_size=world_size)\tmodel = DDP(model, device_ids=[rank])\t# forward\tinput = torch.randn(2, 4, device=f'cuda:{rank}')\toutput = model(input)\tprint(\"Output shape:\", output.shape)\tcleanup()if __name__ == \"__main__\":\tmp.spawn(demo, args=(2,), nprocs=2)\n\n# CUDA_VISIBLE_DEVICES=1,2 python demo.pyRank 1 gathered weight shape: torch.Size([8, 4]), device: cuda:1Rank 1 gathered bias shape: torch.Size([8]), device: cuda:1Rank 0 gathered weight shape: torch.Size([8, 4]), device: cuda:0Rank 0 gathered bias shape: torch.Size([8]), device: cuda:0Output shape: torch.Size([2, 8])Output shape: torch.Size([2, 8])\n\nFSDP\n以图为例，重新分散模型：\n\nunit 0: [layer 0, layer 3]\nunit 1: [layer 1, layer 2]\nunit 2: [layer 4, layer 5]\n\n这里的 unit 即节点， sharding 本身没有任何顺序或者既定的比例，随便分散即可。每个节点执行 FWD 和 BWD 时都是先 gather 全部参数，计算，在释放非自己的 shard， BWD 同理，最后同步 gradient 。\n\n再从计算和通信上看，在进行完 AG0 (all-gather layer 0)时可以同时 FWD0 和 AG1，此时通信和计算就会overlap，但不影响双方。\n\n在 BWD 时也是 all-gather 完了进行 BWD 计算，这里直接进行了 reduce-scatter，因为 FSDP 还是类似 DDP 一样，要维持相同的模型或者梯度更新，这一步直接当前梯度计算完，直接同步。\n\nFSDP 和 DDP 的区别还是很明显的，DDP 要在每个节点上保留一个完整的模型副本，模型比较大时就非常吃亏，因为大部分显存都被模型占着。而 FSDP 让所有节点共同存储一个模型，每个模型平摊代价，使得每个节点只需要承担 sub-model 级别的代价，使得可以跑更大的 batch 。\n# 大概用 FSDP 包装下模型就可以了，当然也要设置类似 DDP 的参数import torchfrom torch.distributed._fsdp import FullyShardedDataParallel as FSDPtorch.cuda.set_device(device_id)model = Net()sharded_model = FSDP(model)optim = torch.optim.Adam(sharded_model.parameters(), lr=0.0001)sharded_model(input).sum().backward()optim.step()\n\nMNIST FSDP# Based on: https://github.com/pytorch/examples/blob/master/mnist/main.pyimport osimport argparseimport functoolsimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transformsfrom torch.optim.lr_scheduler import StepLRimport torch.distributed as distimport torch.multiprocessing as mpfrom torch.utils.data.distributed import DistributedSamplerfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDPfrom torch.distributed.fsdp.wrap import (    size_based_auto_wrap_policy,    enable_wrap,    wrap,)# Distributed training setupdef setup(rank, world_size):    os.environ['MASTER_ADDR'] = 'localhost'    os.environ['MASTER_PORT'] = '12355'    # initialize the process group    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)def cleanup():    dist.destroy_process_group()class Net(nn.Module):        def __init__(self):        super(Net, self).__init__()        self.conv1 = nn.Conv2d(1, 32, 3, 1)        self.conv2 = nn.Conv2d(32, 64, 3, 1)        self.dropout1 = nn.Dropout(0.25)        self.dropout2 = nn.Dropout(0.5)        self.fc1 = nn.Linear(9216, 128)        self.fc2 = nn.Linear(128, 10)    def forward(self, x):        x = self.conv1(x)        x = F.relu(x)        x = self.conv2(x)        x = F.relu(x)        x = F.max_pool2d(x, 2)        x = self.dropout1(x)        x = torch.flatten(x, 1)        x = self.fc1(x)        x = F.relu(x)        x = self.dropout2(x)        x = self.fc2(x)        output = F.log_softmax(x, dim=1)        return output    def train(model, rank, train_loader, optimizer, epoch, sampler=None):    model.train()    ddp_loss = torch.zeros(2).to(rank)    if sampler:        sampler.set_epoch(epoch)    for batch_idx, (data, target) in enumerate(train_loader):        data, target = data.to(rank), target.to(rank)        optimizer.zero_grad()        output = model(data)        loss = F.nll_loss(output, target, reduction='sum')        loss.backward()        optimizer.step()        ddp_loss[0] += loss.item()        ddp_loss[1] += len(data)    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)    if rank == 0:        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))        def test(model, rank, test_loader):    model.eval()    correct = 0    ddp_loss = torch.zeros(3).to(rank)    with torch.no_grad():        for data, target in test_loader:            data, target = data.to(rank), target.to(rank)            output = model(data)            ddp_loss[0] += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability            ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()            ddp_loss[2] += len(data)    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)    if rank == 0:        test_loss = ddp_loss[0] / ddp_loss[2]        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(            test_loss, int(ddp_loss[1]), int(ddp_loss[2]),            100. * ddp_loss[1] / ddp_loss[2]))        def fsdp_main(rank, world_size, args):    setup(rank, world_size)    transform=transforms.Compose([        transforms.ToTensor(),        transforms.Normalize((0.1307,), (0.3081,))    ])    dataset1 = datasets.MNIST('./mnist', train=True, download=True,                        transform=transform)    dataset2 = datasets.MNIST('./mnist', train=False,                        transform=transform)    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}    cuda_kwargs = {'num_workers': 2,                    'pin_memory': True,                    'shuffle': False}    train_kwargs.update(cuda_kwargs)    test_kwargs.update(cuda_kwargs)    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)    \t# !!! 只有参数数量超过某个阈值的模块，才会被 FSDP 自动包裹（即切分）    my_auto_wrap_policy = functools.partial(        size_based_auto_wrap_policy, min_num_params=100    )    torch.cuda.set_device(rank)    model = Net().to(rank)    model = FSDP(model)    optimizer = optim.Adam(model.parameters(), lr=args.lr)    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)        for epoch in range(1, args.epochs + 1):        train(model, rank, train_loader, optimizer, epoch, sampler=sampler1)        test(model, rank, test_loader)        scheduler.step()    if args.save_model:        # use a barrier to make sure training is done on all ranks        dist.barrier()        states = model.state_dict()        if rank == 0:            torch.save(states, \"mnist_cnn.pt\")    cleanup()    if __name__ == '__main__':    # Training settings    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')    parser.add_argument('--batch-size', type=int, default=64, metavar='N',                        help='input batch size for training (default: 64)')    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',                        help='input batch size for testing (default: 1000)')    parser.add_argument('--epochs', type=int, default=10, metavar='N',                        help='number of epochs to train (default: 14)')    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',                        help='learning rate (default: 1.0)')    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',                        help='Learning rate step gamma (default: 0.7)')    parser.add_argument('--no-cuda', action='store_true', default=False,                        help='disables CUDA training')    parser.add_argument('--seed', type=int, default=1, metavar='S',                        help='random seed (default: 1)')    parser.add_argument('--save-model', action='store_true', default=False,                        help='For Saving the current Model')    args = parser.parse_args()    torch.manual_seed(args.seed)    WORLD_SIZE = torch.cuda.device_count()    mp.spawn(fsdp_main,        args=(WORLD_SIZE, args),        nprocs=WORLD_SIZE,        join=True)\n\n参考\nhttps://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html\nhttps://github.com/chunhuizhang/pytorch_distribute_tutorials/blob/main/tutorials/FSDP.ipynb\n\n","categories":["分布式"],"tags":["分布式"]},{"title":"CLIP - Contrastive Language-Image Pre-training","url":"/2025/05/11/Multi-modal-series/CLIP/","content":"Learning Transferable Visual Models From Natural Language Supervision paper：https://arxiv.org/abs/2103.00020\nOpenAI 推出了 CLIP，即 对比式语言-图像预训练（Contrastive Language-Image Pre-training）。简而言之，该模型学习的是整句话与其描述的图像之间的关系；也就是说，当模型训练完成后，给定一段输入文本，它能够检索出与该文本最相关的图像。这里的一个关键点是，它的训练是基于完整句子，而不是单一的类别（例如“汽车”、“狗”等）。其核心直觉在于，使用完整句子进行训练可以让模型学到更多信息，从而在图像与文本之间发现潜在的模式。\n\n\nCLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples.\n\n说直白点，就是让 image embedding 和 （与 image 配对的text）的 text embedding 相似度高，与其他不匹配的 text 的 text embedding 相似度低。我们训练对象是 image encoder 和 text encoder，更准确点，我们会用预训练模型作为 encoder，然后在其后面加入 projection head 来将源模态的 embedding 转为为 unified embedding (统一的模态)，encoder 预训练，仅微调，projection head 从头开始训练，这也解决了 embedding 维度不一样的问题。\n\n\nAt test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes.\n\n在实际 zero-shot 分类时，取各个类别转换为 text，经过 text encoder 转换为各个类别的 text embedding。同时，需要分类的 image 经过 image encoder 转换为 image embedding，计算 image embedding 和 text embedding 之前的相似度，softmax 后最高者对应的类别就是分类结果。\nimport osimport cv2import numpy as npimport pandas as pdfrom tqdm import tqdmimport albumentations as Aimport timmimport torchfrom torch import nnfrom transformers import AutoModel, AutoTokenizer, DistilBertConfigimport osos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n加载 image - captionFlicker-8k 数据地址：https://www.kaggle.com/datasets/adityajn105/flickr8k\nFlicker-8k 将配对数据保存为 txt，此处读取为 dataframe\nimage_path = \"./Flicker-8k/Images\"captions_path = \"./Flicker-8k/captions.txt\"train_ratio = 0.5batch_size = 32num_workers = 4size = 224dataframe = pd.read_csv(captions_path)# print(dataframe[:10].to_markdown())print('num of data:', len(dataframe))dataframe = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)train_size = int(len(dataframe) * train_ratio)train_dataframe = dataframe[:train_size]test_dataframe = dataframe[train_size:]\n\n\n\n\n\nimage\ncaption\n\n\n\n0\n1000268201_693b08cb0e.jpg\nA child in a pink dress is climbing up a set of stairs in an entry way .\n\n\n1\n1000268201_693b08cb0e.jpg\nA girl going into a wooden building .\n\n\n2\n1000268201_693b08cb0e.jpg\nA little girl climbing into a wooden playhouse .\n\n\n3\n1000268201_693b08cb0e.jpg\nA little girl climbing the stairs to her playhouse .\n\n\n4\n1000268201_693b08cb0e.jpg\nA little girl in a pink dress going into a wooden cabin .\n\n\n5\n1001773457_577c3a7d70.jpg\nA black dog and a spotted dog are fighting\n\n\n6\n1001773457_577c3a7d70.jpg\nA black dog and a tri-colored dog playing with each other on the road .\n\n\n7\n1001773457_577c3a7d70.jpg\nA black dog and a white dog with brown spots are staring at each other in the street .\n\n\n8\n1001773457_577c3a7d70.jpg\nTwo dogs of different breeds looking at each other on the road .\n\n\n9\n1001773457_577c3a7d70.jpg\nTwo dogs on pavement moving toward each other .\n\n\n每个图片都有 5 个 caption 来描述其内容。\ntokenizer = AutoTokenizer.from_pretrained(\"/home/guest/others/data_collection/distilbert-base-uncased\")print(tokenizer(\"hello\")) # {'input_ids': [101, 7592, 102], 'attention_mask': [1, 1, 1]}\n\n数据集设置很简单，相比于有监督学习的 x 和 y，对比学习是正样本和负样本。以上每一行的 image 和 caption 就是正样本，而负样本是同一 batch 的其他 caption。\n虽然严格意义上，我们可能遇到同一 batch 里的两个 image 是一样，但其对应 caption 是语义上类似，但不严格相同的。这种情况下还是会被视为负样本，但考虑到这样的情况很少出现，即很小概率正样本被当作负样本处理，所以整体训练上是没毛病的。\ndef get_transforms(mode=\"train\"):\tif mode == \"train\":\t\treturn A.Compose([\t\t\tA.Resize(size, size),\t\t\tA.Normalize(max_pixel_value=255.0),\t\t])\telse:\t\treturn A.Compose([\t\t\tA.Resize(size, size),\t\t\tA.Normalize(max_pixel_value=255.0),\t\t])class CLIPDataset(torch.utils.data.Dataset):\t\tdef __init__(self, image_filenames, captions, tokenizer, transforms):\t\t\"\"\"\t\timage_filenames and cpations must have the same length; so, if there are\t\tmultiple captions for each image, the image_filenames must have repetitive\t\tfile names \t\t\"\"\"\t\tself.image_filenames = image_filenames\t\tself.captions = list(captions)\t\tself.encoded_captions = tokenizer(\t\t\tlist(captions), padding=True, truncation=True, max_length=200\t\t)\t\tself.transforms = transforms\t# 每个数据是一个 image 和一个 caption ，准确点是一个 tensor 化的 image 和一个 tokenized 的 caption\tdef __getitem__(self, idx):\t\titem = {\t\t\tkey: torch.tensor(values[idx])\t\t\tfor key, values in self.encoded_captions.items()\t\t}\t\timage = cv2.imread(f\"{image_path}/{self.image_filenames[idx]}\")\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\t\timage = self.transforms(image=image)['image']\t\titem['image'] = torch.tensor(image).permute(2, 0, 1).float()\t\titem['caption'] = self.captions[idx]\t\treturn item\tdef __len__(self):\t\treturn len(self.captions)\tdef build_loaders(dataframe, tokenizer, mode, batch_size=32, num_workers=4):\ttransforms = get_transforms(mode=mode)\tdataset = CLIPDataset(\t\tdataframe[\"image\"].values,\t\tdataframe[\"caption\"].values,\t\ttokenizer=tokenizer,\t\ttransforms=transforms,\t)\tdataloader = torch.utils.data.DataLoader(\t\tdataset,\t\tbatch_size=batch_size,\t\tnum_workers=num_workers,\t\tshuffle=True if mode == \"train\" else False,\t)\treturn dataloader\ttrain_loader = build_loaders(train_dataframe, tokenizer, mode=\"train\", batch_size=batch_size)valid_loader = build_loaders(train_dataframe, tokenizer, mode=\"valid\", batch_size=batch_size)# print(train_loader.dataset.__getitem__(0))# print(next(iter(train_loader)))\n\n\na_batch = next(iter(train_loader))for k, v in a_batch.items():\tprint(f\"key name: {k}, value data type: {type(v)}, value shape: {v.shape if isinstance(v, torch.Tensor) else len(v)}\")\n\n\nkey name: input_ids, value data type: &lt;class 'torch.Tensor'&gt;, value shape: torch.Size([32, 42])key name: attention_mask, value data type: &lt;class 'torch.Tensor'&gt;, value shape: torch.Size([32, 42])key name: image, value data type: &lt;class 'torch.Tensor'&gt;, value shape: torch.Size([32, 3, 224, 224])key name: caption, value data type: &lt;class 'list'&gt;, value shape: 32\n\n模型\nimage encoder 用 resnet 或者 vit，forward不变，返回图片 embedding\ntext encoder 用 bert 变体 distilbert，foward返回 CLS token 位置的 embedding\n两个 projection head 分别把 image embedding 和 text embedding 转换为统一的 embedding\n\nclass ImageEncoder(nn.Module):\t\"\"\"\tEncode images to a fixed size vector\t\"\"\"\tdef __init__(\t\tself, model_name='resnet50', pretrained=True, trainable=True, pretrained_cfg_overlay=None\t):\t\tsuper().__init__()\t\tself.model = timm.create_model(\t\t\tmodel_name, pretrained, num_classes=0, global_pool=\"avg\", pretrained_cfg_overlay=pretrained_cfg_overlay\t\t)\t\tfor p in self.model.parameters():\t\t\tp.requires_grad = trainable\tdef forward(self, x):\t\treturn self.model(x)class TextEncoder(nn.Module):\t\tdef __init__(self, model_name=\"distilbert-base-uncased\", pretrained=True, trainable=True):\t\tsuper().__init__()\t\tif pretrained:\t\t\tself.model = AutoModel.from_pretrained(model_name)\t\telse:\t\t\tself.model = AutoModel(config=DistilBertConfig())\t\t\t\t\tfor p in self.model.parameters():\t\t\tp.requires_grad = trainable\t\t# we are using the CLS token hidden representation as the sentence's embedding\t\tself.target_token_idx = 0\tdef forward(self, input_ids, attention_mask):\t\toutput = self.model(input_ids=input_ids, attention_mask=attention_mask)\t\t# (batch_size, sequence_length, hidden_size)\t\tlast_hidden_state = output.last_hidden_state\t\t# use CLS token hidden representation\t\t# return (batch_size, hidden_size)\t\treturn last_hidden_state[:, self.target_token_idx, :]class ProjectionHead(nn.Module):\tdef __init__(\t\tself,\t\tembedding_dim,\t\tprojection_dim=256,\t\tdropout=0.1\t):\t\tsuper().__init__()\t\tself.projection = nn.Linear(embedding_dim, projection_dim)\t\tself.gelu = nn.GELU()\t\tself.fc = nn.Linear(projection_dim, projection_dim)\t\tself.dropout = nn.Dropout(dropout)\t\tself.layer_norm = nn.LayerNorm(projection_dim)\t\tdef forward(self, x):\t\tprojected = self.projection(x)\t\tx = self.gelu(projected)\t\tx = self.fc(x)\t\tx = self.dropout(x)\t\tx = x + projected\t\tx = self.layer_norm(x)\t\treturn xclass CLIPModel(nn.Module):\t\tdef __init__(\t\tself,\t\timage_encoder,\t\ttext_encoder,\t\timage_embedding=2048,\t\ttext_embedding=768,\t\ttemperature=1.0,\t):\t\tsuper().__init__()\t\tself.image_encoder = image_encoder\t\tself.text_encoder = text_encoder\t\tself.image_projection = ProjectionHead(embedding_dim=image_embedding)\t\tself.text_projection = ProjectionHead(embedding_dim=text_embedding)\t\tself.temperature = temperature\tdef forward(self, images:torch.tensor, input_ids:torch.tensor, attention_masks:torch.tensor):\t\t# Getting Image and Text Features\t\timage_features = self.image_encoder(images)\t\ttext_features = self.text_encoder(\t\t\tinput_ids=input_ids, attention_mask=attention_masks\t\t)\t\t# Getting Image and Text Embeddings (with same dimension)\t\timage_embeddings = self.image_projection(image_features)\t\ttext_embeddings = self.text_projection(text_features)\t\t# Calculating the Loss\t\tlogits = (text_embeddings @ image_embeddings.T) / self.temperature\t\t\t\treturn logits, image_embeddings, text_embeddings\t\n\n\nimage_encoder = ImageEncoder(\tmodel_name='resnet50',\tpretrained_cfg_overlay=dict(file='/home/guest/others/xp/clip/timm-resnet50/pytorch_model.bin'),)text_encoder = TextEncoder(\tmodel_name=\"/home/guest/others/data_collection/distilbert-base-uncased\")model = CLIPModel(\timage_encoder=image_encoder,\ttext_encoder=text_encoder,\timage_embedding=2048,\ttext_embedding=768,\ttemperature=1.0,)\n\n\n\n训练函数loss 有两种写法\n\nCLIP 原文用的就是对角矩阵为 label 的情况，只要分别在 dim0 和 dim1 与 arange(n) 序列作交叉熵计算就行。原文就是这样实现的，以下是论文中的伪代码。\n\n\n\nGiven a batch of N (image, text) pairs, CLIP is trained to predict which of the N × N possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a  with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. Finally all WordNet synsets not already in the query list are added.  multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N^2 − N incorrect pairings.\n\nbatch_size = image_embeddings.shape[0]labels = torch.arange(batch_size).to(device)texts_loss = self.criterion(logits, labels)images_loss = self.criterion(logits.T, labels)\tloss = (images_loss + texts_loss) / 2.0 \n\n\n还有一种是以 image-image 和 text-text 相似度为 ground truth，对角线肯定相似度很高，另外部分非对角线数据也会根据其实际 image-image 和 text-text 的相似度。这种方式我感觉可以弥补正样本被视为负样本的情况。\n\nimages_similarity = image_embeddings @ image_embeddings.Ttexts_similarity = text_embeddings @ text_embeddings.Ttargets = F.softmax(\t(images_similarity + texts_similarity) / 2 * self.temperature, dim=-1)texts_loss = nn.CrossEntropyLoss()(logits, targets)images_loss = nn.CrossEntropyLoss()(logits.T, targets.T)loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n\n\ndef train_batch(model, optimizer, batch, device):\toptimizer.zero_grad()\tcriterion = nn.CrossEntropyLoss()\timages = batch[\"image\"].to(device)\tinput_ids = batch[\"input_ids\"].to(device)\tattention_masks = batch[\"attention_mask\"].to(device)\t\t# image_embeddings and text_embeddings have same batch size\t# logits is a matrix of shape (batch_size, batch_size)\t# logits[i][j] is the similarity score between image i and text j\tlogits, image_embeddings, text_embeddings = model(\t\timages=images,\t\tinput_ids=input_ids,\t\tattention_masks=attention_masks\t)\tbatch_size = image_embeddings.shape[0]\tlabels = torch.arange(batch_size).to(device)\ttexts_loss = criterion(logits, labels)\timages_loss = criterion(logits.T, labels)\tloss = (images_loss + texts_loss) / 2.0 \tloss.backward()\toptimizer.step()\treturn loss.item()def train_epoch(model, optimizer, dataloader, device):\tmodel.train()\tlosses = []\tfor batch in tqdm(dataloader):\t\t\t\tbatch_loss = train_batch(model, optimizer, batch, device)\t\tlosses.append(batch_loss)\tave_loss = np.mean(losses)\treturn ave_lossdef test_batch(model, batch, device):\t\timages = batch[\"image\"].to(device)\tinput_ids = batch[\"input_ids\"].to(device)\tattention_masks = batch[\"attention_mask\"].to(device)\tlogits, image_embeddings, text_embeddings = model(\t\timages=images,\t\tinput_ids=input_ids,\t\tattention_masks=attention_masks\t)\tgt = torch.arange(logits.size(0), device=logits.device)\tpred = torch.argmax(logits, dim=1)\tcorrect = (pred == gt).sum().item()\treturn correct / logits.size(0)def test_epoch(model, dataloader, device):\tmodel.eval()\taccs = []\twith torch.no_grad():\t\tfor batch in tqdm(dataloader):\t\t\tbatch_acc = test_batch(model, batch, device)\t\t\taccs.append(batch_acc)\tave_acc = np.mean(accs)\treturn ave_acc\n\n训练流程这里考虑到 image encoder 和 text encoder 都是已经预训练的，而 projection head 是从 scatch 开始训的，所以最好设置 encoder 的学习率较低， projection head 的学习率为正常值。\nn_epochs = 20device = torch.device(\"cuda:1\")model = model.to(device)# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)optimizer = torch.optim.AdamW([\t{\"params\": model.image_encoder.parameters(), \"lr\": 1e-5},\t{\"params\": model.text_encoder.parameters(), \"lr\": 1e-5},\t{\"params\": model.image_projection.parameters(), \"lr\": 1e-3},\t{\"params\": model.text_projection.parameters(), \"lr\": 1e-3},])lr_scheduler = torch.optim.lr_scheduler.StepLR(\toptimizer, step_size=5, gamma=0.5,)for epoch in range(20):\tprint(f\"Epoch: {epoch + 1}\")\tprint(f'Learning Rate: {lr_scheduler.get_last_lr()[0]}')\t\ttrain_loss = train_epoch(model, optimizer, train_loader, device)\tprint(f\"Train Loss: {train_loss:.4f}\")\ttest_acc = test_epoch(model, valid_loader, device)\tprint(f\"Test Accuracy: {test_acc:.4f}\")\t\tlr_scheduler.step()\t# break\n\nEpoch: 1Learning Rate: 1e-05100%|██████████| 633/633 [02:00&lt;00:00,  5.27it/s]Train Loss: 2.2545100%|██████████| 633/633 [00:50&lt;00:00, 12.64it/s]Test Accuracy: 0.7001Epoch: 2Learning Rate: 1e-05100%|██████████| 633/633 [02:07&lt;00:00,  4.98it/s]Train Loss: 0.7770100%|██████████| 633/633 [01:08&lt;00:00,  9.23it/s]Test Accuracy: 0.8489Epoch: 3Learning Rate: 1e-05100%|██████████| 633/633 [02:18&lt;00:00,  4.58it/s]Train Loss: 0.4705100%|██████████| 633/633 [01:04&lt;00:00,  9.77it/s]Test Accuracy: 0.9080Epoch: 4Learning Rate: 1e-05100%|██████████| 633/633 [01:53&lt;00:00,  5.56it/s]Train Loss: 0.3176100%|██████████| 633/633 [00:50&lt;00:00, 12.51it/s]Test Accuracy: 0.9228Epoch: 5Learning Rate: 1e-05100%|██████████| 633/633 [01:57&lt;00:00,  5.40it/s]Train Loss: 0.2462100%|██████████| 633/633 [00:51&lt;00:00, 12.20it/s]Test Accuracy: 0.9343","categories":["多模态"],"tags":["多模态"]},{"title":"ViT - vision transformer","url":"/2025/05/16/Multi-modal-series/ViT/","content":"ViT (vision transformer) 这个模型的重点在于对特征的提炼能力，预训练只用简单的 softmax 作为 分类头，使得特征提取尽量优化，而不是更复杂的分类头\nBG将自注意力机制直接应用于图像时，需要每个像素与所有其他像素进行交互。这种全连接的方式导致计算复杂度随像素数量呈二次增长，因此难以扩展到实际图像尺寸。为了解决这一问题，以下多种策略在图像处理中应用 Transformer ：\n\n局部像素注意力（Local Attention）：只在邻近像素之间施加注意力，从而构建局部的多头自注意力模块，可在一定程度上替代卷积操作。\n全局注意力近似（Global Attention Approximation）：如 Sparse Transformer 提出了可扩展的稀疏注意力机制，使得 Transformer 能够处理大规模图像输入。\n基于图像块的注意力（Patch-wise Attention）：将输入图像划分为固定大小的图像块（如 2×2 patch），在每个块之间施加完整的自注意力机制。Cordonnier 等人的工作即采用了这种策略，并在较小图像上进行了实验。\n\nViT（Vision Transformer）本质上延续了最后一种思路，但对其进行了尺度扩展。相比于早期方法仅处理 2×2 的小块，ViT采用更大尺寸的 patch，从而能够适应中等分辨率的图像任务，实现了更强的表达能力和更广的应用范围。\nBERT因为 ViT 底层设计和 BERT 很类似，因此先讲下 BERT 的基本原理。\n\nBERT 的 input是一条文本。文本中的每个词（token）我们都通过 embedding 矩阵 把它表示成了向量的形式。\nembedding = token_embedding(将单个词转变为词向量) + position_embedding(位置编码，用于表示 token 在输入序列中的位置) + **segment_emebdding(**非必须，在 bert 中用于表示每个词属于哪个句子)。\n在 VIT 中，同样存在 token_embedding 和 postion_emebedding。\n在 Bert 中，我们同时做 2 个训练任务：\n\nNext Sentence Prediction Model（下一句预测）：input 中会包含两个句子，这两个句子有 50% 的概率是真实相连的句子，50% 的概率是随机组装在一起的句子。我们在每个 input 前面增加特殊符&lt;cls&gt;，这个位置所在的 token 将会在训练里不断学习整条文本蕴含的信息。最后它将作为 “下一句预测” 任务的输入向量，该任务是一个二分类模型，输出结果表示两个句子是否真实相连。\nMasked Language Model（遮蔽词猜测）：在 input 中，我们会以一定概率随机遮盖掉一些 token（&lt;mask&gt;)，以此来强迫模型通过 Bert 中的 attention 结构更好抽取上下文信息，然后在 “遮蔽词猜测” 任务重，准确地将被覆盖的词猜测出来。\n\n就和这次讲的 ViT 一样， BERT也只是重点放在特征的提取，同样也最多适用于分类任务。\nViT model design\n\n标准transformer接受 1D序列的 embedding （实际整体是2D）。为了针对 2D 序列的图片，要把图片扁平为 2D 的patch。将 HxWxC 的图片转换为 Nx(P^2xC) 的patch，注意是把 图片尺寸 HxW 切分成 P^2 的小块。\n\npatch embeddings: 之后为了转换为 embedding，将每个patch扁平化，并用linear层映射到embedding，类似于bert的 [cls] token，我们也需要用一个token，来作为最终的整体图片表示，这个 token 最终的标表征用于分类任务，实现起来就是定义一组可学习参数。transformers 里源码是定义 (1,1,hs) 的矩阵，FWD时会在batch维度扩展，再在第二维度（序列维度）上拼接。\n\n# initself.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))# fowardcls_tokens = self.cls_token.expand(batch_size, -1, -1)embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n\n\nposition embedding: 另外将 position embedding添加到 patch embedding中以保留位置信息。我们使用标准可学习的1D位置嵌入，因为我们没有观察到使用更高级的2D感知能position embedding的性能提高，源码里除了正常图片的处理外，还有非正常大小图片的处理，大概就是将原本的 PE 插值化到目标图片的大小。\n\n# add positional encoding to each tokenif interpolate_pos_encoding:    embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)else:    embeddings = embeddings + self.position_embeddings\n\n\ntransformer blocks: 之后整体就和正常 transformer 一样了，都是 attention 和MLP的叠加，最后有一个分类head，是由MLP在预训练时间和微调时间的单个线性层的MLP实现的。\n\n通常，我们在大型数据集上预先培训VIT，并对（较小）下游任务进行微调。为此，我们删除了预训练的预测头，并连接一个初始化为0的d×k的FFN，其中k是下游类的数量。比预训练更高的分辨率进行微分解通常是有益的。在喂食较高分辨率的图像时，我们保持patch大小相同，从而导致较大的有效序列长度。ViT可以处理任意序列长度（直至记忆约束），但是，预训练的position embedding可能不再有意义。因此，我们根据原始图像中的位置，对预训练的position embedding进行2D插值。\nViT CNN 混合模型原文在最后还提出了混合结构，作为原始图像patch的替代方法，可以先从CNN的特征图中形成输入序列。在此混合模型中，patch embeddings 应用于从CNN特征图中提取的patch，相当于替代 linear 层提取特征。\n\nAs an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.\n\n虽说 ViT 是作为替代 CNN 的方法，但用 CNN 和这个不矛盾，由于这一步只是输入预处理阶段，和主体模型没有关系。\nTransformers ViT 微调预下载数据集和模型\nexport HF_ENDPOINT=\"https://hf-mirror.com\"huggingface-cli download --repo-type dataset  --resume-download beans --local-dir ./beanshuggingface-cli download google/vit-base-patch16-224-in21k --local-dir vit-base-patch16-224-in21k\n\nloading dataset - beansfrom datasets import load_datasetimport os# load cifar10 (only small portion for demonstration purposes) parquet_path = 'beans/data'ds = load_dataset('parquet',\tdata_files={    \"train\": os.path.join(parquet_path, 'train-*.parquet'),\t\"validation\": os.path.join(parquet_path, 'validation-*.parquet'),    \"test\": os.path.join(parquet_path, 'test-*.parquet')\t})ds\n\n\nDatasetDict({    train: Dataset({        features: ['image_file_path', 'image', 'labels'],        num_rows: 1034    })    validation: Dataset({        features: ['image_file_path', 'image', 'labels'],        num_rows: 133    })    test: Dataset({        features: ['image_file_path', 'image', 'labels'],        num_rows: 128    })})\n\n每个样本包含三个特征：\n\nimage：一个 PIL 图像对象\nimage_file_path：图像文件的路径，类型为字符串，该路径对应的图像已被加载为 image\nlabels：一个 datasets.ClassLabel 特征，这里会以整数形式表示每个样本的标签\n\nex = ds['train'][400]ex\n\n\n{'image_file_path': '/home/albert/.cache/huggingface/datasets/downloads/extracted/967f0d9f61a7a8de58892c6fab6f02317c06faf3e19fba6a07b0885a9a7142c7/train/bean_rust/bean_rust_train.148.jpg', 'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500&gt;, 'labels': 1}\n\n打印下图片\nimage = ex['image']image\n\n\n​    \n打印出该样本对应的类别标签。可以使用 ClassLabel 提供的 int2str 函数来实现，该函数可以将类别的整数表示转换为对应的字符串标签\nlabels = ds['train'].features['labels']print(labels)print(labels.int2str(ex['labels']))\n\nClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'], id=None)bean_rust\n\n\nloading ViT Image Processor在训练 ViT 模型时，输入图像会经过特定的变换处理。如果对图像应用了错误的变换，模型将无法理解其所看到的内容！\n为了确保应用正确的图像变换，我们将使用与预训练模型一同保存的配置来初始化一个 ViTImageProcessor。在本例中，使用的是 google/vit-base-patch16-224-in21k 模型\n可以直接打印 ImageProcessor 来显示其 config\nfrom transformers import ViTImageProcessormodel_name_or_path = \"../DC/vit-base-patch16-224-in21k\"processor = ViTImageProcessor.from_pretrained(model_name_or_path)processor\n\n\n\n\nViTImageProcessor {  \"do_convert_rgb\": null,  \"do_normalize\": true,  \"do_rescale\": true,  \"do_resize\": true,  \"image_mean\": [    0.5,    0.5,    0.5  ],  \"image_processor_type\": \"ViTImageProcessor\",  \"image_std\": [    0.5,    0.5,    0.5  ],  \"resample\": 2,  \"rescale_factor\": 0.00392156862745098,  \"size\": {    \"height\": 224,    \"width\": 224  }}\n\n\n\n要处理一张图像，只需将其传递给图像处理器的 __call__ 函数即可。该函数会返回一个字典，其中包含 pixel_values，即图像的数值表示，可直接输入到模型中。\n默认情况下返回的是 NumPy 数组，但如果添加参数 return_tensors='pt'，则会返回 PyTorch 张量。\nret = processor(image, return_tensors='pt')print(ret)print(ret['pixel_values'].shape)\n\n{'pixel_values': tensor([[[[ 0.7882,  0.6706,  0.7098,  ..., -0.1922, -0.1294, -0.1765],          [ 0.7098,  0.6000,  0.6784,  ..., -0.2863, -0.1608, -0.1608],          [ 0.4902,  0.3882,  0.4667,  ..., -0.1922, -0.0196,  0.0275],          ...,          [ 0.3804,  0.5294,  0.4824,  ..., -0.8275, -0.8196, -0.8039],          [ 0.0902,  0.3725,  0.3804,  ..., -0.8667, -0.8431, -0.8510],          [-0.0510,  0.2784,  0.3176,  ..., -0.8588, -0.8275, -0.8353]],         [[ 0.4902,  0.3490,  0.3804,  ..., -0.6078, -0.5373, -0.5843],          [ 0.3569,  0.2000,  0.3176,  ..., -0.7255, -0.6000, -0.5922],          [ 0.0431, -0.0902,  0.0588,  ..., -0.6392, -0.4745, -0.4275],          ...,          [-0.2235, -0.0510, -0.0902,  ..., -0.9686, -0.9529, -0.9294],          [-0.5059, -0.2078, -0.1922,  ..., -0.9922, -0.9922, -1.0000],          [-0.6471, -0.2941, -0.2471,  ..., -0.9843, -0.9765, -0.9843]],         [[ 0.4196,  0.2706,  0.3020,  ..., -0.7098, -0.6392, -0.6863],          [ 0.2314,  0.0824,  0.2078,  ..., -0.8039, -0.6627, -0.6627],          [-0.1137, -0.2314, -0.0824,  ..., -0.7020, -0.5373, -0.4980],          ...,          [-0.2784, -0.1373, -0.2000,  ..., -0.9529, -0.9529, -0.9451],          [-0.6000, -0.3098, -0.3176,  ..., -0.9765, -0.9843, -0.9922],          [-0.7569, -0.4118, -0.3804,  ..., -0.9765, -0.9686, -0.9686]]]])}torch.Size([1, 3, 224, 224])\n\n\nProcessing the Dataset接下来就要批量将 dataset 里的 image 转换为数值，我们直接利用 ViT 的 ImageProcessor。我们定义这个函数，仅返回训练所需的数据，即 1. 数值化的图片 2.分类标签\ndef process_example(example):    inputs = processor(example['image'], return_tensors='pt')    inputs['labels'] = example['labels']    return inputsprocess_example(ds['train'][0])\n\n\n\n\n{'pixel_values': tensor([[[[-0.5686, -0.5686, -0.5608,  ..., -0.0275,  0.1843, -0.2471],          [-0.6078, -0.6000, -0.5765,  ..., -0.0353, -0.0196, -0.2627],          [-0.6314, -0.6314, -0.6078,  ..., -0.2314, -0.3647, -0.2235],          ...,          [-0.5373, -0.5529, -0.5843,  ..., -0.0824, -0.0431, -0.0902],          [-0.5608, -0.5765, -0.5843,  ...,  0.3098,  0.1843,  0.1294],          [-0.5843, -0.5922, -0.6078,  ...,  0.2627,  0.1608,  0.2000]],         [[-0.7098, -0.7098, -0.7490,  ..., -0.3725, -0.1608, -0.6000],          [-0.7333, -0.7333, -0.7569,  ..., -0.3647, -0.3255, -0.5686],          [-0.7490, -0.7490, -0.7725,  ..., -0.5373, -0.6549, -0.5373],          ...,          [-0.7725, -0.7804, -0.8196,  ..., -0.2235, -0.0353,  0.0824],          [-0.7961, -0.8118, -0.8118,  ...,  0.1922,  0.3098,  0.3725],          [-0.8196, -0.8196, -0.8275,  ...,  0.0824,  0.2784,  0.3961]],         [[-0.9922, -0.9922, -1.0000,  ..., -0.5451, -0.3569, -0.7255],          [-0.9922, -0.9922, -1.0000,  ..., -0.5529, -0.5216, -0.7176],          [-0.9843, -0.9922, -1.0000,  ..., -0.6549, -0.7569, -0.6392],          ...,          [-0.8431, -0.8588, -0.8980,  ..., -0.5765, -0.5529, -0.5451],          [-0.8588, -0.8902, -0.9059,  ..., -0.2000, -0.2392, -0.2627],          [-0.8824, -0.9059, -0.9216,  ..., -0.2549, -0.2000, -0.1216]]]]), 'labels': 0}\n\n\n\nmap 返回的数据结构会将 PyTorch tensor 自动转换为 Python list（tolist()），以保证结果是 JSON 可序列化的。\nprepared_ds = ds.map(\tprocess_example,\tremove_columns=ds['train'].column_names,\tbatched=False,\tdesc=\"Processing dataset\")\n\n\nds['train']\n\n\nDataset({    features: ['image_file_path', 'image', 'labels'],    num_rows: 1034})\n\n\nprepared_ds['train']\n\n\nDataset({    features: ['labels', 'pixel_values'],    num_rows: 1034})\n\n\n\nTraining and Evaluation数据已经处理完毕，现在你可以开始搭建训练流程了。本教程使用的是 Hugging Face 的 Trainer，但在此之前我们需要完成以下几项准备工作：\n\n定义一个 collate 函数：用于将一个 batch 的数据整理成模型可以接受的格式。\n定义评估指标：在训练过程中，模型需要根据预测准确率进行评估，因此你需要定义一个 compute_metrics 函数来计算这一指标。\n加载预训练模型检查点：你需要加载一个预训练的检查点，并正确配置它以便进行训练。\n定义训练配置：包括超参数设置、保存策略、日志输出等。\n\ndata collator 里再次将数据转换为 tensor，因为 dataset.map 默认还是会把 tensor 改为 list\nimport torchdef collate_fn(batch):    return {        'pixel_values': torch.stack([torch.tensor(x['pixel_values']).squeeze(0) for x in batch]),        'labels': torch.tensor([x['labels'] for x in batch])    }\n\n\ncollate_fn([prepared_ds['train'][1], prepared_ds['train'][1]])['pixel_values'].shape\n\n\ntorch.Size([2, 3, 224, 224])\n\n\n\n我们利用 evaluate 的 accuracy 函数来计算分类准确率\nimport numpy as npfrom evaluate import loadmetric = load(\"../DC/evaluate/metrics/accuracy\")def compute_metrics(p):    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n\n现在让我们加载预训练模型。在初始化时，我们会传入 num_labels 参数，以便模型构建一个具有正确输出单元数量的分类头。同时，我们还会提供 id2label 和 label2id 的映射关系，以便在将模型推送到 Hugging Face Hub 时，能够在界面中显示可读的标签名称。\nfrom transformers import ViTForImageClassificationlabels = ds['train'].features['labels'].namesprint(f'num of labels: {len(labels)}')model = ViTForImageClassification.from_pretrained(    model_name_or_path,    num_labels=len(labels),    id2label={i: c for i, c in enumerate(labels)},    label2id={c: i for i, c in enumerate(labels)})\n\nnum of labels: 3Some weights of ViTForImageClassification were not initialized from the model checkpoint at ../DC/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nfrom transformers import TrainingArgumentstraining_args = TrainingArguments(  output_dir=\"./vit-base-beans\",  per_device_train_batch_size=16,  eval_strategy=\"steps\",  num_train_epochs=2,  fp16=True,  save_steps=100,  eval_steps=10,  logging_steps=1,  learning_rate=2e-4,  save_total_limit=2,  remove_unused_columns=False,  push_to_hub=False,  report_to='wandb',  # 使用 wandb 进行训练监控  load_best_model_at_end=True,)\n\n\nfrom transformers import Trainertrainer = Trainer(    model=model,    args=training_args,    data_collator=collate_fn,    compute_metrics=compute_metrics,    train_dataset=prepared_ds[\"train\"],    eval_dataset=prepared_ds[\"validation\"],    processing_class=processor,)\n\n\ntrain_results = trainer.train()trainer.save_model()trainer.log_metrics(\"train\", train_results.metrics)trainer.save_metrics(\"train\", train_results.metrics)trainer.save_state()\n\n\n***** train metrics *****  epoch                    =         2.0  total_flos               = 149248978GF  train_loss               =       0.201  train_runtime            =  0:04:42.28  train_samples_per_second =       7.326  train_steps_per_second   =       0.461\n\n\nmetrics = trainer.evaluate(prepared_ds['validation'])trainer.log_metrics(\"eval\", metrics)trainer.save_metrics(\"eval\", metrics)\n\n***** eval metrics *****  epoch                   =        2.0  eval_accuracy           =     0.9925  eval_loss               =     0.0374  eval_runtime            = 0:00:09.34  eval_samples_per_second =     14.238  eval_steps_per_second   =       1.82\n\n\ninfer推理很简单了，直接把某个 image  通过 ImageProcessor 处理下图片为 tensor，接着 forward 到 model 即可，得到 logit 后再 argmax 就得到了预测类别。\nex = ds['test'][0]ex\n\n\n{'image_file_path': '/home/albert/.cache/huggingface/datasets/downloads/extracted/807042d188eb9a5d1d9a4179867e5b93eea6ed98d063904065fe40011681df29/test/angular_leaf_spot/angular_leaf_spot_test.0.jpg', 'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500&gt;, 'labels': 0}\n\n\n\n\nwith torch.no_grad():\tpixel_values = processor(ex['image'], return_tensors='pt').pixel_values\tpixel_values = pixel_values.to(model.device)\toutputs = model(pixel_values)logits = outputs.logitsprint(logits.shape)prediction = logits.argmax(-1)print(\"Predicted class index:\", prediction.item())print(\"Predicted class:\", model.config.id2label[prediction.item()])\n\ntorch.Size([1, 3])Predicted class index: 0Predicted class: angular_leaf_spot\n\n\n\n","categories":["多模态"],"tags":["多模态","CV"]},{"title":"Alpaca 源码分析","url":"/2025/05/16/LLM-basic-series/alpaca/","content":"\nSelf-Instruct 过程是一种迭代自举算法，它从一组手动编写的指令种子集开始，并用它们来提示语言模型生成新的指令以及相应的输入 - 输出实例。然后对这些生成结果进行过滤以去除低质量或相似的实例，所得数据被添加回任务池中。这个过程可以重复多次，从而产生大量的指令数据集合，可用于微调语言模型以更有效地遵循指令。\nalpaca 源码地址: https://github.com/tatsu-lab/stanford_alpaca.git\n加载模型model_name_or_path = '../DataCollection/officials/Qwen2.5-1.5b-Instruct'model = AutoModelForCausalLM.from_pretrained(    model_name_or_path,)tokenizer = AutoTokenizer.from_pretrained(    model_name_or_path,    padding_side=\"right\",    use_fast=False,)\n\n注意需要检查tokenizer.pad_token_id，因为在padding时会用到，其他eos之类的不需要检查。\n加载json数据数据原始格式(假设只有两条微调数据)\n[    {        \"instruction\": \"Give three tips for staying healthy.\",        \"input\": \"\",        \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"    },    {        \"instruction\": \"What are the three primary colors?\",        \"input\": \"\",        \"output\": \"The three primary colors are red, blue, and yellow.\"    },]\n\n每条都是三元组的形式， 数据字段如下：\n\ninstruction（指令）：描述模型应执行的任务。52000 条指令中的每一条都是唯一的。\n\ninput（输入）：任务的可选上下文或输入。大约 40% 的示例有输入。（可选的，因为可有可无，所以需要有两种拼接格式）\n\noutput（输出）：由 text-davinci-003 生成的指令答案。\n\n\ndef _make_r_io_base(f, mode: str):    if not isinstance(f, io.IOBase):        f = open(f, mode=mode)    return fdef jload(f, mode=\"r\"):    \"\"\"Load a .json file into a dictionary.\"\"\"    f = _make_r_io_base(f, mode)    jdict = json.load(f)    f.close()    return jdict# 加载指令微调数据，格式为list[dict]data_path = './alpaca_data.json'# data_path = './alpaca_data_100.json'list_data_dict = jload(data_path)print(len(list_data_dict))pprint(list_data_dict[0])\n\n输出\n52002{'input': '', 'instruction': 'Give three tips for staying healthy.', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits '           'and vegetables. \\n'           '2. Exercise regularly to keep your body active and strong. \\n'           '3. Get enough sleep and maintain a consistent sleep schedule.'}\n\n拼接dict数据PROMPT_DICT = {    \"prompt_input\": (        \"Below is an instruction that describes a task, paired with an input that provides further context. \"        \"Write a response that appropriately completes the request.\\n\\n\"        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"    ),    \"prompt_no_input\": (        \"Below is an instruction that describes a task. \"        \"Write a response that appropriately completes the request.\\n\\n\"        \"### Instruction:\\n{instruction}\\n\\n### Response:\"    ),}prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]sources = [    prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)    for example in list_data_dict]targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n\npprint(list_data_dict[0])print(10*'-')print(sources[0])print(10*'-')print(targets[0])\n\n输出\n{'input': '', 'instruction': 'Give three tips for staying healthy.', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits '           'and vegetables. \\n'           '2. Exercise regularly to keep your body active and strong. \\n'           '3. Get enough sleep and maintain a consistent sleep schedule.'}----------Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction:Give three tips for staying healthy.### Response:----------1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. 2. Exercise regularly to keep your body active and strong. 3. Get enough sleep and maintain a consistent sleep schedule.&lt;|im_end|&gt;\n\n注意并不是直接拼接，而是加入了类似system prompt的前置说明，和指令和生成内容的标识符\n预处理数据 tokenize这一节将句子转换为input_ids和label。注意label只有output部分内容是有有效的，其他无效（包括prompt）。在指令微调（Instruction Tuning）中，通常我们仅设置输出部分的 label（即目标序列）是有效的，而忽略输入部分的 label，这是因为以下原因：\n在指令微调（Instruction Tuning）中，通常我们仅设置输出部分的 label（即目标序列）是有效的，而忽略输入部分的 label，这是因为以下原因：\n1.输入部分是提示（Prompt），无需计算损失\n\n指令微调的核心目标是让模型学会在特定提示（Prompt）下生成符合预期的输出。\n输入部分（指令和上下文）作为条件提供给模型，用于引导模型生成合适的输出。它本身并不需要预测，因此不应对输入部分计算损失或更新权重。\n如果对输入部分计算损失，模型可能会尝试“记住”输入，而非专注于学习如何生成正确的输出。\n\n2.语言模型的自回归性质\n\nTransformer 模型（如 GPT 或 LLaMA）的自回归训练目标是最大化下一个 token 的概率。\n在微调时，输入部分（Prompt）已经是已知的条件，因此模型的主要任务是基于输入生成正确的输出（即目标文本）。对输入部分计算损失没有意义。\n\n3.对齐训练目标\n\n微调的训练目标是让模型在给定提示下生成期望的响应。这种训练目标的优化重点是输出部分的预测。\n通过忽略输入部分的 label，只优化输出部分的生成，能够更准确地对齐训练目标与实际使用目标。\n\n4.对生成任务的意义\n\n指令微调模型通常应用于生成任务（如回答问题、对话、翻译等），其重点是生成的内容，而非输入的内容。\n忽略输入部分的 label 有助于模型专注于如何生成符合上下文和指令的内容，而不是浪费资源在回归输入上。\n\n5.实际效果\n\n如果强行对输入部分计算损失，训练后的模型可能会出现以下问题：生成的输出可能更倾向于复制输入内容，而非理解指令后生成有意义的回答。对生成任务的泛化能力较弱，因为输入部分的损失干扰了输出部分的优化。\n\n6.避免梯度干扰\n\n如果对输入部分和输出部分同时计算损失，模型可能会受到梯度干扰：输入部分的 token 会被错误地视为目标，导致模型尝试“预测”已知的输入内容。\n这会对生成任务的优化目标造成负面影响，降低模型在输出部分生成正确内容的能力。\n\n总结：在指令微调时，只对输出部分计算 label 是为了：\n\n专注于优化生成目标。\n避免梯度干扰。\n提高模型对指令生成任务的泛化能力。\n\nIGNORE_INDEX = -100def _tokenize_fn(strings: Sequence[str], tokenizer: AutoTokenizer) -&gt; Dict:    \"\"\"Tokenize a list of strings.\"\"\"    # 先将每个元素tokenize，按照最大长度padding，但实际每次只输入一个句子，根本不会padding    tokenized_list = [        tokenizer(            text,            return_tensors=\"pt\",            padding=\"longest\",            max_length=tokenizer.model_max_length,            truncation=True,        )        for text in strings    ]    # 取出input_ids和labels为数组    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]    # 统计每个句子的非 padding token 的 token 数量    input_ids_lens = labels_lens = [        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list    ]    return dict(        input_ids=input_ids,        labels=labels,        input_ids_lens=input_ids_lens,        labels_lens=labels_lens,    )def preprocess(    sources: Sequence[str],    targets: Sequence[str],    tokenizer: AutoTokenizer,) -&gt; Dict:    \"\"\"Preprocess the data by tokenizing.\"\"\"    # 将prompt和预期输出组合    examples = [s + t for s, t in zip(sources, targets)]    # 分别对组合字符串和单独prompt进行tokenize    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]    input_ids = examples_tokenized[\"input_ids\"]    labels = copy.deepcopy(input_ids)    # 将 prompt 部分的 label 设置为 -100 (交叉熵的忽略 index)    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):        label[:source_len] = IGNORE_INDEX    return dict(input_ids=input_ids, labels=labels)data_dict = preprocess(sources, targets, tokenizer)\n\nfor k in data_dict.keys():    print(len(data_dict[k]))    print(data_dict[k][0])\n\n52002tensor([ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,          9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,         14374,  29051,    510,  35127,   2326,  10414,    369,  19429,   9314,           382,  14374,   5949,     25,     16,   5142,    266,    264,  23831,          9968,    323,   1281,   2704,    311,   2924,  11260,    315,  25322,           323,  23880,     13,    715,     17,     13,  32818,  15502,    311,          2506,    697,   2487,   4541,    323,   3746,     13,    715,     18,            13,   2126,   3322,   6084,    323,  10306,    264,  12966,   6084,          9700,     13, 151645])52002tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,          -100,   -100,   -100,   -100,     16,   5142,    266,    264,  23831,          9968,    323,   1281,   2704,    311,   2924,  11260,    315,  25322,           323,  23880,     13,    715,     17,     13,  32818,  15502,    311,          2506,    697,   2487,   4541,    323,   3746,     13,    715,     18,            13,   2126,   3322,   6084,    323,  10306,    264,  12966,   6084,          9700,     13, 151645])\n\n封装为datasetclass SupervisedDataset(Dataset):    def __init__(self, input_ids, labels):        super(SupervisedDataset, self).__init__()        self.input_ids = input_ids        self.labels = labels    def __len__(self):        return len(self.input_ids)    def __getitem__(self, i) -&gt; Dict[str, torch.Tensor]:        return dict(input_ids=self.input_ids[i], labels=self.labels[i])    ds = SupervisedDataset(**data_dict)\n\n这时候每个句子还是单独的tensor，由于transformer中的已有DataCollator一般不会接受label（会报错，一般只接受input_ids和attention_mask），所以需要单独写一个DataCollator\npadding DataCollator@dataclassclass DataCollatorForSupervisedDataset(object):    \"\"\"Collate examples for supervised fine-tuning.\"\"\"    tokenizer: AutoTokenizer    def __call__(self, instances: Sequence[Dict]) -&gt; Dict[str, torch.Tensor]:        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))        # input_ids 用 pad_token_id 补齐        input_ids = torch.nn.utils.rnn.pad_sequence(            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id        )        # labels 用 IGNORE_INDEX 补齐        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)        return dict(            input_ids=input_ids,            labels=labels,            attention_mask=input_ids.ne(self.tokenizer.pad_token_id), # attention_mask 直接基于 input_ids        )dc = DataCollatorForSupervisedDataset(tokenizer)# DataCollator的输入是list[dict[str, tensor]]ret = [ds[index] for index in range(2,4)]ret = dc(ret)print([len(input_ids) for input_ids in ret['input_ids']])\n\n注意DataCollator的输入是list[dict[str, tensor]]，不要直接把dataset的元素以dataset[a:b]的格式输入给DataCollator。\ntraintrain_dataset = SupervisedDataset(**data_dict)eval_dataset=Nonedata_collator = DataCollatorForSupervisedDataset(tokenizer)\n\ntraining_args = TrainingArguments(output_dir='./output',                                  num_train_epochs=3,                                  per_device_train_batch_size=8,                                  per_device_eval_batch_size=8,                                  gradient_accumulation_steps=8,                                  evaluation_strategy='no',                                  save_strategy='steps',                                  save_steps=2000,                                  save_total_limit=1,                                  learning_rate=2e-5,                                  weight_decay=0.,                                  warmup_ratio=0.03,                                  lr_scheduler_type='cosine',                                  logging_steps=1,                                  report_to=[]                                  )trainer = Trainer(model=model,                   tokenizer=tokenizer,                   args=training_args,                   train_dataset=train_dataset,                  eval_dataset=eval_dataset,                  data_collator=data_collator,                  )trainer.train()\n\n修改Alpaca源码改进文件读取和预处理1.huggingface的dataset读取\nfrom datasets import load_datasetdataset = load_dataset('json',                        data_dir='/data02/hyzhang10/pengxia2/tws/data',                        data_files={                           'train': 'alpaca_data_100.json',                         #    'test': 'alpaca_data_100.json'                           }                       )print(dataset)\n\n输出\nDatasetDict({    train: Dataset({        features: ['instruction', 'input', 'output'],        num_rows: 100    })})\n\n2.dataset的批量预处理\nIGNORE_INDEX = -100PROMPT_DICT = {    \"prompt_input\": (        \"Below is an instruction that describes a task, paired with an input that provides further context. \"        \"Write a response that appropriately completes the request.\\n\\n\"        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"    ),    \"prompt_no_input\": (        \"Below is an instruction that describes a task. \"        \"Write a response that appropriately completes the request.\\n\\n\"        \"### Instruction:\\n{instruction}\\n\\n### Response:\"    ),}prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]def preprocess_func(example):    source = prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)    target = f\"{example['output']}{tokenizer.eos_token}\"    full_example = source + target    full_example_tokenzied = tokenizer(full_example, return_tensors=\"pt\",padding=\"longest\", max_length=tokenizer.model_max_length, truncation=True)    input_ids = full_example_tokenzied['input_ids'][0]    labels = copy.deepcopy(input_ids)    source_tokenzied = tokenizer(source, return_tensors=\"pt\",padding=\"longest\", max_length=tokenizer.model_max_length, truncation=True)    labels[:len(source_tokenzied['input_ids'][0])] = IGNORE_INDEX    return dict(        input_ids=input_ids,         labels=labels    )# preprocess_func(dataset['train'][0])train_ds = dataset['train'].map(preprocess_func, remove_columns=list(dataset['train'].features.keys()))print(train_ds)\n\n输出\nDataset({    features: ['input_ids', 'labels'],    num_rows: 100})\n\n半精度训练1.模型加载，加载时设置torch_dtype\nmodel_name_or_path = '../DataCollection/officials/Llama-2-7b'# model_name_or_path = '../DataCollection/officials/Qwen2.5-1.5b-Instruct'model = AutoModelForCausalLM.from_pretrained(    model_name_or_path,    torch_dtype=torch.bfloat16)tokenizer = AutoTokenizer.from_pretrained(    model_name_or_path,    padding_side=\"right\",    use_fast=False,)\n\n必须要在加载前设置，不然训练时还会以默认的fp32加载入cuda，最终还会OOM\n2.训练参数，增加bf16=True\ntraining_args = TrainingArguments(output_dir='./output',                                  num_train_epochs=3,                                  per_device_train_batch_size=2,                                  per_device_eval_batch_size=8,                                  gradient_accumulation_steps=8,                                  evaluation_strategy='no',                                  save_strategy='steps',                                  save_steps=2000,                                  save_total_limit=1,                                  learning_rate=2e-5,                                  weight_decay=0.,                                  warmup_ratio=0.03,                                  lr_scheduler_type='cosine',                                  logging_steps=1,                                  report_to=[],                                  bf16=True                                  )trainer = Trainer(model=model,                   tokenizer=tokenizer,                   args=training_args,                   train_dataset=train_dataset,                  eval_dataset=eval_dataset,                  data_collator=data_collator,                  )train_output = trainer.train()\n\n","categories":["LLM 基础"]},{"title":"LoRA - Low-Rank Adaptation","url":"/2025/05/17/LLM-basic-series/lora/","content":"自然语言处理的一个重要范式是：在通用领域数据上进行大规模预训练，再对特定任务或领域进行适配。随着预训练模型规模的不断扩大，全量微调（即重新训练所有模型参数）变得越来越不可行。以 GPT-3 175B 为例——为每个下游任务部署一份包含 1750 亿参数的微调模型实例，成本极其高昂。\n为此，LoRA 提出了一种低秩适配方法（Low-Rank Adaptation，简称 LoRA），该方法冻结预训练模型的权重，并在 Transformer 架构的每一层中注入可训练的秩分解矩阵，从而显著减少下游任务所需训练参数的数量。\n\n\nMany applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning, which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model.\nMany sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques often introduce inference latency by extending model depth or reduce the model’s usable sequence length. \nWe take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed Low-Rank Adaptation (LoRA) approach.\n\nLORA 的做法并不局限于大模型领域，它的作用是在 linear 层上，理论上有 linear 层就可以用。\nLORA 的优点在于:\n\n对下游任务的高度适应：lora 很小，可以很快切换 lora 块以切换到另外的任务，而且很多的任务也对存储没大的压力。\n训练更高效，大部分参数都不需要计算梯度和优化器状态，只需要计算很少的 lora 部分。\nlora 训练完后可以与原模型合并，推理时就不存在推理延迟，PS：如果是一般的 adapter，它需要顺序执行，而 transformer 本身是高度并行的，adapter 会拖慢 transformer 本身的推理速度。\n\n已有方法插入 Adapter 层主要问题：\n\n推理时增加延迟：\nAdapter 层即使参数很少（&lt;1%原模型），也需要额外计算；\n由于大模型高度依赖硬件并行以降低延迟，而 Adapter 层通常是顺序处理的，因此在小 batch size 的在线推理中（如 GPT-2 单 GPU 推理）会显著拉高延迟。\n\n\n不易跳过计算：\n无法轻易通过剪枝或跳跃来绕过 Adapter 层。\n\n\n模型切分后的通信开销高：\n若模型需进行跨 GPU 切分，Adapter 层的额外深度会引入更多通信操作（如 AllReduce, Broadcast），除非冗余存储参数。\n\n\n\n\n直接优化 Prompt（如前缀微调，Prefix Tuning）主要问题：\n\n优化困难：\n前缀参数难以训练，表现出非单调性（performance 与参数量不成正比）。\n\n\n占用输入序列长度：\nPrefix 占据部分输入序列长度，压缩了模型处理实际任务信息的窗口，可能导致性能下降。\n\n\n\n\n\n\n\n方法\n主要问题\n\n\n\nAdapter 层\n增加推理延迟；顺序处理限制并行；模型切分时通信开销高\n\n\nPrompt 优化方法\n参数难以优化；占用序列长度导致任务性能下降\n\n\n这也说明了为何需要寻找新的、更高效的参数高效化微调方法。\n问题定义在全量微调（full fine-tuning）过程中，模型初始化为预训练权重 ，并更新为 ，通过重复使用梯度上升来最大化条件语言建模目标：\n\n全量微调的主要缺点之一是，对于每个下游任务，我们都要学习一组不同的参数 ，其维度  与  相同。因此，如果预训练模型非常大（例如 GPT-3， Billion），存储和部署多个微调模型实例将变得非常困难，甚至不可行。\n在 LORA 中，我们采用一种更高效的参数学习方法：将任务特定的参数增量表示为 ，其中  是一个更小规模的参数集合，满足 。于是，寻找  的任务就变成了对  的优化：LORA 提出使用低秩表示（low-rank representation）来编码 ，以实现计算和内存的双重高效。当预训练模型为 GPT-3（175B）时，训练所需的参数量  可以低至  的 0.01%。\n方法说明神经网络中包含许多执行矩阵乘法的全连接层。这些层中的权重矩阵通常具有满秩（full-rank）。Aghajanyan 等人（2020）指出，尽管预训练语言模型经过随机投影到更小的子空间，仍然能高效学习，说明其具有较低的“内在维度”。受此启发，我们假设：适配过程中的权重更新也具有较低的“内在秩”。\n对于预训练权重矩阵 ，我们将其更新限制为低秩分解的形式：其中 ，且秩 。这样  的表达会受限，但做微调够了。\n在训练期间， 冻结，不参与梯度更新，只有  和  是可训练参数。注意  和  都与相同的输入  相乘，它们的输出在对应位置上逐元素相加。改写后的前向传播公式为：若原本全连接层为768×768。我们通过A,B替代，可以变成768×8 、8×768， 参数量从768×768变成了768×8 + 8×768，微调参数量为原来的 2%.\n我们使用高斯分布随机初始化 ，并将  初始化为全零，因此训练开始时 。接着我们将  乘以缩放因子 ，其中  是关于  的常数。在使用 Adam 优化器时，如果初始化得当，调整  的效果大致等同于调整学习率。因此，我们简单地将  设置为尝试的第一个  值对应的常数，并不进行调参。实际前向传播应为：\n一种更一般的微调形式是仅微调部分预训练参数。而 LoRA 更进一步，它在适配过程中不要求累积的梯度更新具有满秩。这意味着，如果将 LoRA 应用于所有权重矩阵并训练所有 bias，我们可以通过设置 LoRA 的秩  与原始权重矩阵的秩一致，来近似恢复全量微调的表达能力。换句话说，随着可训练参数数量的增加，LoRA 的训练效果逐渐接近于原始模型的微调；而基于 Adapter 的方法最终趋近于一个 MLP，前缀方法则更适用于不能处理长输入序列的模型。\n在实际部署时，我们可以显式地计算并存储合并后的权重 ，并像普通模型那样进行推理。注意  和  都在  空间中。当我们切换到另一个下游任务时，只需从  中减去当前的 ，再加上新的 ，这是一种非常快速、几乎不占内存的操作。关键是：这种适配过程不会引入任何额外的推理延迟。\n源码https://github.com/microsoft/LoRA.git 这是LORA的源码\n# lora 相关参数的基类class LoRALayer():    def __init__(        self,         r: int,         lora_alpha: int,         lora_dropout: float,        merge_weights: bool,    ):        self.r = r        self.lora_alpha = lora_alpha        # Optional dropout        if lora_dropout &gt; 0.:            self.lora_dropout = nn.Dropout(p=lora_dropout)        else:            self.lora_dropout = lambda x: x        # Mark the weight as unmerged        self.merged = False        self.merge_weights = merge_weights        # 继承 linear 类class Linear(nn.Linear, LoRALayer):    # LoRA implemented in a dense layer    def __init__(        self,         in_features: int,         out_features: int,         r: int = 0,         lora_alpha: int = 1,         lora_dropout: float = 0.,        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)        merge_weights: bool = True,        **kwargs    ):        nn.Linear.__init__(self, in_features, out_features, **kwargs)        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,                           merge_weights=merge_weights)        self.fan_in_fan_out = fan_in_fan_out        # Actual trainable parameters  初始化时额外创建两个矩阵 B A        if r &gt; 0:            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))            self.scaling = self.lora_alpha / self.r            # Freezing the pre-trained weight matrix            self.weight.requires_grad = False        self.reset_parameters()        if fan_in_fan_out:            self.weight.data = self.weight.data.transpose(0, 1)    # lora 矩阵的初始化    def reset_parameters(self):        nn.Linear.reset_parameters(self)        if hasattr(self, 'lora_A'):            # initialize B the same way as the default for nn.Linear and A to zero            # this is different than what is described in the paper but should not affect performance            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))            nn.init.zeros_(self.lora_B)\t    # 设置是否训练 lora，不训练时是预训练阶段，训练就是微调阶段，这里只设置状态变量 merged 和权重    def train(self, mode: bool = True):        def T(w):            return w.transpose(0, 1) if self.fan_in_fan_out else w        nn.Linear.train(self, mode)        if mode:            if self.merge_weights and self.merged:                # Make sure that the weights are not merged                if self.r &gt; 0:                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling                self.merged = False        else:            if self.merge_weights and not self.merged:                # Merge the weights and mark it                if self.r &gt; 0:                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling                self.merged = True           # 当 merged 时，额外加上 lora 的结果，否则只用 linear 的结果    def forward(self, x: torch.Tensor):        def T(w):            return w.transpose(0, 1) if self.fan_in_fan_out else w        if self.r &gt; 0 and not self.merged:            result = F.linear(x, T(self.weight), bias=self.bias)                        result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling            return result        else:            return F.linear(x, T(self.weight), bias=self.bias)\n\n这个是作者的源码，看上去还是很直接的\n以下是 peft 的源码\nclass Linear(nn.Module, LoraLayer):        def forward(self, x: torch.Tensor, *args: Any, **kwargs: Any) -&gt; torch.Tensor:        self._check_forward_args(x, *args, **kwargs)        adapter_names = kwargs.pop(\"adapter_names\", None)        if self.disable_adapters:            if self.merged:                self.unmerge()            result = self.base_layer(x, *args, **kwargs)        elif adapter_names is not None:            result = self._mixed_batch_forward(x, *args, adapter_names=adapter_names, **kwargs)        elif self.merged:            result = self.base_layer(x, *args, **kwargs)        else:            result = self.base_layer(x, *args, **kwargs)            torch_result_dtype = result.dtype            lora_A_keys = self.lora_A.keys()            for active_adapter in self.active_adapters:                if active_adapter not in lora_A_keys:                    continue                lora_A = self.lora_A[active_adapter]                lora_B = self.lora_B[active_adapter]                dropout = self.lora_dropout[active_adapter]                scaling = self.scaling[active_adapter]                x = self._cast_input_dtype(x, lora_A.weight.dtype)                if not self.use_dora[active_adapter]:                    result = result + lora_B(lora_A(dropout(x))) * scaling                 else:                    if isinstance(dropout, nn.Identity) or not self.training:                        base_result = result                    else:                        x = dropout(x)                        base_result = None                    result = result + self.lora_magnitude_vector[active_adapter](                        x,                        lora_A=lora_A,                        lora_B=lora_B,                        scaling=scaling,                        base_layer=self.get_base_layer(),                        base_result=base_result,                    )            result = result.to(torch_result_dtype)        return result\n\nresult = result + lora_B(lora_A(dropout(x))) * scaling 这个是不适用 dora 情况的处理，即正常 lora。\nPEFT lora加载预训练模型import jsonimport transformersfrom copy import deepcopyfrom typing import Unionfrom dataclasses import dataclass, asdictfrom datasets import load_datasetfrom transformers import (\tAutoModelForCausalLM,\tAutoTokenizer,)from peft import (    LoraConfig,\tTaskType,    get_peft_model,)\n\n\nmodel_name_or_path = \"../DC/qwen2.5-3b\"model = AutoModelForCausalLM.from_pretrained(\tmodel_name_or_path,)tokenizer = AutoTokenizer.from_pretrained(\tmodel_name_or_path,\tuse_fast=False,)print(model)\n\n\nQwen2ForCausalLM(  (model): Qwen2Model(    (embed_tokens): Embedding(151936, 2048)    (layers): ModuleList(      (0-35): 36 x Qwen2DecoderLayer(        (self_attn): Qwen2Attention(          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)          (k_proj): Linear(in_features=2048, out_features=256, bias=True)          (v_proj): Linear(in_features=2048, out_features=256, bias=True)          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)        )        (mlp): Qwen2MLP(          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)          (act_fn): SiLU()        )        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)      )    )    (norm): Qwen2RMSNorm((2048,), eps=1e-06)    (rotary_emb): Qwen2RotaryEmbedding()  )  (lm_head): Linear(in_features=2048, out_features=151936, bias=False))\n\n定义 lora 模型\ntask_type: peft 会根据任务类型进行调整，需要传入 peft.TaskType 中的对象\nr: LoRA模型的注意力维度（也叫秩）。表示低秩适应矩阵的维度。\ntarget_modules: 要应用LoRA的模块名称。如果是字符串，会执行正则匹配；如果是列表，会精确匹配或检查模块名是否以指定的字符串结尾。\nlora_dropout: LoRA层的dropout概率，防止过拟合。\nmodules_to_save:除了LoRA适配器层之外，还要保存并训练的模块。用于某些模型，如分类任务中的输出层。\nlora_alpha：缩放因子,起到的是调节作用。\n\nlora_config = LoraConfig(\ttask_type=TaskType.CAUSAL_LM, \ttarget_modules=['q_proj', 'v_proj'], \tr=16, \tlora_alpha=16)asdict(lora_config)\n\n\n\n\n{'task_type': &lt;TaskType.CAUSAL_LM: 'CAUSAL_LM'&gt;, 'peft_type': &lt;PeftType.LORA: 'LORA'&gt;, 'auto_mapping': None, 'base_model_name_or_path': None, 'revision': None, 'inference_mode': False, 'r': 16, 'target_modules': {'q_proj', 'v_proj'}, 'exclude_modules': None, 'lora_alpha': 16, 'lora_dropout': 0.0, 'fan_in_fan_out': False, 'bias': 'none', 'use_rslora': False, 'modules_to_save': None, 'init_lora_weights': True, 'layers_to_transform': None, 'layers_pattern': None, 'rank_pattern': {}, 'alpha_pattern': {}, 'megatron_config': None, 'megatron_core': 'megatron.core', 'trainable_token_indices': None, 'loftq_config': {}, 'eva_config': None, 'corda_config': None, 'use_dora': False, 'layer_replication': None, 'runtime_config': {'ephemeral_gpu_offload': False}, 'lora_bias': False}\n\n\n\n\npeft_model = get_peft_model(model, lora_config)peft_model.print_trainable_parameters()print(peft_model)\n\ntrainable params: 3,686,400 || all params: 3,089,625,088 || trainable%: 0.1193PeftModelForCausalLM(  (base_model): LoraModel(    (model): Qwen2ForCausalLM(      (model): Qwen2Model(        (embed_tokens): Embedding(151936, 2048)        (layers): ModuleList(          (0-35): 36 x Qwen2DecoderLayer(            (self_attn): Qwen2Attention(              (q_proj): lora.Linear(                (base_layer): Linear(in_features=2048, out_features=2048, bias=True)                (lora_dropout): ModuleDict(                  (default): Identity()                )                (lora_A): ModuleDict(                  (default): Linear(in_features=2048, out_features=16, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=16, out_features=2048, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()                (lora_magnitude_vector): ModuleDict()              )              (k_proj): Linear(in_features=2048, out_features=256, bias=True)              (v_proj): lora.Linear(                (base_layer): Linear(in_features=2048, out_features=256, bias=True)                (lora_dropout): ModuleDict(                  (default): Identity()                )                (lora_A): ModuleDict(                  (default): Linear(in_features=2048, out_features=16, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=16, out_features=256, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()                (lora_magnitude_vector): ModuleDict()              )              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)            )            (mlp): Qwen2MLP(              (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)              (up_proj): Linear(in_features=2048, out_features=11008, bias=False)              (down_proj): Linear(in_features=11008, out_features=2048, bias=False)              (act_fn): SiLU()            )            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)          )        )        (norm): Qwen2RMSNorm((2048,), eps=1e-06)        (rotary_emb): Qwen2RotaryEmbedding()      )      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)    )  ))\n\nQV 矩阵都多了 lora 相关的矩阵\n加载数据template ={    \"description\": \"Legacy template, used by Original Alpaca repository.\",    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\",    \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\",    \"response_split\": \"### Response:\"    }def generate_prompt(\tinstruction: str,\tinput: Union[None, str] = None,\tlabel: Union[None, str] = None,    ) -&gt; str:\tif input:\t\tprompt = template[\"prompt_input\"].format(\t\t\tinstruction=instruction, input=input\t\t)\telse:\t\tprompt = template[\"prompt_no_input\"].format(\t\t\tinstruction=instruction\t\t)\tif label:\t\ttarget = f\"{label}{tokenizer.eos_token}\"\telse:\t\ttarget = \"\"\treturn prompt, targetdef preprocess_func(example):    source, target = generate_prompt(\t\tinstruction=example['instruction'],\t\tinput=example['input'],\t\tlabel=example['output']\t)    full_example = source + target    full_example_tokenzied = tokenizer(full_example, return_tensors=\"pt\",padding=\"longest\", max_length=tokenizer.model_max_length, truncation=True)    input_ids = full_example_tokenzied['input_ids'][0]    labels = deepcopy(input_ids)    source_tokenzied = tokenizer(source, return_tensors=\"pt\",padding=\"longest\", max_length=tokenizer.model_max_length, truncation=True)    labels[:len(source_tokenzied['input_ids'][0])] = -100    return dict(        input_ids=input_ids,         labels=labels    )\n\n\ndata = load_dataset(\"json\", data_files='./alpaca_data_gpt4.json')[\"train\"].select(range(2000))ds = data.train_test_split(test_size=0.2, shuffle=True, seed=42)train_ds = ds[\"train\"].map(\tpreprocess_func,\tremove_columns=ds['train'].column_names,\tbatched=False,\tdesc=\"Processing dataset\")val_ds = ds[\"test\"].map(\tpreprocess_func,\tremove_columns=ds['test'].column_names,\tbatched=False,\tdesc=\"Processing dataset\")\n\n训练from transformers import Trainer, TrainingArgumentsfrom transformers import DataCollatorForSeq2Seqdata_collator = DataCollatorForSeq2Seq(\ttokenizer=tokenizer,\treturn_tensors=\"pt\",\tpadding=True,)training_args = TrainingArguments(\toutput_dir=\"./lora-alpaca\",\tper_device_train_batch_size=4,\tgradient_accumulation_steps=8,\tper_device_eval_batch_size=8,\tnum_train_epochs=2,\tlearning_rate=2e-4,\tweight_decay=0.01,\tlogging_steps=10,\tsave_steps=100,\teval_strategy=\"steps\",\teval_steps=20,\tsave_total_limit=1,\tload_best_model_at_end=True,\treport_to='none')trainer = Trainer(\tmodel=peft_model,\targs=training_args,\ttrain_dataset=train_ds,\teval_dataset=val_ds,\tdata_collator=data_collator,)\n\ntrainer.train()trainer.evaluate()\n\n\n{'eval_loss': 0.9849340915679932, 'eval_runtime': 63.2362, 'eval_samples_per_second': 6.325, 'eval_steps_per_second': 0.791, 'epoch': 2.0}\n\n\n\nout_dir 内容如下:\n├── README.md├── adapter_config.json├── adapter_model.safetensors├── added_tokens.json├── checkpoint-100│   ├── README.md│   ├── adapter_config.json│   ├── adapter_model.safetensors│   ├── added_tokens.json│   ├── merges.txt│   ├── optimizer.pt│   ├── rng_state.pth│   ├── scheduler.pt│   ├── special_tokens_map.json│   ├── tokenizer_config.json│   ├── trainer_state.json│   ├── training_args.bin│   └── vocab.json├── merges.txt├── special_tokens_map.json├── tokenizer_config.json├── training_args.bin└── vocab.json\n\n第一层保留的最后的结果，必要的就是 adapter_config.json，adapter_model.safetensors，打开 adapter_config.json 可以看到预训练模型的地址，以及一堆 lora config\n\n\n\n\n使用以下代码可以加载 peft adapter 并合并\nlora_train_model = PeftModel.from_pretrained(model, model_id=\"./output_model/checkpoint\")merge_model = lora_train_model.merge_and_unload()merge_model.save_pretrained(\"./output_model/merge_model\")\n\n\n\n\n\n","categories":["LLM 基础"]},{"title":"function calling agent","url":"/2025/05/17/agent/function_calling_agent/","content":"在知道 agent 基础知识后，简单的 function calling agent 就可以手搓，或者直接用 openai 的 function calling\nimport osfrom openai import OpenAIclient = OpenAI(    api_key=\"XXX\",    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",)\n\n工具定义 + 工具 schema 化import jsonimport inspectfrom typing import get_type_hints, Anyclass OpenAITool:    def __init__(self, func):        self.func = func        self.name = func.__name__        self.description = inspect.getdoc(func) or \"\"        self.signature = inspect.signature(func)        self.annotations = get_type_hints(func)    def __call__(self, *args, **kwargs):        return self.func(*args, **kwargs)    def openai_schema(self) -&gt; dict:        properties = {}        required = []        for name, param in self.signature.parameters.items():            param_type = self.annotations.get(name, str)            properties[name] = {                \"type\": self._python_type_to_json_type(param_type),                \"description\": \"\"            }            if param.default is inspect.Parameter.empty:                required.append(name)        return {            \"name\": self.name,            \"description\": self.description,            \"parameters\": {                \"type\": \"object\",                \"properties\": properties,                \"required\": required            }        }    def _python_type_to_json_type(self, t: Any) -&gt; str:        if t in [int]: return \"integer\"        if t in [float]: return \"number\"        if t in [bool]: return \"boolean\"        if t in [list]: return \"array\"        if t in [dict]: return \"object\"        return \"string\"  # 默认类型def tool(func):    return OpenAITool(func)\n\n\n@tooldef get_weather(city: str, time: str) -&gt; str:    \"\"\"Get the weather for a given city at a specific time. Time can be 'now' or a datetime string.\"\"\"    return f\"[DUMMY WEATHER] The weather in {city} at {time} is sunny with 25°C.\"@tooldef calculator(expression: str) -&gt; float:    \"\"\"Evaluate a mathematical expression and return the result.\"\"\"    try:        return eval(expression, {\"__builtins__\": {}})    except Exception as e:        return float('nan')available_tools = [get_weather, calculator]available_tool_schema = [    item.openai_schema()    for item in available_tools]\n\n\ndef schema_to_prompt(tools: list[dict]) -&gt; str:    lines = []    for tool in tools:        lines.append(f\"Tool Name: {tool['name']}\")        lines.append(f\"Description: {tool['description']}\")        lines.append(\"Inputs:\")        for name, spec in tool['parameters']['properties'].items():            typ = spec.get(\"type\", \"string\")            desc = spec.get(\"description\", \"\")            lines.append(f\"- `{name}` ({typ}): {desc}\")        lines.append(\"\")    return \"\\n\".join(lines)tool_prompt = schema_to_prompt(available_tool_schema)print(tool_prompt)\n\nTool Name: get_weather\nDescription: Get the weather for a given city at a specific time. Time can be 'now' or a datetime string.\nInputs:\n- `city` (string): \n- `time` (string): \n\nTool Name: calculator\nDescription: Evaluate a mathematical expression and return the result.\nInputs:\n- `expression` (string): \n\n​    \nTool Name: get_weatherDescription: Get the weather for a given city at a specific time. Time can be 'now' or a datetime string.Inputs:- `city` (string): - `time` (string): Tool Name: calculatorDescription: Evaluate a mathematical expression and return the result.Inputs:- `expression` (string): \n\n\nsystem_message = \"\"\"You are a helpful agent. You have access to the following tools:{tools_description}Use the following reasoning pattern to solve tasks:&lt;thought&gt;I should consider what the user is asking...&lt;/thought&gt;&lt;action&gt;{{\"tool\": \"tool_name\", \"args\": {{\"param1\": \"value\", ...}}}}&lt;/action&gt;&lt;observation&gt;The result returned by the tool.&lt;/observation&gt;&lt;thought&gt;Ok, one problems solved, there is another...&lt;/thought&gt;&lt;action&gt;{{\"tool\": \"tool_name\", \"args\": {{\"param1\": \"value\", ...}}}}&lt;/action&gt;...&lt;thought&gt;Ok, I have all the information I need to answer the user.&lt;/thought&gt;&lt;final_answer&gt;My final answer to the user.&lt;/final_answer&gt;Repeat Thought/Action/Observation as needed. When the task is complete, return the final answer to the user.\"\"\".format(\ttools_description=tool_prompt)\n\n简单测试query = \"请帮我计算 1 + 2 * 3 的值，并告诉我明天的天气。\"messages = [\t{\"role\": \"system\", \"content\": system_message},\t{\"role\": \"user\", \"content\": query},]# 1. 发送消息response = client.chat.completions.create(\tmodel=\"qwen-turbo\",\tmessages=messages,)# 2. 解析响应response_message = response.choices[0].messageprint(response_message.content)\n\n&lt;thought&gt;首先，我需要使用计算器工具来计算数学表达式的值。&lt;/thought&gt;&lt;action&gt;{\"tool\": \"calculator\", \"args\": {\"expression\": \"1 + 2 * 3\"}}&lt;/action&gt;&lt;observation&gt;计算结果为 7。&lt;/observation&gt;&lt;thought&gt;接下来，我需要查询明天的天气情况。&lt;/thought&gt;&lt;action&gt;{\"tool\": \"get_weather\", \"args\": {\"city\": \"北京\", \"time\": \"tomorrow\"}}&lt;/action&gt;&lt;observation&gt;明天北京的天气预报显示为晴朗，气温介于15°C到25°C之间。&lt;/observation&gt;&lt;final_answer&gt;1 + 2 * 3 的值是 7，明天北京的天气预计为晴朗，气温在15°C到25°C之间。&lt;/final_answer&gt;\n\n此时，模型出现了幻觉现象，因为它生成了一个虚构的“Observation” —— 即模型自主生成的响应，而不是实际调用函数或工具的结果。为防止这种情况，我们在生成到“”之前就停止生成。这样，我们可以手动运行函数，然后将真实的输出作为 Observation 插入进去。\n接下来就可以正常解析了。但接下来的处理方式有两种\n\nThought / Action / Observation 全部放在一个 message 中（单轮 message）\n\n&lt;thought&gt;我应该调用搜索工具。&lt;/thought&gt;&lt;action&gt;{\"tool\": \"search\", \"args\": {\"query\": \"天气\"}}&lt;/action&gt;&lt;observation&gt;天气是晴天。&lt;/observation&gt;&lt;thought&gt;那我可以回答用户了。&lt;/thought&gt;&lt;final_answer&gt;今天是晴天。&lt;/final_answer&gt;\n\nAction 后中断，将 Observation 放入新的 message（多 message）\n\n# round 1&lt;thought&gt;我需要搜索天气信息。&lt;/thought&gt;&lt;action&gt;{\"tool\": \"search\", \"args\": {\"query\": \"天气\"}}&lt;/action&gt;→ 停止生成，执行工具调用 → 得到 observation# round 2&lt;observation&gt;天气是晴天。&lt;/observation&gt;→ 继续生成下一条消息（新 message）：&lt;thought&gt;现在我知道天气了，可以回答了。&lt;/thought&gt;&lt;final_answer&gt;今天是晴天。&lt;/final_answer&gt;\n\n当然 observation 可以追加到 action 后面，或者单独作为一个 message。\nif '&lt;action&gt;' in response_message.content:\taction = json.loads(response_message.content.split(\"&lt;action&gt;\")[1].split(\"&lt;/action&gt;\")[0])\ttool_name = action[\"tool\"]\targs = action[\"args\"]\t# 3. 调用工具\tfor tool in available_tools:\t\tif tool.name == tool_name:\t\t\tresult = tool(**args)\t\t\tbreak\t# 4. 返回结果\tmessages.append({\t\t\"role\": \"assistant\",\t\t\"content\": response_message.content + f\"&lt;observation&gt;{result}&lt;/observation&gt;\",\t})\t\tresponse = client.chat.completions.create(\t\tmodel=\"qwen-turbo\",\t\tmessages=messages,\t\tstop=[\"&lt;observation&gt;\"],\t)\tprint(f\"Messages: {json.dumps(messages, indent=2, ensure_ascii=False)}\")\tresponse_message = response.choices[0].message\tprint(response_message.content)elif '&lt;final_answer&gt;' in response_message.content:\tfinal_answer = response_message.content.split(\"&lt;final_answer&gt;\")[1].split(\"&lt;/final_answer&gt;\")[0]\tprint(final_answer)else:\tprint(\"No action and answer found in the response.\")\n\nMessages: [  {    \"role\": \"system\",    \"content\": \"You are a helpful agent. You have access to the following tools:\\n\\nTool Name: get_weather\\nDescription: Get the weather for a given city at a specific time. Time can be 'now' or a datetime string.\\nInputs:\\n- `city` (string): \\n- `time` (string): \\n\\nTool Name: calculator\\nDescription: Evaluate a mathematical expression and return the result.\\nInputs:\\n- `expression` (string): \\n\\n\\nUse the following reasoning pattern to solve tasks:\\n\\n&lt;thought&gt;I should consider what the user is asking...&lt;/thought&gt;\\n&lt;action&gt;{\\\"tool\\\": \\\"tool_name\\\", \\\"args\\\": {\\\"param1\\\": \\\"value\\\", ...}}&lt;/action&gt;\\n&lt;observation&gt;The result returned by the tool.&lt;/observation&gt;\\n&lt;thought&gt;Ok, one problems solved, there is another...&lt;/thought&gt;\\n&lt;action&gt;{\\\"tool\\\": \\\"tool_name\\\", \\\"args\\\": {\\\"param1\\\": \\\"value\\\", ...}}&lt;/action&gt;\\n...\\nRepeat Thought/Action/Observation as needed. When the task is complete, answer with:\\n&lt;final_answer&gt;Your final answer to the user.&lt;/final_answer&gt;\\n\"  },  {    \"role\": \"user\",    \"content\": \"请帮我计算 1 + 2 * 3 的值，并告诉我明天的天气。\"  },  {    \"role\": \"assistant\",    \"content\": \"&lt;thought&gt;首先，我应该使用计算器工具来计算数学表达式的值。&lt;/thought&gt;\\n&lt;action&gt;{\\\"tool\\\": \\\"calculator\\\", \\\"args\\\": {\\\"expression\\\": \\\"1 + 2 * 3\\\"}}&lt;/action&gt;\\n&lt;observation&gt;7&lt;/observation&gt;\"  }]&lt;thought&gt;接下来，我需要获取明天的天气信息。假设现在是2023年4月5日，那么明天的日期就是2023年4月6日。&lt;/thought&gt;&lt;action&gt;{\"tool\": \"get_weather\", \"args\": {\"city\": \"北京\", \"time\": \"2023-04-06\"}}&lt;/action&gt;\n\n现在看出大模型觉得第一个计算任务已经结束，还有另外查询天气任务需要处理，最新的 message 就是查询天气的 action\n自定义的多 message 的 function callingquery = \"请帮我计算 3的8次方 的值，并告诉我明天的天气。\"messages = [\t{\"role\": \"system\", \"content\": system_message},\t{\"role\": \"user\", \"content\": query},]while True:\tresponse = client.chat.completions.create(\t\tmodel=\"qwen-turbo\",\t\tmessages=messages,\t\tstop=[\"&lt;observation&gt;\"],\t)\t# 2. 解析响应\tresponse_message = response.choices[0].message\tif '&lt;action&gt;' in response_message.content:\t\taction = json.loads(response_message.content.split(\"&lt;action&gt;\")[1].split(\"&lt;/action&gt;\")[0])\t\ttool_name = action[\"tool\"]\t\targs = action[\"args\"]\t\t# 3. 调用工具\t\tfor tool in available_tools:\t\t\tif tool.name == tool_name:\t\t\t\tresult = tool(**args)\t\t\t\tbreak\t\t# 4. 返回结果\t\tmessages.append({\t\t\t\"role\": \"assistant\",\t\t\t\"content\": response_message.content + f\"&lt;observation&gt;{result}&lt;/observation&gt;\",\t\t})\t\t# print(f\"Messages: {json.dumps(messages, indent=2, ensure_ascii=False)}\")\t\t# response_message = response.choices[0].message\t\t# print(response_message.content)\telif '&lt;final_answer&gt;' in response_message.content:\t\tmessages.append({\t\t\t\"role\": \"assistant\",\t\t\t\"content\": response_message.content,\t\t})\t\tfinal_answer = response_message.content.split(\"&lt;final_answer&gt;\")[1].split(\"&lt;/final_answer&gt;\")[0]\t\tbreak\telse:\t\t# print(\"No action and answer found in the response.\")\t\tmessages.append({\t\t\t\"role\": \"assistant\",\t\t\t\"content\": response_message.content,\t\t})\t\tbreakprint(f\"Messages: {json.dumps(messages, indent=2, ensure_ascii=False)}\")\n\nMessages: [  {    \"role\": \"system\",    \"content\": \"You are a helpful agent. You have access to the following tools:\\n\\nTool Name: get_weather\\nDescription: Get the weather for a given city at a specific time. Time can be 'now' or a datetime string.\\nInputs:\\n- `city` (string): \\n- `time` (string): \\n\\nTool Name: calculator\\nDescription: Evaluate a mathematical expression and return the result.\\nInputs:\\n- `expression` (string): \\n\\n\\nUse the following reasoning pattern to solve tasks:\\n\\n&lt;thought&gt;I should consider what the user is asking...&lt;/thought&gt;\\n&lt;action&gt;{\\\"tool\\\": \\\"tool_name\\\", \\\"args\\\": {\\\"param1\\\": \\\"value\\\", ...}}&lt;/action&gt;\\n&lt;observation&gt;The result returned by the tool.&lt;/observation&gt;\\n&lt;thought&gt;Ok, one problems solved, there is another...&lt;/thought&gt;\\n&lt;action&gt;{\\\"tool\\\": \\\"tool_name\\\", \\\"args\\\": {\\\"param1\\\": \\\"value\\\", ...}}&lt;/action&gt;\\n...\\n&lt;thought&gt;Ok, I have all the information I need to answer the user.&lt;/thought&gt;\\n&lt;final_answer&gt;My final answer to the user.&lt;/final_answer&gt;\\n\\nRepeat Thought/Action/Observation as needed. When the task is complete, return the final answer to the user.\\n\"  },  {    \"role\": \"user\",    \"content\": \"请帮我计算 3的8次方 的值，并告诉我明天的天气。\"  },  {    \"role\": \"assistant\",    \"content\": \"&lt;thought&gt;我需要先计算数学表达式，然后查询明天的天气。&lt;/thought&gt;\\n&lt;action&gt;{\\\"tool\\\": \\\"calculator\\\", \\\"args\\\": {\\\"expression\\\": \\\"3^8\\\"}}&lt;/action&gt;\\n&lt;observation&gt;11&lt;/observation&gt;\"  },  {    \"role\": \"assistant\",    \"content\": \"&lt;thought&gt;已经得到了3的8次方的结果，现在我要查询明天的天气。&lt;/thought&gt;\\n&lt;action&gt;{\\\"tool\\\": \\\"get_weather\\\", \\\"args\\\": {\\\"city\\\": \\\"北京\\\", \\\"time\\\": \\\"tomorrow\\\"}}&lt;/action&gt;\\n&lt;observation&gt;[DUMMY WEATHER] The weather in 北京 at tomorrow is sunny with 25°C.&lt;/observation&gt;\"  },  {    \"role\": \"assistant\",    \"content\": \"3的8次方的值是6561。明天北京的天气预报为晴朗，气温约为25°C。\"  }]\n输出还是有点小问题，最终答案没有 thought 也没有 &lt;final_answer&gt;\nopenai 格式的 function callingquery = \"请帮我计算 3的8次方 的值，并告诉我明天的天气。\"available_tool_schema = [\t{\t\t\"type\": \"function\",\t\t\"function\": {\t\t\t\"name\": \"get_weather\",\t\t\t\"description\": \"Get the weather for a given city at a specific time. Time can be 'now' or a datetime string.\",\t\t\t\"parameters\": {\t\t\t\t\"type\": \"object\",\t\t\t\t\"properties\": {\t\t\t\t\t\"city\": {\t\t\t\t\t\t\"type\": \"string\",\t\t\t\t\t\t\"description\": \"The name of the city.\"\t\t\t\t\t},\t\t\t\t\t\"time\": {\t\t\t\t\t\t\"type\": \"string\",\t\t\t\t\t\t\"description\": \"The time for which to get the weather.\"\t\t\t\t\t}\t\t\t\t},\t\t\t\t\"required\": [\"city\", \"time\"]\t\t\t}\t\t}\t},\t{\t\t\"type\": \"function\",\t\t\"function\": {\t\t\t\"name\": \"calculator\",\t\t\t\"description\": \"Evaluate a mathematical expression and return the result.\",\t\t\t\"parameters\": {\t\t\t\t\"type\": \"object\",\t\t\t\t\"properties\": {\t\t\t\t\t\"expression\": {\t\t\t\t\t\t\"type\": \"string\",\t\t\t\t\t\t\"description\": \"The mathematical expression to evaluate.\"\t\t\t\t\t}\t\t\t\t},\t\t\t\t\"required\": [\"expression\"]\t\t\t}\t\t}\t}]messages = [\t{\"role\": \"user\", \"content\": query},]while True:\tresponse = client.chat.completions.create(\t\tmodel=\"qwen-turbo\",\t\tmessages=messages,\t\ttools=available_tool_schema,\t)\tresponse_message = response.choices[0].message\tmessages.append(response_message)\tif not response_message.tool_calls:\t\tbreak\t# 执行tools\tfor tool_call in response_message.tool_calls:\t\tfunction_args = json.loads(tool_call.function.arguments) \t\tresult = globals()[tool_call.function.name](**function_args)  \t\tprint(f\"use tool: {tool_call.function.name}\")\t\ttool_call_id = tool_call.id\t\t# 将tools执行的结果加入messages中\t\tmessages.append({\"role\": \"tool\", \"tool_call_id\": tool_call_id, \"content\": str(result)})print(messages[-1].content)\n\nuse tool: calculator\nuse tool: get_weather\n\nuse tool: calculatoruse tool: get_weather3的8次方的值是6561。明天纽约的天气预计是晴朗，气温约为25°C。\n\n","categories":["agent"]},{"title":"Agent 概念","url":"/2025/05/17/agent/agent_concept/","content":"任何高效的 AI 系统都需要为大语言模型（LLM）提供某种接触现实世界的能力：例如，调用搜索工具以获取外部信息，或者操控特定程序来完成任务。换句话说，LLM 应当具备自主性（agency）。具备代理能力的程序，是 LLM 通向外部世界的桥梁。\n\n代理（Agent）是指一个系统，它利用 AI 模型与环境交互，以实现用户定义的目标。它结合了推理、规划和行动执行（通常通过外部工具）来完成任务。\n**AI Agent 是一种程序，其工作流程由 LLM 的输出控制。**LLM 是 Agent 的一部分，扮演其“大脑（mind）”的角色；而 Agent 本身更像是一个“工作流（workflow）”，围绕 LLM 组织了一系列模块与机制，管理从输入到输出的整个流程。\nLLM 是 Agent 的组成部分，但 Agent 是围绕 LLM 构建起来的一个智能行为控制系统，它将模型的文本生成能力嵌入到了一个可交互、可执行的工作流中。\n\n你可以将代理理解为由两个主要部分组成：\n\n大脑（AI 模型）：所有的思考都发生在这里。AI 模型负责推理和规划，并根据当前情境决定采取哪些行动（Action）。\n\n身体（能力与工具）：代表代理所配备的一切操作能力。\n\n\n可执行的行为范围取决于代理被配备了什么功能。例如，人类没有翅膀，就不能执行“飞行”这个动作，但可以“走路”“奔跑”“跳跃”“抓取”等。同理，LLM 虽然很强大，但它本身只能生成文本。因此，开发者为它实现了额外功能（称为工具 Tools），通过这些工具，LLM 可以完成我们为其实现的各种动作（Actions）。\n任何使用 LLM 的系统，都会将 LLM 的输出整合进代码流程中。LLM 对程序控制流程的影响程度，就是该系统中 LLM 的“自主性”水平。注意，这里的“代理”并不是一个离散的“是 / 否”状态，而是在一个连续光谱上演进的概念——你赋予 LLM 越多控制权，它的自主性就越强。\n\n下面的表格展示了不同系统中的代理水平差异：\n\n\n\n简称\n描述\n自主性等级\n\n\n\n简单 LLM\nLLM 的输出对程序流程没有影响\n☆☆☆\n\n\nRouter\nLLM 输出控制一个 if/else 语句分支\n★☆☆\n\n\nTool call\nLLM 输出控制某个函数的调用\n★★☆\n\n\nMulti-step Agent\nLLM 输出控制程序的迭代与继续执行\n★★☆\n\n\nMulti-Agent\n一个代理流程可以启动另一个代理流程\n★★★\n\n\nCode Agents\nLLM 在代码中运行，能够定义自己的工具或启动其他代理\n★★★\n\n\n接下来是一个多步代理（Multi-step Agent）的代码结构示例：（可根据需求翻译或展示代码）\nmemory = [user_defined_task]while llm_should_continue(memory): # this loop is the multi-step part    action = llm_get_next_action(memory) # this is the tool-calling part    observations = execute_action(action)    memory += [action, observations]\n\n该 agentic 系统在一个循环中运行，每一步执行一个新的动作（action）（这个动作通常是调用某些预设的“工具”，这些工具其实就是函数），直到根据其观察结果判断：已经达到了可以解决当前任务的满意状态为止。\n\nAgent 如何控制工作流 - Messages and Special Tokensmessage 定义就像 ChatGPT 一样，用户通常是通过聊天界面与代理（Agent）交互的。因此，我们需要理解 LLM 是如何处理聊天内容的。当你与 ChatGPT 这样的系统对话时，你实际上是在交换消息（messages）。在幕后，这些消息会被拼接并格式化为模型可以理解的提示词（prompt）。\n这正是“聊天模板（chat templates）”发挥作用的地方。它们是用户和助手之间对话消息与模型特定格式要求之间的桥梁。换句话说，聊天模板负责结构化用户与代理之间的沟通，确保每个模型（即便使用不同的特殊标记符）都能接收到正确格式的提示内容。\n\n我们再次提到特殊标记符（special tokens），因为它们是模型用来区分用户与助手轮次开始和结束的标志。就像每个模型有自己的 EOS（End Of Sequence）标记符一样，它们对对话消息的格式和分隔符也有不同的规范。\n\n消息通常分为以下几种类型：\n\nSystem Message（系统消息，也称为系统提示词）：用于定义模型应该如何行为。它们提供的是持久性指令，指导后续的所有交互。在使用代理时，系统消息还会：\n\n提供有关可用工具的信息；\n指示模型如何格式化其要采取的动作；\n给出思考过程应如何分段的指导原则。\n\n\nUser 和 Assistant Messages（用户消息与助手消息）：顾名思义，分别对应用户的输入与模型的响应内容。\n\n\nmessage 格式化可以用 tokenizer 的 apply_chat_template 函数来转换 messages，tokenize 置为 false 就不会转换为 token id， add_generation_prompt 则是为下一个 assistant 的 message 加入前缀学习，即 &lt;|im_start|&gt;assistant\\n ，这部分属于固定格式，也相当于提示大模型接下来要输出 assistant 角色的内容。\nmessages = [    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},    {\"role\": \"user\", \"content\": \"What is calculus?\"},    {\"role\": \"assistant\", \"content\": \"Calculus is a branch of mathematics...\"},    {\"role\": \"user\", \"content\": \"Can you give me an example?\"},]from transformers import (\tAutoModelForCausalLM,\tAutoTokenizer,)model_name_or_path = \"../DC/qwen2.5-3b\"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)print(rendered_prompt)\n\n&lt;|im_start|&gt;systemYou are a math tutor.&lt;|im_end|&gt;&lt;|im_start|&gt;userWhat is calculus?&lt;|im_end|&gt;&lt;|im_start|&gt;assistantCalculus is a branch of mathematics...&lt;|im_end|&gt;&lt;|im_start|&gt;userCan you give me an example?&lt;|im_end|&gt;&lt;|im_start|&gt;assistant\n\n工具工具定义工具（Tool）是提供给大语言模型（LLM）调用的函数，每个工具都应该用于完成一个明确的目标。\n以下是 AI Agent 中常用的一些工具示例：\n\n\n\n工具名称\n描述\n\n\n\n网络搜索\n允许 Agent 从互联网上获取最新信息。\n\n\n图像生成\n根据文本描述生成图像。\n\n\n信息检索\n从外部知识库中检索信息。\n\n\nAPI 接口\n与外部服务的 API（如 GitHub、YouTube、Spotify 等）交互。\n\n\n一个好的工具应当是对大语言模型能力的补充。例如，当你需要执行算术运算时，提供一个计算器工具比依赖 LLM 自身的推理能力更能获得稳定准确的结果。\n\n工具的存在是为了弥补大模型的能力不足或缺陷，或仅仅是为了确保结果更加稳定与正确。\n\n此外，LLM 是基于训练数据来预测 prompt 的补全结果，这意味着它的内部知识只能覆盖训练数据之前的事件。因此，如果你的 Agent 需要访问实时数据，你必须通过工具提供这些数据。举例来说，如果你问 LLM“今天巴黎的天气如何”，而没有接入搜索工具，那么它可能会凭空捏造一个天气情况（hallucination）。\n\n一个工具应包含以下内容：\n\n对函数功能的文字描述；\n一个可调用对象（Callable）；\n带类型标注的参数（Arguments）；\n（可选）带类型标注的输出（Outputs）。\n\n\n需要注意的是，LLM 只能接受文本输入并生成文本输出，它自身无法主动调用工具。所谓“给 Agent 提供工具”，其实是指：教会 LLM 这些工具的存在，并在需要时引导其生成基于文本的调用指令。\n例如，假设你提供了一个可以查询实时天气的工具，然后问模型“巴黎今天的天气如何？”，LLM 会意识到这是一次调用“天气工具”的机会。它不会自己返回天气数据，而是生成类似 call weather_tool('Paris') 的调用文本。\n\n这时，Agent 系统会读取 LLM 的输出，识别出需要调用工具，代表 LLM 执行实际的工具函数调用，并获取真实的天气信息。\n工具调用的过程通常不会展示给用户：Agent 会将调用结果作为一个新消息追加进对话历史，并将更新后的对话再次传入 LLM。这时 LLM 会根据新的上下文生成自然流畅的最终答复。\n从用户的角度来看，似乎是 LLM 直接调用了工具，但实际上，整个工具调用过程是由 Agent 在后台完成的。\n\n至于大模型怎么知道去调用这些工具，除了需要微调外，最初的要求是把工具告诉大模型。以下就是一个带工具说明的 system message: \nsystem_message = \"You are an AI assistant. Your primary goal is to provide helpful, precise and clear respones.You have access to the following tools:{tools_description}\"\n\n要使工具调用机制正常运作，我们必须非常准确和明确地定义以下几点：\n\n工具的功能是什么\n它需要什么样的输入\n它会输出什么类型的结果\n\n正因如此，工具的描述通常采用既具表达力又精准的结构，例如编程语言格式或 JSON 结构。 这并不是说必须使用这些格式，任何精确且结构一致的格式都是可行的。\n工具实现 - 功能和描述def calculator(a: int, b: int) -&gt; int:    \"\"\"Multiply two integers.\"\"\"    return a * b\n\n该工具需要以下输入：\n\na（int）：一个整数\nb（int）：一个整数\n\n该工具的输出是一个整数，可以描述如下：\n\n（int）：a 与 b 的乘积。\n\n我们可以用以下字符串来描述这个 tool\nTool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int\n\n但为了更规范化的获取 tool 描述，可以为 tool 包装为 class\nfrom typing import Callableclass Tool:    \"\"\"    A class representing a reusable piece of code (Tool).    Attributes:        name (str): Name of the tool.        description (str): A textual description of what the tool does.        func (callable): The function this tool wraps.        arguments (list): A list of argument.        outputs (str or list): The return type(s) of the wrapped function.    \"\"\"    def __init__(self,                 name: str,                 description: str,                 func: Callable,                 arguments: list,                 outputs: str):        self.name = name        self.description = description        self.func = func        self.arguments = arguments        self.outputs = outputs    def to_string(self) -&gt; str:        \"\"\"        Return a string representation of the tool,        including its name, description, arguments, and outputs.        \"\"\"        args_str = \", \".join([            f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments        ])        return (            f\"Tool Name: {self.name},\"            f\" Description: {self.description},\"            f\" Arguments: {args_str},\"            f\" Outputs: {self.outputs}\"        )    def __call__(self, *args, **kwargs):        \"\"\"        Invoke the underlying function (callable) with provided arguments.        \"\"\"        return self.func(*args, **kwargs)    calculator_tool = Tool(    \"calculator\",                   # name    \"Multiply two integers.\",       # description    calculator,                     # function to call    [(\"a\", \"int\"), (\"b\", \"int\")],   # inputs (names and types)    \"int\",                          # output)calculator_tool.to_string()# Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int'\n\n更好的方式是 python 修饰器，decorator，用修饰器修饰函数，就可实现统一的一些功能\nfrom functools import wrapsimport inspectclass ToolWrapper:    def __init__(self, func):        self.func = func        wraps(func)(self)  # 保留原函数元信息    def __call__(self, *args, **kwargs):        return self.func(*args, **kwargs)    def to_string(self):        # 提取函数签名和文档字符串        sig = inspect.signature(self.func)        doc = inspect.getdoc(self.func) or \"\"        return f\"Tool name: {self.func.__name__}\\nSignature: {sig}\\nDescription: {doc}\"def tool(func):    return ToolWrapper(func)# 示例用法@tooldef calculator(a: int, b: int) -&gt; int:    \"\"\"Multiply two integers.\"\"\"    return a * bprint(calculator.to_string())\n\nTool name: calculatorSignature: (a: int, b: int) -&gt; intDescription: Multiply two integers.\n\n\n\nAgent 周期 Thought-Action-Observation周期概述\nAgent 以**思考（Thought）→ 行动（Action）→ 观察（Observation）**的连续循环方式运行。\n我们来逐步拆解这些步骤：\n\nThought（思考）：Agent 中的大语言模型部分决定下一步应该做什么。\nAction（行动）：Agent 执行一个动作，即调用相应工具并传入对应参数。\nObservation（观察）：模型对工具返回的结果进行反思和处理。\n\n在许多 Agent 框架中，这些规则与流程会直接嵌入到系统提示词（system prompt）中，以确保每一次循环都遵循预设的逻辑。\nsystem_message=\"\"\"You are an AI assistant designed to help users efficiently and accurately. Your primary goal is to provide helpful, precise, and clear responses.You have access to the following tools:Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: intYou should think step by step in order to fulfill the objective with a reasoning divided into Thought/Action/Observation steps that can be repeated multiple times if needed.You should first reflect on the current situation using `Thought: {your_thoughts}`, then (if necessary), call a tool with the proper JSON formatting `Action: {JSON_BLOB}`, or print your final answer starting with the prefix `Final Answer:`\"\"\"\n\n我们可以看到，在 System Message 中我们定义了：\n\nAgent 的行为方式；\nAgent 可使用的工具；\n思考–行动–观察（Thought-Action-Observation）循环，并将其嵌入到了对 LLM 的指令中。\n\n为什么需要 thought*Thought（思考）代表了 Agent 为了解决任务而进行的*内部推理与规划过程。它依赖于 Agent 所用的大语言模型（LLM）在 prompt 中提供的信息上进行*分析与理解。你可以将其看作 Agent 的*内心独白，在其中它思考手头的任务并制定行动策略。\nAgent 的思考部分负责利用当前的观察信息，决定接下来的行动。通过这个过程，Agent 可以：\n\n将复杂问题拆解为更易管理的子任务；\n回顾过往经验；\n根据新信息不断调整其策略。\n\n以下是一些常见的思考类型及示例：\n\n\n\n思考类型\n示例\n\n\n\n规划（Planning）\n“我需要将这个任务分成三步：1）收集数据，2）分析趋势，3）生成报告。”\n\n\n分析（Analysis）\n“根据错误信息来看，问题可能出在数据库连接参数上。”\n\n\n决策（Decision）\n“考虑到用户的预算限制，我应该推荐中档方案。”\n\n\n问题解决（Problem Solving）\n“要优化这段代码，我应该先做性能分析找出瓶颈。”\n\n\n记忆整合（Memory Integration）\n“用户之前提到偏好 Python，所以我将用 Python 举例。”\n\n\n自我反思（Self-Reflection）\n“我上一次的方法效果不好，应该换一种策略尝试。”\n\n\n目标设定（Goal Setting）\n“完成这个任务前，我需要先明确验收标准。”\n\n\n优先级排序（Prioritization）\n“安全漏洞应在添加新功能之前优先修复。”\n\n\n对于专门为函数调用（function-calling）微调过的 LLM，这种“思考过程”并非强制必须的。\n一种关键方法是 ReAct 方法，即将“推理（Reasoning）”与“行动（Acting）”结合。这是一种简单的提示技术：在模型解码前加一句 “Let’s think step by step”（让我们一步步思考）。\n这种方式鼓励模型生成的下一个 token 更倾向于生成一个计划，而不是直接给出最终答案。因为模型被引导去分解问题为子任务，从而使它在处理复杂问题时能更加细致，通常比直接输出最终答案更少出错。\n\nActionAction 类型**Action（行动）*是 AI Agent 用来*与外部环境交互的具体操作步骤。\n无论是上网搜索信息，还是控制某个物理设备，每一个 Action 都是 Agent 有意执行的操作。\n例如，一个客户服务 Agent 可能会执行以下操作：检索客户数据、提供帮助文章，或将问题转交给人工客服。\n\n不同类型的 Agent 在执行 Action 时方式不同：\n\n\n\nAgent 类型\n描述\n\n\n\nJSON Agent\n所需执行的 Action 以 JSON 格式指定。\n\n\nCode Agent\nAgent 编写一段代码，由外部系统解释执行。\n\n\nFunction-calling Agent\n属于 JSON Agent 的一个子类型，经过微调后为每个 Action 生成一个新消息。它是 OpenAI 提供的一种更严格、有组织的格式。\n\n\n\nAction 的用途多种多样，例如：\n\n\n\nAction 类型\n描述\n\n\n\n信息获取（Information Gathering）\n执行网页搜索、查询数据库、检索文档等操作。\n\n\n工具调用（Tool Usage）\n调用 API、执行计算、运行代码等操作。\n\n\n环境交互（Environment Interaction）\n操控数字界面或控制实体设备等。\n\n\n信息沟通（Communication）\n通过聊天与用户互动，或与其他 Agent 协作。\n\n\n\n值得注意的是，LLM 本身只能处理文本，它会使用文本来描述想要执行的 Action 以及所需的参数。\n为了让 Agent 正常运行，LLM 必须在完整生成一个 Action 所需的所有 tokens 后停止解码，然后将控制权交还给 Agent。 这一步至关重要，因为它确保了生成结果是可解析的——无论该格式是 JSON、代码还是函数调用（function-calling）。\n暂停-解析实现 Action 的一个关键方法是 “停止并解析（stop and parse）”机制。这种方法确保 Agent 的输出是结构化且可预测的，主要包括以下三个步骤：\n\n以结构化格式生成输出：Agent 以明确、预先定义的格式（如 JSON 或代码）输出其要执行的动作。\n停止进一步生成：当动作的文本已完全生成后，LLM 停止继续生成 token，以避免多余或错误的输出。\n解析输出内容：由外部的解析器读取结构化的动作文本，确定需要调用哪个工具，并提取所需参数。\n\n例如，一个需要查询天气的 Agent 可能会输出如下内容：\nThought: I need to check the current weather for New York.Action :{  \"action\": \"get_weather\",  \"action_input\": {\"location\": \"New York\"}}\n\n\n另一种方法是使用 Code Agent（代码代理）。其核心思想是：不再输出简单的 JSON 对象，而是由 Code Agent 生成一个可执行的代码块 —— 通常使用像 Python 这样高级的编程语言。\n相比于类似 JSON 的片段，使用代码来编写 Action 具有以下优势：\n\n可组合性（Composability）：你能在 JSON 中嵌套 Action 吗？能像定义 Python 函数那样预定义一组 JSON 操作并重复调用吗？代码可以。\n对象管理能力（Object management）：如果你用 JSON 来执行 generate_image，你该如何存储它的输出？\n通用性（Generality）：代码天生就是为了表达计算机可以做的几乎任何事情。\n训练数据中的表示能力（Representation in LLM training data）：大量高质量的代码样例早已被用于训练 LLM，这意味着模型已经对这类任务具备了良好的理解与生成能力！\n\nObservation**Observation（观察）*是指 Agent 感知其行为后果的方式。 它们为 Agent 的思考过程提供了关键信息，并指引其未来的行动。观察可以看作是来自环境的*反馈信号，无论是 API 返回的数据、错误信息，还是系统日志，都会影响下一轮的思考循环。\n在观察阶段，Agent 会：\n\n收集反馈：接收执行操作后的数据或确认（无论是成功还是失败）；\n追加结果：将新获得的信息整合进当前上下文，相当于更新自身“记忆”；\n调整策略：利用更新后的上下文优化下一步的思考和行动。\n\n这种反馈的循环式融入机制确保了 Agent 始终动态对齐其目标，并能根据真实世界的结果不断学习与调整。\n在执行某个 Action 后，Agent 框架通常依照以下步骤进行：\n\n解析 Action：识别要调用的函数及其参数；\n执行该 Action；\n将执行结果追加为 Observation（观察）。\n\n","categories":["agent"]},{"title":"对比学习在CV与NLP领域的应用：SimCLR、SimCSE 与 SimVLM","url":"/2025/05/18/Multi-modal-series/SimCLR_SimCSE_SimVLM/","content":"对比学习作为一种重要的自监督预训练范式，已在计算机视觉（CV）与自然语言处理（NLP）等多个领域展现出强大能力。本文将聚焦三种经典代表方法：SimCLR、SimCSE 及 SimVLM，探讨其核心思想与技术实现。\nSimCLR: A Simple Framework for Contrastive Learning of Visual Representations\nSimCSE: Simple Contrastive Learning of Sentence Embeddings\nSimVLM: Simple Visual Language Model Pretraining with Weak Supervision\nInfoNCEInfoNCE（Information Noise-Contrastive Estimation）是一种对比损失函数，用于训练通过区分正负样本对来学习表征的模型。其最初被提出用于自监督学习场景中。\n对于一个锚点 (anchor) 样本 ，一个正样本 ，以及一组负样本 ，InfoNCE 损失定义为：𝟙PS：复杂公式渲染比较失败，我把公式在我本地正常渲染的图片也放进来\n\n其中：\n\n：通常为余弦相似度；\n：温度超参数，用于控制分布的尖锐程度；\n𝟙：指示函数，避免样本与自身比较。\n\nInfoNCE 通常默认使用同一批次内的非正样本作为负样本，无需人工标注负对。该损失函数鼓励模型将正样本对拉近，而将负样本对在嵌入空间中推远。\nSimCLR CV领域的对比学习\nWe simplify recently proposed contrastive selfsupervised learning algorithms without requiring specialized architectures or a memory bank.\nwe systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. \n\nSimCLR 是一种简化的对比自监督学习方法，其不依赖于特殊架构或内存机制，并系统性地分析了其关键组成部分。研究表明：\n\n数据增强的组合对定义有效的预测任务至关重要；\n在表征与对比损失之间引入可学习的非线性变换有助于提升学习质量；\n对比学习在大批量和更长训练周期下优于监督学习。\n\n将这些因素结合后，SimCLR 在 ImageNet 上大幅超越了以往的自监督和半监督方法。在该方法学习到的表征基础上训练的线性分类器可达到 76.5% 的 top-1 准确率，比当时的 SOTA 高 7%，甚至可与监督训练的 ResNet-50 相媲美。\n\n\nComposition of multiple data augmentation operations is crucial in defining the contrastive prediction tasks that yield effective representations. In addition, unsupervised contrastive learning benefits from stronger data augmentation than supervised learning.\n\nSimCLR 的核心流程如下：\n\n使用随机数据增强模块生成两种相关视图（作为正样本对），增强操作依次为：随机裁剪+恢复尺寸、颜色扰动、高斯模糊；\n\n使用编码器（如 ResNet）提取表示：\n\n\n\n\n使用两层 MLP 投影头生成对比空间中的嵌入：\n\n\n一个批次包含  个图像，经增强后生成  个视图，构造对比任务。每对正样本的损失如下：𝟙\n其中：\n\n：表示正样本对  的对比损失；\n：表示嵌入向量之间的相似度（例如余弦相似度）；\n：温度系数；\n𝟙：指示函数，当  时为 1，否则为 0；\n：表示总样本数，可能包括原始样本和其增强版本。\n\n\n为了系统地研究数据增强的影响，作者考虑了几种常见的数据增强方式。一类增强属于空间/几何变换，例如裁剪与缩放（含水平翻转）、旋转和 cutout；另一类则属于外观变换，例如颜色扰动（包括去色、亮度、对比度、饱和度和色调变化）、高斯模糊和 Sobel 滤波。\n为了理解单个增强方式的效果以及增强组合的重要性，作者对比了分别使用单一增强和组合增强的模型性能。结果表明，单独的一种变换不足以学习出高质量的表征，尽管模型几乎可以完美识别出正样本对。而当使用组合增强时，尽管对比预测任务变得更加困难，但最终学习到的表征质量却显著提升。注意对角线是单独变换，非对角线是组合变换，最后一列是平均效果，明显对角线都会比平均效果差，代表组合变化对于表征学习更好，虽然会让任务变难。\n\nSimCSE NLP领域的对比学习首先，作者提出了一种无监督方法，该方法将输入句子作为自身的预测目标，并在对比学习目标下仅使用标准的 dropout 作为噪声。实验发现，dropout 起到了最小的数据增强作用，一旦移除 dropout，会导致表征坍缩（representation collapse），即表征的有效性不强了。\n随后，作者提出了一种有监督方法，将自然语言推理（NLI）数据集中的标注句对引入对比学习框架。其中，“蕴含”（entailment）句对被用作正样本，而“矛盾”（contradiction）句对则作为 hard negative。\n在 NLI 任务中，前提句与假设句之间的关系可分为三类：蕴含、矛盾和中性。蕴含表示假设句可以从前提句推出；矛盾表示假设句的否定可以从前提句推出；中性则表示无法判断是否成立或矛盾。SimCSE 利用蕴含对可自然作为正样本的特点，同时发现引入对应的矛盾对作为 hard negatives 可进一步提升性能。作者指出，无监督的 SimCSE 通过 dropout 噪声在避免表征塌缩的同时提升了表征空间的均匀性，从而增强了表示的表达能力。\nUnsupervised SimCSE在对比学习中，一个关键问题是如何构造正样本对。在视觉表征中，一个有效的做法是对同一图像进行两种随机变换（例如裁剪、翻转、扭曲或旋转），然后将其视为正样本对。类似的策略也被引入到了语言表征中，通过词语删除、词序打乱、替换等增强方式构造语言的正样本。然而，由于语言的离散性，NLP 中的数据增强本质上更为困难。在 SimCSE 中，作者发现仅使用标准的 dropout 对中间表征施加扰动，就能优于上述离散增强方法。\n在标准的 Transformer 训练中，dropout mask 被应用于全连接层以及注意力概率上。\n设 ，其中  是一个随机的 dropout mask。作者将同一个输入句子两次送入编码器，在 dropout mask 不同的情况下获得两个不同的句子表示  和 。需要注意的是， 是 Transformer 中原生的 dropout mask，模型中未引入任何额外的 dropout 操作。\n作者将 dropout 视为一种最小形式的数据增强：正样本对使用完全相同的输入句子，其表示之间仅由不同的 dropout mask 引入差异。常见的数据增强方式，如裁剪、词语删除或替换，可以形式化为 ，其中  是作用于输入  的随机离散操作。实验表明，即便仅删除一个词，也会显著降低性能，而没有一种离散增强方式能超过 dropout 噪声的效果。PS：本来 dropout 就是加强模型鲁棒性的，希望即使中间有部分内容随机消失的情况下，也能够正确表述，正好和正样本的概念类似。\n\n作者进一步尝试了不同的 dropout 概率，发现所有变化形式的效果均低于 Transformer 默认的 。其中两个极端情况尤为值得关注：一是完全不使用 dropout（），二是使用默认 ，但在两次编码中采用相同的 dropout mask。在这两种情形下，生成的两个表示完全相同，导致模型性能大幅下降，所以说不用 dropout 就会表征塌陷， dropout 不随机化也会。\n\n作者还在训练过程中每隔 10 步保存一次模型，并可视化 alignment（对齐性）与 uniformity（均匀性）两个指标。结果显示，从预训练参数出发，所有模型都能显著提升均匀性，但上述两个特殊变体的 alignment 指标迅速退化，而无监督 SimCSE 由于引入了 dropout 噪声，保持了良好的对齐性。\nSupervised SimCSE作者已经证明，引入 dropout 噪声能够在正样本对之间保持良好的对齐性。因此，进一步探讨是否可以利用有监督的数据集，提供更强的训练信号以提升对齐质量。以往的研究已表明，自然语言推理（NLI）任务中的有监督数据在学习句子嵌入方面非常有效，其任务目标是判断两个句子之间的关系属于“蕴含”“中性”还是“矛盾”。\n在 SimCSE 的有监督版本中，作者直接使用 NLI 数据集中的蕴含 entailment 对作为正样本，并进一步利用其中的矛盾 contradiction 对作为 hard negative，增强对比学习的难度和效果。\n形式上，将二元组  扩展为三元组 ，其中  表示前提句， 和  分别表示蕴含与矛盾的假设句。其训练目标函数  定义如下（其中  为 mini-batch 的大小）：\n\n引入 hard negative 可以进一步提升模型性能（例如在 STS-B 任务中由 84.9 提升至 86.2），这也构成了最终的有监督 SimCSE 方法。\n如今，许多 RAG（Retrieval-Augmented Generation）系统中的检索器也采用类似的有监督对比学习策略进行训练，使用问题-正向文段对作为正样本，并结合同批次中的其他样本作为负样本——这一方式与 SimCSE 的有监督训练模式非常相似。\n\n\n\n项目\nUnsupervised SimCSE\nSupervised SimCSE\n\n\n\n正样本 (Positive Pairs)\n同一句子经过两次独立 Dropout 的编码表示（即：，其中 ）\n来自 NLI 数据中的蕴含对 ，即前提和蕴含句\n\n\n负样本 (Negative Pairs)\nIn-batch negatives：同一 mini-batch 中其他句子（不同语义）自动构成负样本\nNLI 中的矛盾对  显式作为负样本，构成三元组 \n\n\n训练目标\nInfoNCE 损失，最大化相同句子（不同 dropout）间的相似度，最小化与 batch 中其他句子的相似度\n变种的对比损失，最大化  的相似度，最小化与  的相似度（见你之前发的公式）\n\n\n数据来源\n原始无标签语料（如 Wikipedia）\n有标注的 NLI 数据集（如 MNLI）\n\n\n监督信号\n无\n有（entailment/contradiction）\n\n\n训练稳定性\n对 batch size、dropout 敏感\n更稳定，有明确正负标签\n\n\nSimVLM VLM领域的对比学习随着视觉与文本联合建模技术的不断进展，视觉-语言预训练（Vision-Language Pretraining, VLP）在众多多模态下游任务中已取得显著成果。然而，现有方法通常依赖高成本的标注数据，如干净的图文描述和区域标签，这极大限制了其可扩展性。此外，为适配不同数据集目标而引入的多种任务目标，也使得预训练过程变得更加复杂。\n在 VLP 领域，虽然提出了多种不同方法，但其中很大一部分都需要引入目标检测模型，来回归图像区域特征或进行标签预测，作为预训练任务的一部分。这类方法通常依赖于如 Fast R-CNN 或 Faster R-CNN 等强大的目标检测器，而这些检测器本身也需依赖人工标注数据进行训练。将这类标注数据作为预训练的前置条件，既增加了训练流程的成本，也进一步限制了方法的扩展性。\n尽管近期已有部分研究尝试摆脱目标检测模块，但这类方法多仅依赖于小规模、干净的图文数据，因此其零样本能力受到限制。另一方面，多数方法还提出了跨模态损失函数，但这些损失往往与图像描述生成、掩码语言建模等其他目标混合使用，从而形成复合的预训练损失。这种多任务、多数据集的组合增加了权重平衡与优化的难度。\n因此，早期的视觉-语言预训练方法大致可归为两类：一类通过引入中间的 CV 任务，另一类则构造复杂的跨模态损失。SimVLM 则试图简化这一预训练流程。\n与先前工作不同，SimVLM 利用大规模弱监督图文对，采用单一的前缀语言建模（Prefix Language Modeling）目标实现端到端训练，显著降低训练复杂度。在不依赖额外数据或任务定制的前提下，该模型在多种判别式与生成式视觉-语言任务上超越已有方法，达到了新的 SOTA 水平。\n\n受益于语言建模损失（LM loss）在预训练中所展现出的零样本能力，作者提出使用前缀语言建模（Prefix Language Modeling，PrefixLM）对视觉-语言表征进行预训练。PrefixLM 相较于标准语言建模的区别在于：它允许对前缀序列（如公式中的 ）进行双向注意力建模，而仅对剩余部分（如 ）进行自回归建模。在预训练过程中，会从输入序列中截取一个随机长度为  的前缀，训练目标如下：\n直观来看，图像通常在网页等真实文档中出现在文本之前，因此可将图像视为文本的前缀。对于给定的图文对，模型将图像特征序列（长度为 ）置于文本序列之前，并设定前缀长度 ，使得损失函数仅对文本部分进行语言建模（图 1 中提供了示意）。相比传统的掩码语言建模（MLM）风格的视觉-语言预训练方法，PrefixLM 在序列到序列框架下不仅具备 MLM 式的上下文建模能力，还具备语言模型（LM）的文本生成能力。\n模型主干采用 Transformer 架构，因其在语言与视觉任务中均表现出色。与标准语言模型不同，PrefixLM 允许在前缀内进行双向注意力，因此可兼容仅使用解码器（decoder-only）或编码-解码器（encoder-decoder）的序列到序列模型。实验发现，编码-解码结构所引入的 inductive bias（将编码与生成过程解耦）对于提升下游任务性能具有积极作用。\n在视觉模态方面，受 ViT 启发，模型将输入图像  划分为一维序列形式的图像 patch：，其中  为 Transformer 层的隐藏维度， 是图像分块后的 token 数， 为 patch 大小。模型使用 ResNet 的前三个卷积模块提取上下文相关的图像 patch，这种方式被证明优于 ViT 中使用的简单线性投影（即  卷积），与 Xiao 等人的观察结果一致。\n在文本模态方面，还是和正常模型一样。为了保留位置信息，模型为图像与文本输入分别引入了两个可学习的一维位置编码，并在 Transformer 层中为图像 patch 增加了二维相对注意力机制。\n与以往需要两个预训练阶段和多个辅助目标的 VLP 方法相比，SimVLM 仅需一次性预训练，并在端到端的框架下使用单一的语言建模损失函数。这种设计大大简化了训练流程，因此被命名为“Simple Visual Language Model”（SimVLM，简洁视觉语言模型）。\n","categories":["多模态"],"tags":["多模态","CV","对比学习"]},{"title":"Qwen 2.5 VL 推理","url":"/2025/05/30/Multi-modal-series/qwen2.5_vl_infer/","content":"QWEN2.5-VL框架展示了视觉编码器和语言模型解码器的集成，以处理多模式输入，包括图像和视频。视觉编码器旨在以其本机分辨率处理输入，并支持动态FPS采样。具有不同FPS速率的不同尺寸和视频帧的图像被动态映射到长度不同的token。\n\n加载模型import torchfrom qwen_vl_utils import process_vision_infofrom transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor# default: Load the model on the available device(s)model_name_or_path = \"../DC/Qwen2.5-VL-3B-Instruct\"model = Qwen2_5_VLForConditionalGeneration.from_pretrained(    model_name_or_path,     torch_dtype=\"auto\",     device_map=\"cuda:5\",)processor = Qwen2_5_VLProcessor.from_pretrained(model_name_or_path)# model\n\n推理完整流程接下来会一一讲解推理处理流程。\nmessages = [    {        \"role\": \"user\",        \"content\": [            {                \"type\": \"image\",                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",            },            {\"type\": \"text\", \"text\": \"Describe this image.\"},        ],    }]# Preparation for inferencetext = processor.apply_chat_template(    messages, tokenize=False, add_generation_prompt=True)# 按出现顺序将图像和视频信息提取到列表中image_inputs, video_inputs = process_vision_info(messages)inputs = processor(    text=[text],    images=image_inputs,    videos=video_inputs,    padding=True,    return_tensors=\"pt\",)inputs = inputs.to(model.device)# Inference: Generation of the outputgenerated_ids = model.generate(**inputs, max_new_tokens=128)generated_ids_trimmed = [    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]output_text = processor.batch_decode(    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)print(output_text)\n\nQwen2_5_VL 的模型组成如下: Qwen2_5_VisionTransformerPretrainedModel（vision encoder）和 Qwen2_5_VLModel（LM decoder）。vision encoder 还是 ViT, patch embedding 由卷积层实现, 旋转位置编码, 之后就是正常 transformer Block 的叠加, 最后 merger 就是将 vision embedding 投影到 text modal 的 projecter, qwen2.5 vl 用的只是两层 MLP, 注意下 shape, 明显 block 的输出是 1280, merger.mlp 的输入是 5120, 这是因为将相邻的 2*2 个 token 进行融合。\nQwen2_5_VLForConditionalGeneration(  (visual): Qwen2_5_VisionTransformerPretrainedModel(    (patch_embed): Qwen2_5_VisionPatchEmbed(      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)    )    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()    (blocks): ModuleList(      (0-31): 32 x Qwen2_5_VLVisionBlock(        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)        (attn): Qwen2_5_VLVisionSdpaAttention(          (qkv): Linear(in_features=1280, out_features=3840, bias=True)          (proj): Linear(in_features=1280, out_features=1280, bias=True)        )        (mlp): Qwen2_5_VLMLP(          (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)          (up_proj): Linear(in_features=1280, out_features=3420, bias=True)          (down_proj): Linear(in_features=3420, out_features=1280, bias=True)          (act_fn): SiLU()        )      )    )    (merger): Qwen2_5_VLPatchMerger(      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)      (mlp): Sequential(        (0): Linear(in_features=5120, out_features=5120, bias=True)        (1): GELU(approximate='none')        (2): Linear(in_features=5120, out_features=2048, bias=True)      )    )  )  (model): Qwen2_5_VLModel(    (embed_tokens): Embedding(151936, 2048)    (layers): ModuleList(      (0-35): 36 x Qwen2_5_VLDecoderLayer(        (self_attn): Qwen2_5_VLSdpaAttention(          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)          (k_proj): Linear(in_features=2048, out_features=256, bias=True)          (v_proj): Linear(in_features=2048, out_features=256, bias=True)          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)          (rotary_emb): Qwen2_5_VLRotaryEmbedding()        )        (mlp): Qwen2MLP(          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)          (act_fn): SiLU()        )        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)      )    )    (norm): Qwen2RMSNorm((2048,), eps=1e-06)    (rotary_emb): Qwen2_5_VLRotaryEmbedding()  )  (lm_head): Linear(in_features=2048, out_features=151936, bias=False))\n\n\n\n\n预处理message 文本预处理messages = [    {        \"role\": \"user\",        \"content\": [            {                \"type\": \"image\",                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",            },            {\"type\": \"text\", \"text\": \"Describe this image.\"},        ],    }]# Preparation for inferencetext = processor.apply_chat_template(    messages, tokenize=False, add_generation_prompt=True)print(text)\n\n&lt;|im_start|&gt;systemYou are a helpful assistant.&lt;|im_end|&gt;&lt;|im_start|&gt;user&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;Describe this image.&lt;|im_end|&gt;&lt;|im_start|&gt;assistant\n\n输入文本被处理成对话模板的样式，图片或者视频暂时被 &lt;|image_pad|&gt; 这样的占位符替代。\n图片 resize# 按出现顺序将图像和视频信息提取到列表中image_inputs, video_inputs = process_vision_info(messages)image_inputs\n\n内部执行 fetch_image() 和 fetch_video() 函数获取图片和视频，期间还会执行 resize\nresized_height, resized_width = smart_resize(    height,    width,    factor=size_factor,    min_pixels=min_pixels,    max_pixels=max_pixels,)image = image.resize((resized_width, resized_height))\n\nsmart_resize() 只要获得新的图片 resize 尺寸，使其满足以下条件：\n\n高度和宽度都能被指定的“factor”整除；\n图像的总像素数处于[min_pixels, max_pixels]的范围内；\n尽可能保持图像的原始宽高比不变。\n\nIMAGE_FACTOR = 28MIN_PIXELS = 4 * 28 * 28MAX_PIXELS = 16384 * 28 * 28MAX_RATIO = 200def smart_resize(    height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS) -&gt; tuple[int, int]:    \"\"\"    Rescales the image so that the following conditions are met:    1. Both dimensions (height and width) are divisible by 'factor'.    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].    3. The aspect ratio of the image is maintained as closely as possible.    \"\"\"    if max(height, width) / min(height, width) &gt; MAX_RATIO:        raise ValueError(            f\"absolute aspect ratio must be smaller than {MAX_RATIO}, got {max(height, width) / min(height, width)}\"        )    h_bar = max(factor, round_by_factor(height, factor))    w_bar = max(factor, round_by_factor(width, factor))    if h_bar * w_bar &gt; max_pixels:        beta = math.sqrt((height * width) / max_pixels)        h_bar = floor_by_factor(height / beta, factor)        w_bar = floor_by_factor(width / beta, factor)    elif h_bar * w_bar &lt; min_pixels:        beta = math.sqrt(min_pixels / (height * width))        h_bar = ceil_by_factor(height * beta, factor)        w_bar = ceil_by_factor(width * beta, factor)    return h_bar, w_bardef round_by_factor(number: int, factor: int) -&gt; int:    \"\"\"Returns the closest integer to 'number' that is divisible by 'factor'.\"\"\"    return round(number / factor) * factordef ceil_by_factor(number: int, factor: int) -&gt; int:    \"\"\"Returns the smallest integer greater than or equal to 'number' that is divisible by 'factor'.\"\"\"    return math.ceil(number / factor) * factordef floor_by_factor(number: int, factor: int) -&gt; int:    \"\"\"Returns the largest integer less than or equal to 'number' that is divisible by 'factor'.\"\"\"    return math.floor(number / factor) * factor\n\nfactor 是 28, 是 patch size (14) 的两倍，由于 qwen-vl 要把相邻 2*2 的 patch 合并。主要的关键操作，就是把图片的尺寸进行调整。\n\nFurthermore, to reduce the visual tokens of each image, a simple MLP layer is employed after the ViT to compress adjacent 2 × 2 tokens into a single token\n\n上面代码返回\n[&lt;PIL.Image.Image image mode=RGB size=2044x1372&gt;]\n2044 和 1372 都是可以被 28 整除的。\ntext &amp; image 模态预处理inputs = processor(    text=[text],    images=image_inputs,    videos=video_inputs,    padding=True,    return_tensors=\"pt\",)inputs = inputs.to(model.device)for k, v in inputs.items():    if isinstance(v, torch.Tensor):        print(f\"{k}: {v.shape} {v.dtype}\")\n\n输出是这样的\ninput_ids: torch.Size([1, 3602]) torch.int64attention_mask: torch.Size([1, 3602]) torch.int64pixel_values: torch.Size([14308, 1176]) torch.float32image_grid_thw: torch.Size([1, 3]) torch.int64\n\n首先 processor.__call__() 会执行图片的预处理 image_processor.__call__()\nif images is not None:    image_inputs = self.image_processor(images=images, videos=None, **output_kwargs[\"images_kwargs\"])    image_grid_thw = image_inputs[\"image_grid_thw\"]else:    image_inputs = {}    image_grid_thw = Noneimage_processor` 不可跳转，可以打印 `processor.image_processor` 来显示类 `Qwen2VLImageProcessorQwen2VLImageProcessor {  \"do_convert_rgb\": true,  \"do_normalize\": true,  \"do_rescale\": true,  \"do_resize\": true,  \"image_mean\": [    0.48145466,    0.4578275,    0.40821073  ],  \"image_processor_type\": \"Qwen2VLImageProcessor\",  \"image_std\": [    0.26862954,    0.26130258,    0.27577711  ],  \"max_pixels\": 12845056,  \"merge_size\": 2,  \"min_pixels\": 3136,  \"patch_size\": 14,  \"processor_class\": \"Qwen2_5_VLProcessor\",  \"resample\": 3,  \"rescale_factor\": 0.00392156862745098,  \"size\": {    \"longest_edge\": 12845056,    \"shortest_edge\": 3136  },  \"temporal_patch_size\": 2}\nimage_processor.__call__() 内部也是一大堆处理，首先是正常的处理图片。\nimages = make_list_of_images(images)if do_convert_rgb:    images = [convert_to_rgb(image) for image in images]# All transformations expect numpy arrays.images = [to_numpy_array(image) for image in images]height, width = get_image_size(images[0], channel_dim=input_data_format)resized_height, resized_width = height, widthprocessed_images = []for image in images:    if do_resize:        resized_height, resized_width = smart_resize(            height,            width,            factor=patch_size * merge_size,            min_pixels=size[\"shortest_edge\"],            max_pixels=size[\"longest_edge\"],        )        image = resize(            image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format        )    if do_rescale:        image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)    if do_normalize:        image = self.normalize(            image=image, mean=image_mean, std=image_std, input_data_format=input_data_format        )    image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)    processed_images.append(image)\n\nprocessed_images 形状是 [(3, 1372, 2044)], 接下来\n# temporal_patch_size = 2patches = np.array(processed_images)if data_format == ChannelDimension.LAST:    patches = patches.transpose(0, 3, 1, 2)if patches.shape[0] % temporal_patch_size != 0:    repeats = np.repeat(patches[-1][np.newaxis], temporal_patch_size - 1, axis=0)    patches = np.concatenate([patches, repeats], axis=0)\n这段代码先将所有图片变成一个numpy array，然后判断第一维是否为 temporal_patch_size 的倍数， 不是的话复制最后一个 patches 元素 patches[-1]， 并拼接回原数组。这是因为Qwen2-VL把视频当作一秒两帧的图片集合，为了统一框架，需要把图片复制成两个相同的帧（相当于图片 —&gt; 一秒钟视频）。\n\nTo preserve video information as completely as possible, we sampled each video at two frames per second. Additionally, we integrated 3D convolutions (Carreira and Zisserman, 2017) with a depth of two to process video inputs, allowing the model to handle 3D tubes instead of 2D patches, thus enabling it to process more video frames without increasing the sequence length (Arnab et al., 2021). For consistency, each image is treated as two identical frames.\n\n之后就是 reshape 为真正以上的 patch。每一个patch的大小是(channel * self.temporal_patch_size * self.patch_size * self.patch_size)，temporal_patch_size 默认为 2，channel 也不参与划分，所以相当于对于图片来说，patchify 只在 h 和 w 维度进行。\ngrid_t = patches.shape[0] // temporal_patch_sizegrid_h, grid_w = resized_height // patch_size, resized_width // patch_sizepatches = patches.reshape(    grid_t,    temporal_patch_size,    channel,    grid_h // merge_size,    merge_size,    patch_size,    grid_w // merge_size,    merge_size,    patch_size,)patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)flatten_patches = patches.reshape(    grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size)return flatten_patches, (grid_t, grid_h, grid_w)\n\nimage_grid_thw = image_inputs[\"image_grid_thw\"] 即 (grid_t, grid_h, grid_w) or [1, 98, 146]，即 patch 在时序、高度、宽度上的数量。1372/14=98, 2044/14=146\n接下来就是用特殊符号 &lt;|placeholder|&gt; 来暂时占位，原先 self.image_token 即 &lt;|image_pad|&gt; 只有一个，需要为实际的 image patch embedding 来预留位置。\nif image_grid_thw is not None:merge_length = self.image_processor.merge_size**2index = 0for i in range(len(text)):    while self.image_token in text[i]:        text[i] = text[i].replace(            self.image_token,            \"&lt;|placeholder|&gt;\" * (image_grid_thw[index].prod() // merge_length),            1,        )        index += 1    text[i] = text[i].replace(\"&lt;|placeholder|&gt;\", self.image_token)text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\nmerge_length 就是根据将相邻 self.image_processor.merge_size * merge_size 的 patch 组合的每组的 patch 数量。这里最后还是类似纯文本模型的 tokenize，结果是 &lt;|im_start|&gt;system\\nYou are a helpful assistant.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n&lt;|vision_start|&gt;&lt;|image_pad|&gt;...&lt;|image_pad|&gt;&lt;|vision_end|&gt;Describe this image.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n 我把中间的 &lt;|image_pad|&gt; 省略了。\n输入的 [3, 2044, 1372] 维的图片变成了 [14308, 1176] 的 pixel_value，最终输入语言模型的视觉 token 数是 14308/4=3577, 其实就是原本的大小除以 28x28， 即(1372/14) x (2044/14) = 98x146 = 14308。而 1176 = 3x14x14x2 (channel x temporal_patch_size x patch_size x .patch_size), 是 3D 卷积的处理。\n# 建议打断点执行这个代码片段，查看内部数据generated_ids = model.generate(**inputs, max_new_tokens=128)\n\nimage encoder 前向过程以下是 Qwen2_5_VLForConditionalGeneration.forward() 的的一部分内容:\n# 将 input_ids 转换为 embeddinginputs_embeds = self.model.embed_tokens(input_ids)if pixel_values is not None:    # 转换 image 的 dtype    pixel_values = pixel_values.type(self.visual.dtype)    # image encoder 输出 embedding     image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)    n_image_tokens = (input_ids == self.config.image_token_id).sum().item()    n_image_features = image_embeds.shape[0]    if n_image_tokens != n_image_features:        raise ValueError(            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"        )    mask = input_ids == self.config.image_token_id    mask_unsqueezed = mask.unsqueeze(-1)    mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)    image_mask = mask_expanded.to(inputs_embeds.device)    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)  \nself.visual 和 self.model 分别为视觉和文本的模型。 self.visual(pixel_values, grid_thw=image_grid_thw) 转入Qwen2_5_VisionTransformerPretrainedModel.forward()。\n# embedding [14308, 1176] -&gt; [14308, 1280]hidden_states = self.patch_embed(hidden_states)# 传入 patch 的三维长度 grid_thw 是为了计算位置编码rotary_pos_emb = self.rot_pos_emb(grid_thw)window_index, cu_window_seqlens = self.get_window_index(grid_thw)cu_window_seqlens = torch.tensor(    cu_window_seqlens,    device=hidden_states.device,    dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,)cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)seq_len, _ = hidden_states.size()hidden_states = hidden_states.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)hidden_states = hidden_states[window_index, :, :]hidden_states = hidden_states.reshape(seq_len, -1)rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)rotary_pos_emb = rotary_pos_emb[window_index, :, :]rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)position_embeddings = (emb.cos(), emb.sin())cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(    dim=0,    # Select dtype based on the following factors:    #  - FA2 requires that cu_seqlens_q must have dtype int32    #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw    # See https://github.com/huggingface/transformers/pull/34852 for more information    dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,)cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)for layer_num, blk in enumerate(self.blocks):    if layer_num in self.fullatt_block_indexes:        cu_seqlens_now = cu_seqlens    else:        cu_seqlens_now = cu_window_seqlens    if self.gradient_checkpointing and self.training:        hidden_states = self._gradient_checkpointing_func(            blk.__call__, hidden_states, cu_seqlens_now, None, position_embeddings        )    else:        hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)hidden_states = self.merger(hidden_states)reverse_indices = torch.argsort(window_index)hidden_states = hidden_states[reverse_indices, :]return hidden_states\nhidden_states 在正常 transformer block 前向传播后形状为 [14308, 1280], 经过 merger (两层的 MLP), 期间通过 reshape 变成了 [3577, 5120], 经过 MLP 后变成了 [3577, 2048]，embedding 维度就和文本一致了。\nclass Qwen2_5_VLPatchMerger(nn.Module):    def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -&gt; None:        super().__init__()        # 计算 reshape 尺寸，需要将 spatial_merge_size * spatial_merge_size 个维度为 context_dim 的 patch embedding 重新计算为一个 virtual token        self.hidden_size = context_dim * (spatial_merge_size**2)         self.ln_q = Qwen2RMSNorm(context_dim, eps=1e-6)        self.mlp = nn.Sequential(            nn.Linear(self.hidden_size, self.hidden_size),            nn.GELU(),            nn.Linear(self.hidden_size, dim),        )    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:        x = self.mlp(self.ln_q(x).view(-1, self.hidden_size))        return x\n\n接着返回是 Qwen2_5_VLForConditionalGeneration.forward() 的代码:\n# 将 input_ids 转换为 embeddinginputs_embeds = self.model.embed_tokens(input_ids)if pixel_values is not None:    # 转换 image 的 dtype    pixel_values = pixel_values.type(self.visual.dtype)    # image encoder 输出 embedding     image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)    # 检查 image token 的数量    n_image_tokens = (input_ids == self.config.image_token_id).sum().item()    # 判断 image token 数量是否和实际 image embedding 的维度一样    n_image_features = image_embeds.shape[0]    if n_image_tokens != n_image_features:        raise ValueError(            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"        )    # 创建 mask，原始 mask 只是在 seq len 维度上和数据对齐，需要分别在 batch 和 embedding 维度上对齐，最后 mask_expanded.shape == inputs_embeds.shape    mask = input_ids == self.config.image_token_id    mask_unsqueezed = mask.unsqueeze(-1)    mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)    image_mask = mask_expanded.to(inputs_embeds.device)    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)  \n在 image_embeds 计算完后，检查预留的 image token 数量是否和计算的 image_embeds 数量相同 (预处理时根据 image size 预留了 image token)，之后创建一个和 inputs_embeds 形状完全一样的 mask(mask_expanded), 之后就将 image_embeds 赋值到对应位置，这样 text embedding 和 image embedding 正式结合在 unimodal latent space 里，其实就是 text embedding space。之后就是正常的 forward。\n以下时 3D-rope 的计算过程，不是本文重点所以不讲了。\nif position_ids is None and (attention_mask is None or attention_mask.ndim == 2):    # calculate RoPE index once per generation in the pre-fill stage only    if (        (cache_position is not None and cache_position[0] == 0)        or self.rope_deltas is None        or (past_key_values is None or past_key_values.get_seq_length() == 0)    ):        position_ids, rope_deltas = self.get_rope_index(            input_ids,            image_grid_thw,            video_grid_thw,            second_per_grid_ts,            attention_mask,        )        self.rope_deltas = rope_deltas    # then use the prev pre-calculated rope-deltas to get the correct position ids    else:        batch_size, seq_length, _ = inputs_embeds.shape        delta = (            (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)            if cache_position is not None            else 0        )        position_ids = torch.arange(seq_length, device=inputs_embeds.device)        position_ids = position_ids.view(1, -1).expand(batch_size, -1)        if cache_position is not None:  # otherwise `deltas` is an int `0`            delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)        position_ids = position_ids.add(delta)        position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)outputs = self.model(    input_ids=None,    position_ids=position_ids,    attention_mask=attention_mask,    past_key_values=past_key_values,    inputs_embeds=inputs_embeds,    use_cache=use_cache,    output_attentions=output_attentions,    output_hidden_states=output_hidden_states,    return_dict=return_dict,    cache_position=cache_position,)\n\n之后就是经典的 shift label 交叉熵。\nhidden_states = outputs[0]logits = self.lm_head(hidden_states)loss = Noneif labels is not None:    # Upcast to float if we need to compute the loss to avoid potential precision issues    logits = logits.float()    # Shift so that tokens &lt; n predict n    shift_logits = logits[..., :-1, :].contiguous()    shift_labels = labels[..., 1:].contiguous()    # Flatten the tokens    loss_fct = CrossEntropyLoss()    shift_logits = shift_logits.view(-1, self.config.vocab_size)    shift_labels = shift_labels.view(-1)    # Enable model parallelism    shift_labels = shift_labels.to(shift_logits.device)    loss = loss_fct(shift_logits, shift_labels)if not return_dict:    output = (logits,) + outputs[1:]    return (loss,) + output if loss is not None else outputreturn Qwen2_5_VLCausalLMOutputWithPast(    loss=loss,    logits=logits,    past_key_values=outputs.past_key_values,    hidden_states=outputs.hidden_states,    attentions=outputs.attentions,    rope_deltas=self.rope_deltas,)\n\n后处理 - 解码 token idgenerated_ids_trimmed = [    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]output_text = processor.batch_decode(    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)print(output_text)\n\n因为上述代码都是 batch 输入的，generated_ids_trimmed 简而言之就是把 input_ids 截断，保留新产生的 token id。\n['The image depicts a serene beach scene with a person and a dog. The person is sitting on the sandy beach, facing the ocean. They are wearing a plaid shirt and black pants, and they have long hair. The dog, which appears to be a Labrador Retriever, is sitting on the sand and is interacting with the person by placing its paw on their hand. The dog is wearing a harness with a colorful collar. The background shows the ocean with gentle waves, and the sky is clear with a soft light, suggesting it might be early morning or late afternoon. The overall atmosphere of the image is peaceful and joyful.']","categories":["多模态"],"tags":["多模态"]},{"title":"多轮对话数据微调 qwen","url":"/2025/06/19/LLM-basic-series/finetune_qwen_conversation_dataset/","content":"多轮数据训练可以让模型学会在连续对话中理解上下文、保持对话连贯性和角色一致性。相比直接生成（单轮问答）, 多轮训练能让模型更好地处理复杂对话场景, 实现更自然的人机交互。直接生成只关注单次提问和回答, 无法捕捉对话历史信息。\n以下以 qwen2.5-3b-instruct 在一个心理辅导数据集上以 messages 格式训练。\n环境搭建export HF_ENDPOINT=https://hf-mirror.comhuggingface-cli download Qwen/Qwen2.5-3B-Instruct --local-dir DC/qwen2.5-3B-ins --resume-download\n\n\nimport os, json, torchfrom torch.utils.data import Datasetfrom typing import Dict, Optional, Listfrom transformers import AutoModelForCausalLM, AutoTokenizeros.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\n\nmodel_name_or_path = \"DC/qwen2.5-3B-ins\"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained(\tmodel_name_or_path, \tdevice_map=\"auto\",     torch_dtype=\"auto\",\ttrust_remote_code=True)print(model.dtype)\n\n最好指定下 torch_dtype=”auto”, 不然模型会以 fp32 精度加载。\n推理测试 - 直接生成和 chat 生成# 直接生成input = \"What is the capital of France?\"inputs = tokenizer(input, return_tensors=\"pt\").to(model.device)outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.7, top_p=0.9)output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)print(output_text)\n\n\n# chat 生成messages = [\t{\t\t\"role\": \"system\",\t\t\"content\": \"You are a helpful assistant.\"\t},\t{\t\t\"role\": \"user\",\t\t\"content\": \"What is the capital of France?\"\t}]# 使用 tokenizer.apply_chat_templat e来处理对话消息 messages, 只对有 chat template 的模型有效# 一般会将 messages 内容加上角色的标识, add_generation_prompt 是指加上属于模型的标识, 这个其实不需要模型生成, 直接加了会起到提示大模型该轮到你说了inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)inputs = tokenizer(inputs, return_tensors=\"pt\").to(model.device)outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.9)output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)print(output_text)\n\n\n\n直接生成:\nWhat is the capital of France? The capital of France is Paris. Paris is a beautiful city known for its art, fashion, food, and culture. It is also home to many famous landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum\n\nchat 格式生成:\n&lt;|im_start|&gt;systemYou are a helpful assistant.&lt;|im_end|&gt;&lt;|im_start|&gt;userWhat is the capital of France?&lt;|im_end|&gt;&lt;|im_start|&gt;assistantThe capital of France is Paris.&lt;|im_end|&gt;\n\n区别在于特殊的角色标识符号, 可以更显式的区别人与模型的内容, 可以人与模型的内容交叉实现对话的形式。\n加载数据集from datasets import load_datasetds = load_dataset(\"Amod/mental_health_counseling_conversations\")print(ds)print(json.dumps(ds[\"train\"][0], indent=2, ensure_ascii=False))\n\nDatasetDict({\ttrain: Dataset({\t\tfeatures: ['Context', 'Response'],\t\tnum_rows: 3512\t})}){\t\"Context\": \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\",\t\"Response\": \"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media.  Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living.  They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible.   Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\"}\n\n\n将数据格式化为 openai 格式的 messages将原始的心理健康咨询对话数据集（ds）中的每条数据转换为对话格式（message_format_ds）, 每条数据包含一组 user-assistant 消息, 便于后续用于对话模型的训练或推理。\ndef process_into_chat_format(example):\tmessages = []\tmessages.append({\t\t\"role\": \"system\", \t\t\"content\": \"You are a mental health counselor to help those who are suffering from a number of disorders including anxiety or depression..\"\t})\tmessages.append({\t\t\"role\": \"user\", \t\t\"content\": example[\"Context\"].replace(\"\\xa0\", \" \")\t})\tmessages.append({\t\t\"role\": \"assistant\", \t\t\"content\": example[\"Response\"].replace(\"\\xa0\", \" \")\t})\treturn {\"conversations\": messages}message_format_ds = ds.map(\tprocess_into_chat_format, \tremove_columns=[\"Context\", \"Response\"])message_format_ds = message_format_ds['train'].train_test_split(test_size=0.1, seed=42)print(message_format_ds)print(json.dumps(message_format_ds['train'][0], indent=2, ensure_ascii=False))\n\n\nDatasetDict({\ttrain: Dataset({\t\tfeatures: ['conversations'],\t\tnum_rows: 3160\t})\ttest: Dataset({\t\tfeatures: ['conversations'],\t\tnum_rows: 352\t})}){\t\"conversations\": [\t{\t\t\"content\": \"You are a mental health counselor to help those who are suffering from a number of disorders including anxiety or depression..\",\t\t\"role\": \"system\"\t},\t{\t\t\"content\": \"I just took a job that requires me to travel far away from home. My family and I really need this job.\\n   People keep telling me I have \\\"anxiety\\\" and I'm terrified of having an anxiety attack on the road. This is all new to me. What can I do?\",\t\t\"role\": \"user\"\t},\t{\t\t\"content\": \"It is ok to have anxiety.   Please don't be anxious about being anxious.If you feel anxiety coming over you, then pull off the road to a safe place.   Concentrate on centering yourself and to breath slowly.   Take some sips of water.  Sit still.     The anxiety should pass in about twenty minutes.If it does not pass, then continue calming yourself until you feel safe enough to drive to your hotel.     You can always explain to your supervisor that you were taking care of a medical problem, because anxiety is a medical problem.\",\t\t\"role\": \"assistant\"\t}\t]}\n\n\n\ntokenize 对话tokenizer.apply_chat_template 用在推理时时方便的, 但在转换训练数据时需要对不同角色的conntent和特殊符号分别处理, 以下的函数是基于 qwen template 设计的。\ndef preprocess_openai_messages_qwen_format(\tmessages: List[Dict[str, str]],\ttokenizer: AutoTokenizer,\tmax_length: int = 2048) -&gt; Dict[str, List[int]]:\t\"\"\"\t将对话数据集转换为适用于 Qwen 格式的输入特征, 包括 input_ids、labels 和 attention_mask, 便于后续微调模型。并提供解码函数, 方便检查预处理结果的正确性。\t和非 chat 数据的区别在于需要注意 chat 格式和 label 仅为 assistant 内容。\t\"\"\"\tinput_ids = []\tlabels = []\tfor msg in messages:\t\trole = msg[\"role\"]\t\tcontent = msg[\"content\"]\t\t# 1. &lt;|im_start|&gt;{role}\\n → 不训练\t\tprefix = f\"&lt;|im_start|&gt;{role}\\n\"\t\tprefix_ids = tokenizer(prefix, add_special_tokens=False)[\"input_ids\"]\t\tinput_ids.extend(prefix_ids)\t\tlabels.extend([-100] * len(prefix_ids))\t\t# 2. content → assistant 才训练\t\tcontent_ids = tokenizer(content, add_special_tokens=False)[\"input_ids\"]\t\tinput_ids.extend(content_ids)\t\tif role == \"assistant\":\t\t\tlabels.extend(content_ids)\t\telse:\t\t\tlabels.extend([-100] * len(content_ids))\t\t# 3. &lt;|im_end|&gt; → 仅 assistant 时参与训练\t\tsuffix = \"&lt;|im_end|&gt;\"\t\tsuffix_ids = tokenizer(suffix, add_special_tokens=False)[\"input_ids\"]\t\tinput_ids.extend(suffix_ids)\t\tif role == \"assistant\":\t\t\tlabels.extend(suffix_ids)\t\telse:\t\t\tlabels.extend([-100] * len(suffix_ids))\t\t# 4. 添加换行符\t\tinput_ids.extend(tokenizer('\\n', add_special_tokens=False)[\"input_ids\"])\t\tlabels.append(-100)\tassert len(input_ids) == len(labels), \"Input IDs and labels must have the same length.\"\t# 截断\tinput_ids = input_ids[:max_length]\tlabels = labels[:max_length]\tattention_mask = [1] * len(input_ids)\treturn {\t\t\"input_ids\": input_ids,\t\t\"labels\": labels,\t\t\"attention_mask\": attention_mask\t}def decode_labels(labels: List[int], tokenizer: AutoTokenizer) -&gt; str:\t# 将 labels 中连续的非 -100 段分别 decode, 并用特殊分隔符拼接\tsegments = []\tcurrent = []\tfor t in labels:\t\tif t != -100:\t\t\tcurrent.append(t)\t\telse:\t\t\tif current:\t\t\t\tsegments.append(tokenizer.decode(current, skip_special_tokens=False))\t\t\t\tcurrent = []\tif current:\t\tsegments.append(tokenizer.decode(current, skip_special_tokens=False))\treturn segmentsmessages = [\t{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\t{\"role\": \"user\", \"content\": \"What is the capital of France?\"},\t{\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\t{\"role\": \"user\", \"content\": \"What is the capital of Germany?\"},\t{\"role\": \"assistant\", \"content\": \"The capital of Germany is Berlin.\"}]sample = preprocess_openai_messages_qwen_format(messages, tokenizer)print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))# 打印 input_ids 解码后内容print(\"Decoded input:\\n{}\".format(tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=False)))# 打印 labels 解码后内容（只显示参与训练的内容）print(\"Decoded labels:\\n{}\".format(decode_labels(sample[\"labels\"], tokenizer)))\n\ntokenizer.apply_chat_template:\n&lt;|im_start|&gt;systemYou are a helpful assistant.&lt;|im_end|&gt;&lt;|im_start|&gt;userWhat is the capital of France?&lt;|im_end|&gt;&lt;|im_start|&gt;assistantThe capital of France is Paris.&lt;|im_end|&gt;&lt;|im_start|&gt;userWhat is the capital of Germany?&lt;|im_end|&gt;&lt;|im_start|&gt;assistantThe capital of Germany is Berlin.&lt;|im_end|&gt;\n\nDecoded input:\nDecoded input:&lt;|im_start|&gt;systemYou are a helpful assistant.&lt;|im_end|&gt;&lt;|im_start|&gt;userWhat is the capital of France?&lt;|im_end|&gt;&lt;|im_start|&gt;assistantThe capital of France is Paris.&lt;|im_end|&gt;&lt;|im_start|&gt;userWhat is the capital of Germany?&lt;|im_end|&gt;&lt;|im_start|&gt;assistantThe capital of Germany is Berlin.&lt;|im_end|&gt;Decoded labels:['The capital of France is Paris.&lt;|im_end|&gt;', 'The capital of Germany is Berlin.&lt;|im_end|&gt;']\nDecoded labels:[‘The capital of France is Paris.&lt;|im_end|&gt;’, ‘The capital of Germany is Berlin.&lt;|im_end|&gt;’]\nlabels中非-100的只有\ndef wrapped_preprocess(example, tokenizer, max_length=2048):\t# batched=True: example[\"conversations\"] is a list of conversations\tconversations_list = example[\"conversations\"]\tresults = preprocess_openai_messages_qwen_format(example[\"conversations\"], tokenizer, max_length)\treturn resultsinput_ds = message_format_ds.map(\twrapped_preprocess,\tremove_columns=[\"conversations\"],\tdesc=\"Processing training dataset\",\tfn_kwargs={\"tokenizer\": tokenizer, \"max_length\": 2048} # max_length=8192 时会OOM, 原因时有两个数据太长了, 一般数据都在1k以下)\n\n这个 cell 是可选的, 静态 bucketing, 在 batch 内排序以减小 padding, 下面的结果都是没有执行静态 bucketing 的结果。\ninput_ds = input_ds.map(lambda x: {\"length\": len(x[\"input_ids\"])}, desc=\"Calculating input length\")input_ds = input_ds.sort(\"length\")  # 排序！\n\n\nfrom transformers import DataCollatorForSeq2Seqdata_collator = DataCollatorForSeq2Seq(\ttokenizer=tokenizer,\treturn_tensors=\"pt\",\tpadding=True,)samples = [input_ds['train'][i] for i in range(3)]batch = data_collator(samples)for key, value in batch.items():\tprint(f\"{key}: {value.shape}\")\n\ninput_ids: torch.Size([3, 516])attention_mask: torch.Size([3, 516])labels: torch.Size([3, 516])\n检查 batch 长度防止出现过长的数据, 导致突然 OOM\nimport torchfrom torch.utils.data import DataLoaderimport matplotlib.pyplot as pltfrom tqdm import tqdmdef plot_batch_lengths(dataset, data_collator, batch_size=1, title=\"Batch Token Lengths\"):    dataloader = DataLoader(        dataset,        batch_size=batch_size,        collate_fn=data_collator    )    batch_lengths = []    for batch in tqdm(dataloader, desc=\"Analyzing batches\"):        input_ids = batch[\"input_ids\"]        # 如果是多条样本拼成的 batch, 取最长的那条（最大长度）        if isinstance(input_ids, torch.Tensor):            length = input_ids.shape[1]        else:            # 防止出现 List[List[int]]            length = max(len(seq) for seq in input_ids)        batch_lengths.append(length)    # 绘图    plt.figure(figsize=(12, 4))    plt.plot(batch_lengths, marker='o', markersize=2, linewidth=0.8)    plt.xlabel(\"Batch Index (Step)\")    plt.ylabel(\"Token Length\")    plt.title(title)    plt.grid(True)    plt.tight_layout()    plt.show()    return batch_lengths# 使用方法batch_lengths = plot_batch_lengths(    dataset=input_ds['train'],    data_collator=data_collator,    batch_size=1,    title=\"Token Length per Batch in Training Dataset\")\n\n\n\n\n之前用 max_length=8192 还是过于看得起 4090 了, 有几个数据很长, 大约在 56 step 时就会遇到超长数据, 会突然 OOM。因此最终改 max_length=1024。\n\npeft模型定义from peft import (    LoraConfig,\tTaskType,    get_peft_model,)lora_config = LoraConfig(\ttask_type=TaskType.CAUSAL_LM, \ttarget_modules=['q_proj', 'v_proj'], \tr=16, \tlora_alpha=16)peft_model = get_peft_model(model, lora_config)peft_model.print_trainable_parameters()\n\ntrainable params: 3,686,400 || all params: 3,089,625,088 || trainable%: 0.1193\n模型训练from transformers import Trainer, TrainingArgumentstraining_args = TrainingArguments(\toutput_dir=\"./lora-conversation-2\",\tper_device_train_batch_size=1,\tgradient_accumulation_steps=32,\tper_device_eval_batch_size=4,\tnum_train_epochs=2,\tlearning_rate=2e-4,\tweight_decay=0.01,\tlogging_steps=10,\tsave_steps=100,\teval_strategy=\"steps\",\teval_steps=10,\tsave_total_limit=1,\tload_best_model_at_end=False,\treport_to='none')trainer = Trainer(\tmodel=peft_model,\targs=training_args,\ttrain_dataset=input_ds['train'],\teval_dataset=input_ds['test'],\tdata_collator=data_collator,)\n\n\ntrainer.train()trainer.evaluate()\n\n\n\n{‘eval_loss’: 2.4432363510131836,‘eval_runtime’: 9.3492,‘eval_samples_per_second’: 37.65,‘eval_steps_per_second’: 9.413,‘epoch’: 1.9822784810126581}\ntestmodel_name_or_path = \"DC/qwen2.5-3B-ins\"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained(\tmodel_name_or_path, \tdevice_map=\"auto\",     torch_dtype=\"auto\",\ttrust_remote_code=True)\n\n\n\nexample = message_format_ds['test'][0]['conversations']example_i = example[:-1]  # 去掉最后一条 assistantexample_o = example[-1]  # 最后一条是 assistant 的回复inputs = tokenizer.apply_chat_template(example_i, tokenize=False, add_generation_prompt=True)inputs = tokenizer(inputs, return_tensors=\"pt\").to(model.device)\n\n\noutputs = model.generate(**inputs, max_new_tokens=500, do_sample=True, temperature=0.7, top_p=0.9)output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)print(\"Output:\\n\", output_text)\n\n\nfrom peft import PeftModel, PeftConfigpeft_model_id = \"lora-conversation-2/checkpoint-196\"  # LoRA adapter 保存路径config = PeftConfig.from_pretrained(peft_model_id)model_name_or_path = \"DC/qwen2.5-3B-ins\"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)base_model  = AutoModelForCausalLM.from_pretrained(\tmodel_name_or_path, \tdevice_map=\"auto\",     torch_dtype=\"auto\",\ttrust_remote_code=True)model = PeftModel.from_pretrained(base_model, peft_model_id)outputs = model.generate(**inputs, max_new_tokens=500, do_sample=True, temperature=0.7, top_p=0.9)output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)print(\"Output:\\n\", output_text)\n\n\n原输出Instead of fighting about trust, is it possible for you and your wife to talk with other about areas which upset each of you?Whenever feelings are hurt, knowing what exactly is problematic and being heard and understood by the partner, goes a long way to building trust.These type of discussions are hard to have, and especially for the first time.  A lot of emotions arise and often people lose their conversation focus from this.If you and your wife have a tough time opening up to each other, consider scheduling time with a couples therapist.Just by each of you committing time to invest in the relationship will show faith in the relationship, and this may contribute to restoring trust.\n\n微调前It sounds like you’re experiencing significant distress related to feelings of betrayal and loss of trust, which can be deeply painful and challenging to navigate. It’s important to address these emotions and the situation with care and consideration for both your own emotional well-being and that of your wife.\nHere are some steps you might consider:\n\nCommunication: Have an open, honest conversation with your wife about how you feel. It’s crucial to express your concerns without placing blame. Use “I” statements to avoid making her defensive. For example, say something like, “I’ve been feeling really hurt by the recent changes in our relationship dynamics.”\n\nListening: Allow your wife to express her side of the story. Sometimes, people act out of fear or insecurity. Listening can provide you with insights into why she may have acted as she did and show her that you respect her perspective.\n\nSeek Support: Consider speaking with a therapist or counselor who can help facilitate communication between you and your wife. They can offer strategies for rebuilding trust and understanding.\n\nEstablish Boundaries: Set clear boundaries regarding privacy and communication with friends and romantic partners. This can help prevent similar situations from arising in the future.\n\nSelf-Care: Engage in activities that promote your mental and emotional health. This could include exercise, meditation, or hobbies that bring you joy and relaxation.\n\nProfessional Help: If the situation feels overwhelming, professional guidance can be invaluable. A psychologist or psychiatrist can provide tools and techniques to help manage your anxiety and depression, which are common responses to betrayal and loss of trust.\n\nTime: Give yourself and your wife time to heal. Healing takes time, and it’s essential not to rush this process.\n\n\nRemember, the goal is to strengthen your relationship, not just to survive the current crisis. Trust can be rebuilt over time with patience, honesty, and commitment from both of you.\nHow does this resonate with you, and what specific areas do you need more assistance with?&lt;|im_end|&gt;\n\n微调后It sounds like you are in the middle of a “trust gap” between your spouse and yourself.  You both are in different places emotionally regarding the issue of trust.  It is a good idea for you to start by talking with your wife about what you have experienced and how it has affected you.  She may not be aware of your feelings and concerns.  You may also want to discuss your thoughts and feelings with someone else outside of your marriage, such as a trusted family member, friend, or therapist.  Having an objective listener can help you sort through your feelings and thoughts regarding this situation. &lt;|im_end|&gt;\n回答效果不一定是正向提升, 但整体风格更接近数据集。\n","categories":["LLM 基础"]},{"title":"Direct Preference Optimization (DPO)","url":"/2025/06/24/Reinforcement_learning/DPO/","content":"Direct Preference Optimization (DPO) 是一种专为大型语言模型（LLMs）设计的训练方法，旨在通过人类偏好数据来优化模型，而无需使用复杂的强化学习算法（如 Proximal Policy Optimization, PPO）。DPO 的核心思想是通过偏好数据直接调整模型参数，绕过显式奖励模型的拟合以及复杂的强化学习优化过程。\npreliminariesKL 散度Kullback-Leibler 散度（KL 散度），又称为相对熵，是信息论中的一个重要概念。它用于衡量两个概率分布之间的差异，特别是衡量用一个分布  去近似另一个目标分布  时的效率损失。\nKL 散度可以理解为两种分布之间的 “信息差异”。具体而言，KL 散度衡量的是用分布  来编码分布  所需要的额外信息量。假设分布 是我们想要捕捉的真实分布，而我们用分布  来表示它。如果  偏离 ，我们在编码时会付出信息损失。KL 散度值越小，表示两个分布越接近。\n其中： 是“真实分布”或“目标分布”； 是“估计分布”或“模型分布”。KL 散度衡量的是，如果  是 “真实” 的概率分布，而我们使用  来表示 ，我们将损失多少信息。\nKL 散度具有以下重要性质：  \n\n非负性： ，且当且仅当  时， 。这意味着两个分布完全一致时，KL 散度为零。  \n非对称性： 。KL 散度并不是衡量两个分布对称差异的度量，所以  和  的值可能不同。\n\n\n假设有两个分布：\n\n（真实分布）\n（模型预测）\n\n\n!!! 计算 KL 散度时，必须确保两个分布都“合法” !!!，否则 KL 散度要么不定义，要么变为无穷大（∞）。\n\n 和  都是合法的概率分布：\n\n所有元素满足 \n\n且 \n\n\n\n对于所有 ，如果 ，则必须有 ，否则：\n\n 中  会导致 log 取无穷；\n\n数学上定义为 \n\n\n\n\n\nBT模型Bradley–Terry 模型是用于成对比较（pairwise comparison）的概率模型，广泛用于排名、偏好建模、体育竞技建模等场景。\n设有一组对象 ，每个对象  都具有一个正数能力值 。对于两个对象  和 ，Bradley–Terry 模型定义它们之间比较时  胜过 (符号为 ，不是  )  的概率为：假设我们有两个人：Alice 的能力值  , Bob 的能力值  那么 Alice 赢过 Bob 的概率为：\n\n但更广义的表达是，假设有一个函数  ，可以计算每个对象的能力值，两个对象  和  偏好概率为：\n\n在 ML 里，为了方便建模，我们通常设置：，这样偏好概率就变成： 与原始公式的区别在于以 e 为底，如果直接用一个线性模型或 LLM 计算 ，那么  可能是负数，不能直接当作能力值。所以我们取指数：。同时Sigmoid 和 softmax 的梯度在深度学习中非常稳定，有助于收敛。\nDPO 原理设计 DPO 的初始目的\n\nDPO optimizes for human preferences while avoiding reinforcement learning. Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification objective, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.\n\nDPO 是设计来解决 RLHF 或者 PPO的一个缺点的，即奖励模型 reward model。奖励模型本质上是基于人类打分数据来近似人类的偏好的，其训练前需要采集相对推理数据，和人类偏好结果。\n\nIn the second phase the SFT model is prompted with prompts x to produce pairs of answers (y1, y2) ∼ π(y|x). These are then presented to human labelers who express preferences for one answer, denoted as yw ≻ yl | x where yw and yl denotes the preferred and dispreferred completion amongst (y1, y2) respectively.\n\n而且在语言模型领域，奖励模型是从微调对象模型来初始化的，通常是在 transformer 最后加上一层线性层，产生 reward 的标量。\n\nIn the context of LMs, the network rφ(x, y) is often initialized from the SFT model π(y|x) with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value .\n\n而且 PPO 是需要分别计算 reward 和 advantage 的，这意味着你需要额外训练两个模型分别作为 reward model 和 advantage model，这就是 Actor-Critic 框架。因此 PPO 训练需要 4 个模型，训练模型，参考模型 (用于产生推理数据和计算 KL 散度)，reward model 和 advantage model。奖励模型不仅需要很多数据来单独训练，而且需要额外显存，所以最好把奖励模型剔除。这就是 DPO 的核心任务，但剔除奖励模型后，如何量化 policy (模型的推理)成为了新的问题。\n\nMotivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly.\nUnlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop.\n\nDPO 的做法就是基于**!!! 从奖励模型到最佳策略的分析映射关系，将基于奖励的 loss 函数转换为基于策略的 !!!**。这里记住这个映射关系。\n\nour key insight is to leverage an analytical mapping from reward functions to optimal policies, which enables us to transform a loss function over reward functions into a loss function over policies.\n\n\n原始目标函数接下来就是公式分析，从最原始的基于奖励模型的目标开始：\n这个公式由两项组成\n\n 代表最大化 reward（其中  出自数据集 , ,  出自最佳模型  的推理 ）\n 代表最佳模型  和参考模型  的 KL 散度，这个要限制漂移，因为训练数据是参考模型生成的在训练模型上训练的。\n\n其中  是隐式奖励模型。核心目标是找到一个最佳的策略 ，使得最终的得分期望最高，同时不能与原模型差的太多。\n\n奖励模型–最佳策略的分析映射关系以下推理我们假定奖励模型已经确定。\n我们将KL 散度展开为期望格式， 被省略了：\n\n将其与奖励项合并\n\n由于 ML 里一般用最小化来表示目标，所以统一乘以  将 argmax 变成 argmin\n\n我们继续合并里面的公式，将整体合并成一个log，这里分子本身只有一个发布项，所以不要动分子。\n\n然后目标函数变成了类似于 KL 散度的形式，分子是一个确定存在的分布。\n\n但是分母的分布不一定合法，概率和不一定为 1， 所以我们将分母归一化，归一化因子 ，其实就是一个常数，值为原分布的概率和。(PS: 上面 KL 散度章节有解释)\n\n转换后的分母是一个复合分布\n\n归一化的目标函数变为\n\n其中  是常数，可以忽略，忽略它后  就是 KL 散度形式。\n\n回忆下 KL 散度的非负性，，且当且仅当  时， 。这意味着两个分布完全一致时，KL 散度为零。因此我们最优模型是\n\n但其实我们没法直接得到 ，因为其中  还是未知的，我们以上只是当作它已知。但**!!! 我们已经得到了最优策略和奖励模型之间的映射关系 !!!**。将上面公式变换可得到奖励模型的公式，就得到了理论上的奖励模型\n\n基于BT模型的新目标函数\nFortunately, the Bradley-Terry model depends only on the difference of rewards between two completions. Substituting the reparameterization  in Eq. 5 for r∗(x, y) into the preference model Eq. 1, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy and reference policy.\n\n\n因为 BT 模型只取决于两个推理之间的奖励差，我们就可以把 BT 模型中的 reward 换成上述公式，整理完后公式里只有最佳策略模型和参考模型。其中  是代表 loser，不偏好的回答， 代表 winner，用户偏好的回答。\n\nNow that we have the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy πθ.\n\n在拿到人类在最佳策略模型于参考模型的偏好概率后，我们就可以为策略生成一个最大似然公式，顺便调整了  和  的顺序。DPO 的梯度更新通过以下公式计算 ( PS:  )其中，  是隐式奖励函数，定义为：\n论文中有一段说明，如下图：  \n  \n其中有 2 部分：\n\n 表示对于一个策略，要偏向于增加出现  的可能性，降低出现  的可能性；\n 表示当奖励模型  评估出现错误时，可获得更高的权重\n\n梯度更新的直观理解是增加优胜者的生成概率，减少劣胜者的生成概率。\n\nIntuitively, the gradient of the loss function LDPO increases the likelihood of the preferred completions yw and decreases the likelihood of dispreferred completions yl.\n\n以上是 DPO 的所有公式推理，整体上基于正常 RL 目标得到奖励模型与最佳模型的映射关系，然后代入基于 BT 模型的 RL 目标。\n\nLoss 函数总结以下，DPO loss 为主要参数如下：\n\n 是要微调的大模型，即策略模型\n 是参考模型，即冻结的强化学习前模型\n 是偏好数据\n 是数据集  中采样的 prompt\n 是  对应的人类偏好的回答\n 是  对应的人类不偏好的回答\n 是超参，用于控制偏离  的程度\n\n\n The DPO loss function can be broken down into two main terms, the first term represents the log probability of the human-preferred response . This term aims to maximize the probability of  as generated by the model , relative to the reference model . The division by  serves as a regularizing factor, ensuring that the fine-tuning does not cause the model to deviate excessively from its original training. Maximizing this term effectively increases the likelihood of  generating responses similar to  in response to inputs like , reinforcing the human preference patterns. Conversely, the second term focuses on minimizing the log probability of the human-dispreferred response . This is achieved by reducing the model’s tendency to generate  type responses, as indicated by the negative sign.\n\nDPO 损失函数可以分为两个主要部分：第一项表示人类偏好回答  的对数概率。这一项的目标是最大化模型  生成  的概率，相对于参考模型 。这里除以  起到了正则化的作用，用以确保微调过程不会使模型过度偏离其原始训练状态。最大化这一项，实质上是在提高  在输入  下生成类似  的回答的可能性，从而强化人类的偏好模式。注意原始目标函数里是有策略模型和参考模型的 KL 散度的，最后转换变成了这样子，但实际还是带 KL 散度控制的，只是最终公式里没有显式的出现。\n相反，第二项的目标是最小化人类不偏好回答  的对数概率。由于负号的存在，这一项通过降低模型生成类似  的回答倾向，来达到惩罚不良回答的效果。\n​\t\nDPO from scratch下面是我手搓的 DPO。我参考了 Allam https://github.com/0xallam/Direct-Preference-Optimization.git 这个 repo 中关于 log prob 和 DPO loss 的写法。Allam 和我在计算 log prob 上有些差别，我只计算了回答部分，而且计算前还 shift label了，Allam 计算了 prompt 所有部分的。数据集预处理也部分参考了 Allam 的写法，但是额外遵守了 qwen 原本的 chat template。\nimport osos.environ[\"WANDB_MODE\"] = \"offline\"  # Disable Weights &amp; Biases loggingos.environ['CUDA_VISIBLE_DEVICES'] = '1'  # Set visible GPUs if neededimport random, torch, wandb, copyimport torch.nn as nnimport numpy as npfrom tqdm import tqdmfrom functools import partialimport torch.nn.functional as Ffrom torch.optim import AdamWfrom torch.utils.data import DataLoaderfrom datasets import load_datasetfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seqmax_length = 512batch_size = 8model_name_or_path = \"Qwen/Qwen2.5-0.5B-Instruct\"config = {    \"max_length\": max_length,    \"batch_size\": batch_size,    \"model_name_or_path\": model_name_or_path,}wandb.init(project='DPO_scratch', config=config)DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)if tokenizer.pad_token is None:    tokenizer.pad_token = tokenizer.eos_token# 策略模型model = AutoModelForCausalLM.from_pretrained(    model_name_or_path,    torch_dtype=torch.bfloat16,    device_map=\"auto\")# 参考模型ref_model = copy.deepcopy(model)ref_model.eval()def freeze_model(model):    model.eval()    for param in model.parameters():        param.requires_grad = Falsefreeze_model(ref_model)\n\n数据集 tokenizeDPO 数据集 belike \n{'id': '04c275bf738fd391b7fe25e25fe7bed3','source': 'truthy_dpo','system': 'You are an unbiased, uncensored, helpful assistant.','prompt': \"What's the nearest national park to you?\",'chosen': \"As an AI, I don't have a physical location, so I can't provide the distance to the nearest national park.\",'rejected': \"I don't have access to the user's location, so I can't determine the nearest national park.\"}\n\n只有提示词相关的字段有用: system, prompt, chosen, rejected, 其中 system 和 prompt 组成了整体的 prompt, chosen 和 rejected 分别作为同一个 prompt 的偏好回答和不偏好回答。\nds = load_dataset(\"jondurbin/truthy-dpo-v0.1\")def tokenize_fn(sample, tokenizer, max_length:int=None):    if max_length is None:        max_length = tokenizer.model_max_length    system_message_content = sample[\"system\"]    user_message_content = sample[\"prompt\"]    chosen_assistant_message_content = sample[\"chosen\"]    rejected_assistant_message_content = sample[\"rejected\"]    # === 仅 prompt ===    messages_without_response = [        {\"role\": \"system\", \"content\": system_message_content},        {\"role\": \"user\", \"content\": user_message_content}    ]    # 由于 tokenizer.apply_chat_template 没法实现截断，所以分为两步    messages_without_response_str = tokenizer.apply_chat_template(        messages_without_response,        tokenize=False,        add_generation_prompt=True,    )    messages_without_response_input_ids = tokenizer(        messages_without_response_str,        max_length=max_length,        truncation=True,    )['input_ids']    # === 带偏好回答 ===    messages_with_chosen_response = messages_without_response + [        {\"role\": \"assistant\", \"content\": chosen_assistant_message_content}    ]    messages_with_chosen_response_str = tokenizer.apply_chat_template(        messages_with_chosen_response,        tokenize=False,        add_generation_prompt=False,    )    messages_with_chosen_response_input_ids = tokenizer(        messages_with_chosen_response_str,        max_length=max_length,        truncation=True,    )['input_ids']    messages_with_chosen_response_lables = copy.deepcopy(messages_with_chosen_response_input_ids)    for i in range(len(messages_without_response_input_ids)):        messages_with_chosen_response_lables[i] = -100  # 将 prompt 部分的标签设置为 -100，表示不计算损失    messages_with_chosen_response_attention_mask = [1] * len(messages_with_chosen_response_input_ids)    # === 带不偏好回答 ===    messages_with_rejected_response = messages_without_response + [        {\"role\": \"assistant\", \"content\": rejected_assistant_message_content}    ]    messages_with_rejected_response_str = tokenizer.apply_chat_template(        messages_with_rejected_response,        tokenize=False,        add_generation_prompt=False,    )    messages_with_rejected_response_input_ids = tokenizer(        messages_with_rejected_response_str,        max_length=max_length,        truncation=True,    )['input_ids']    messages_with_rejected_response_lables = copy.deepcopy(messages_with_rejected_response_input_ids)    for i in range(len(messages_without_response_input_ids)):        messages_with_rejected_response_lables[i] = -100  # 将 prompt 部分的标签设置为 -100，表示不计算损失    messages_with_rejected_response_attention_mask = [1] * len(messages_with_rejected_response_input_ids)    length = max(        len(messages_with_chosen_response_input_ids),         len(messages_with_rejected_response_input_ids)    )    ret = {        \"respone_preferred\": {            \"input_ids\": messages_with_chosen_response_input_ids,            \"attention_mask\": messages_with_chosen_response_attention_mask,            \"labels\": messages_with_chosen_response_lables,        },        \"respone_rejected\": {            \"input_ids\": messages_with_rejected_response_input_ids,            \"attention_mask\": messages_with_rejected_response_attention_mask,            \"labels\": messages_with_rejected_response_lables,        },        \"length\": length,    }        return retds_tokenized = ds.map(    partial(tokenize_fn, tokenizer=tokenizer, max_length=max_length),    remove_columns=ds['train'].column_names,)# ds_tokenized = ds_tokenized.sort(\"length\", reverse=True)\n\n处理为 batch tensor 数据其实每个 sample 里有两组 inputs，处理逻辑和正常数据集没什么差别。还有 dataloader 构造时不要直接都放到 cuda 上，显存会直接都占用了，最好在迭代时将 cpu 上的数据移动到 cuda。\ndef collate_fn(batch, data_collator, device=torch.device(\"cpu\")):    respone_preferred_list = [item['respone_preferred'] for item in batch]    respone_rejected_list = [item['respone_rejected'] for item in batch]    respone_preferred_batch_data = data_collator(respone_preferred_list)    respone_rejected_batch_data = data_collator(respone_rejected_list)    if device.type != 'cpu':        respone_preferred_batch_data = {k: v.to(device) for k, v in respone_preferred_batch_data.items()}        respone_rejected_batch_data = {k: v.to(device) for k, v in respone_rejected_batch_data.items()}    ret = {        \"respone_preferred_batch_data\": respone_preferred_batch_data,        \"respone_rejected_batch_data\": respone_rejected_batch_data,    }    return retdata_collator = DataCollatorForSeq2Seq(    tokenizer=tokenizer,    max_length=max_length,    padding=\"longest\",)train_dataloader = DataLoader(    ds_tokenized['train'],    batch_size=batch_size,    shuffle=True, # 如果提前按长度排序了，不要shuffle，我个人习惯先跑最大的batch来确保不会OOM    collate_fn=partial(collate_fn, data_collator=data_collator),)\n\n计算 DPO loss 的函数DPO 公式是\n\n其中  代表 winner，用户偏好的回答，  是代表 loser，不偏好的回答。\ndef get_log_prob_by_labels(logits, labels):    \"\"\"    计算 $、log \\pi(y|x)$    \"\"\"    # logits: [B, L, V] labels: [B, L]    assert logits.shape[:2] == labels.shape    # 先 shift logits 和 labels    shift_logits = logits[:, :-1, :] # [B, L-1, V]    shift_labels = labels[:, 1:] # [B, L-1]    shift_mask = (shift_labels != -100) # [B, L-1]    # 如果用原始的 shift_mask 去执行 torch.gather 会导致异常，因为 -100 不是合法的索引    # 这里我们暂时把 -100 变成 0，然后再 ele-wise 乘以 mask，-100/0 位置的 logits 都会被置为 0    safe_labels = shift_labels.clone()     safe_labels[~shift_mask] = 0    # 先计算 log prob，再取出 labels 对应的 log prob，最后 mask 处理-100 位置，最后取 mean    log_probs = F.log_softmax(shift_logits, dim=-1) # [B, L-1, V]    log_probs_at_labels = torch.gather(log_probs, 2, safe_labels.unsqueeze(-1)).squeeze(-1)  # [B, L-1]    log_probs_at_labels = log_probs_at_labels * shift_mask # [B, L-1]    log_prob_per_example = log_probs_at_labels.sum(dim=1)  # [B]    return log_prob_per_exampledef get_DPO_loss(    respone_preferred_logprob_policy,     respone_preferred_logprob_ref,     respone_rejected_logprob_policy,     respone_rejected_logprob_ref,    beta=0.1,  # beta 是一个超参数，用于平衡奖励与KL散度):    \"\"\"    计算 DPO 损失    \"\"\"        respone_preferred_logprob_relative = respone_preferred_logprob_policy - respone_preferred_logprob_ref    respone_rejected_logprob_relative = respone_rejected_logprob_policy - respone_rejected_logprob_ref    # y_w 奖励 &gt; y_l 奖励的概率统计    reward_accuracies = (respone_preferred_logprob_relative &gt; respone_rejected_logprob_relative).float().mean()    # y_w 与 y_l奖励之差的平均值    reward_margins = (respone_preferred_logprob_relative - respone_rejected_logprob_relative).mean()    # logsigmoid 函数就是 log(sigmoid(x))     loss = - F.logsigmoid(        beta * (respone_preferred_logprob_relative - respone_rejected_logprob_relative)    ).mean()    return (        loss,         respone_preferred_logprob_relative.mean().item(),         respone_rejected_logprob_relative.mean().item(),         reward_accuracies.item(),         reward_margins.item()    )\n\n主循环epochs = 3lr = 1e-6optimizer = AdamW(model.parameters(), lr=lr)model.train()ref_model.eval()for _ in range(epochs):    for batch in tqdm(train_dataloader):        optimizer.zero_grad()                respone_preferred_batch_data = batch['respone_preferred_batch_data']        respone_preferred_batch_data = {k: v.to(DEVICE) for k, v in respone_preferred_batch_data.items()}        respone_rejected_batch_data = batch['respone_rejected_batch_data']        respone_rejected_batch_data = {k: v.to(DEVICE) for k, v in respone_rejected_batch_data.items()}        # ===== 先计算策略模型和参考模型的 logprob =====        # $\\log \\pi_\\theta(y_w|x)$        input_ids = respone_preferred_batch_data['input_ids']        attention_mask = respone_preferred_batch_data['attention_mask']        labels = respone_preferred_batch_data['labels']        logits = model(            input_ids=input_ids,            attention_mask=attention_mask,        ).logits        respone_preferred_logprob_policy = get_log_prob_by_labels(            logits = logits,             labels = labels        )        # $\\log \\pi_{\\text{ref}}(y_w|x))$        with torch.no_grad():            logits = ref_model(                input_ids=input_ids,                attention_mask=attention_mask,            ).logits            respone_preferred_logprob_ref = get_log_prob_by_labels(                logits = logits,                 labels = labels            )        # ===== 再计算参考模型的 logprob =====        # $\\log \\pi_\\theta(y_l|x)$        input_ids = respone_rejected_batch_data['input_ids']        attention_mask = respone_rejected_batch_data['attention_mask']        labels = respone_rejected_batch_data['labels']        logits = model(            input_ids=input_ids,            attention_mask=attention_mask,        ).logits        respone_rejected_logprob_policy = get_log_prob_by_labels(            logits = logits,            labels = labels        )        # $\\log \\pi_{\\text{ref}}(y_l|x))$        with torch.no_grad():            logits = ref_model(                input_ids=input_ids,                attention_mask=attention_mask,            ).logits            respone_rejected_logprob_ref = get_log_prob_by_labels(                logits = logits,                 labels = labels            )                    # ===== 计算 DPO 损失 =====        loss, respone_preferred_logprob_relative, respone_rejected_logprob_relative, reward_accuracies, reward_margins = get_DPO_loss(            respone_preferred_logprob_policy,             respone_preferred_logprob_ref,             respone_rejected_logprob_policy,             respone_rejected_logprob_ref,            beta=0.1        )                loss.backward()        optimizer.step()                # torch.cuda.empty_cache()        wandb.log({            'loss': loss.item(),            'preferred_relative_logprob': respone_preferred_logprob_relative,            'dispreferred_relative_logprob': respone_rejected_logprob_relative,            'reward_accuracy': reward_accuracies,            'reward_margin': reward_margins,        })wandb.finish()\n\n可视化结果\nloss 从 0.69 降到 0.47，大概1/3。reward margin不断变大，accuracy 第一个epoch结束就达到了0.9，后面一直随着数据在波动，但从margin不断变大可看出还在正向训练。\nDPO trainer by HF以下是 huggingface 实现的 DPO trainer 的调用方法。出自其官方文档 https://huggingface.co/docs/trl/en/dpo_trainer#loss-functions。\n# train_dpo.pyimport jsonfrom datasets import load_datasetfrom trl import DPOConfig, DPOTrainerfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")train_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\").select(range(1000))  # 仅选取前1000条数据进行测试# print(json.dumps(train_dataset[0], indent=2, ensure_ascii=False))\n\ntrl-lib/ultrafeedback_binarized 这个数据集 belike.\n{  \"chosen\": [    {      \"content\": \"Use the pygame library to write a version of the classic game Snake, with a unique twist\",      \"role\": \"user\"    },    {      \"content\": \"Sure, I'd be happy to help you write a version of the classic game Snake using the pygame library! Here's a basic outline of how we can approach this:\\n\\n1. First, we'll need to set up the game display and create a game object that we can use to handle the game's state.\\n2. Next, we'll create the game's grid, which will be used to represent the game board. We'll need to define the size of the grid and the spaces within it.\\n3. After that, we'll create the snake object, which will be used to represent the player's movement. We'll need to define the size of the snake and the speed at which it moves.\\n4. We'll also need to create a food object, which will be used to represent the food that the player must collect to score points. We'll need to define the location of the food and the speed at which it moves.\\n5. Once we have these objects set up, we can start handling the game's logic. We'll need to define the rules for the player's movements, such as how the player can move the snake and how the snake will grow as the player collects more food.\\n6. We'll also need to add collisions detection to the game, so that the snake and the food can collide with each other.\\n7. Finally, we'll need to add a user interface to the game, such as a menu and a scoreboard.\\n\\nNow, as for the unique twist, we could add a few features to make the game more interesting. For example, we could add power-ups that give the player special abilities, such as the ability to grow the snake faster or to change its direction. We could also add obstacles, such as walls or pits, that the player must avoid.\\n\\nHere's some sample code to get us started:\\n```\\nimport pygame\\n\\n# Initialize pygame\\npygame.init()\\n\\n# Set up the game display\\nwidth = 800\\nheight = 600\\nscreen = pygame.display.set_mode((width, height))\\n\\n# Define the colors\\nWHITE = (255, 255, 255)\\nBLACK = (0, 0, 0)\\nGREEN = (0, 255, 0)\\n\\n# Define the game objects\\nsnake = pygame.Rect(50, 50, 10, 10)\\nfood = pygame.Rect(100, 100, 10, 10)\\n\\n# Define the game logic\\ndef update_snake():\\n    # Get the mouse position\\n    mouse_pos = pygame.mouse.get_pos()\\n\\n    # Move the snake based on the mouse position\\n    if mouse_pos[0] &gt; snake.x:\\n        snake.x += 10\\n    elif mouse_pos[0] &lt; snake.x:\\n        snake.x -= 10\\n    elif mouse_pos[1] &gt; snake.y:\\n        snake.y += 10\\n    elif mouse_pos[1] &lt; snake.y:\\n        snake.y -= 10\\n\\n    # Update the snake's size\\n    if snake.x == food.x and snake.y == food.y:\\n        snake.width += 10\\n        snake.height += 10\\n\\n# Define the game loop\\ndef game_loop():\\n    # Clear the screen\\n    screen.fill(BLACK)\\n\\n    # Update the game objects\\n    update_snake()\\n\\n    # Draw the game objects\\n    screen.fill(WHITE)\\n    screen.draw.rect(snake, GREEN)\\n    screen.draw.rect(food, BLACK)\\n\\n    # Update the display\\n    pygame.display.flip()\\n\\n# Start the game loop\\ngame_loop()\\n\\n# Run the game\\nwhile True:\\n    for event in pygame.event.get():\\n        if event.type == pygame.QUIT:\\n            pygame.quit()\\n            break\\n\\n    pygame.time.Clock().tick(60)\\n```\\nThis code sets up a basic game display, defines the snake and food objects, and updates the game state based on the player's movements. We've also added a simple user interface and some basic collision detection.\\n\\nAs for the unique twist, we could add a few features to make the game\",      \"role\": \"assistant\"    }  ],  \"rejected\": [    {      \"content\": \"Use the pygame library to write a version of the classic game Snake, with a unique twist\",      \"role\": \"user\"    },    {      \"content\": \"Sure, here's an example of how to write a version of Snake game with a unique twist using the Pygame library:\\n```python\\nimport pygame\\n\\nclass SnakeGame:\\n    def __init__(self, game_width, game_height):\\n        pygame.init()\\n        screen = pygame.display.set_mode((game_width, game_height))\\n        pygame.display.set_caption(\\\"Snake Game\\\")\\n        self.speed = 5  # Speed of the snake\\n        self.food_speed = 1  # Speed of the food\\n        self.direction = 0  # Initial direction of the snake\\n        self.snakelen = 0  # Length of the snake\\n        self.food = pygame.image.load(\\\"snake_food.png\\\")\\n        self.head = pygame.image.load(\\\"snake_head.png\\\")\\n        self.tail = pygame.image.load(\\\"snake_tail.png\\\")\\n        self.game Quint()\\n    def Quint(self):\\n        for i in range(50):\\n            pygame.draw.line(screen, (180, 100, 220), (0, 0, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 0, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 50, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 50, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 80, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 80, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 50, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 50, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 80, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 80, 300), 2)\\n            pygame.display.flip()\\n        self.game.run()\\n    def run(self):\\n        while True:\\n            for event in pygame.event. pygame.KEYDOWN:\\n                if event.key == pygame.K_LEFT:\\n                    self.direction = -1\\n                if event.key == pygame.K_RIGHT:\\n                    self.direction = 1\\n            self.snakelen += 1\\n            if self.snakelen == 0:\\n                self.snakelen = 10\\n            if self.snakelen &gt; 20:\\n                self.snakelen = 20\\n            self.gameQuint()\\n            self.foodCrossing()\\n            self.headRun()\\n            pygame.display.update()\\ngame = SnakeGame(800, 600)\\ngame.run()\\n```\\nIn this game, the snake moves with a constant speed, but the direction of the snake can be controlled by the user using the left and right arrow keys. The snake grows in length every 10 segments, and when it reaches a certain length, it resets to 10 segments. The food moves fast and randomly crosses the screen, and the snake can eat it by colliding with it. The snake's head and tail move independently of each other. The game ends when the snake dies or reaches the end of the screen.\",      \"role\": \"assistant\"    }  ],  \"score_chosen\": 6.0,  \"score_rejected\": 4.0}\n\n它数据已经部分组织好了，chosen 和 rejected 都是包含了问题与回答的完整 messages。还额外多了两个 score 代表满意度之类的。\ntraining_args = DPOConfig(output_dir=\"Qwen2.5-0.5B-DPO\")trainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)trainer.train()\n\ntokenizeDPOTrainer.__init__() 主要负责数据处理\n# 出自 DPOTrainerif data_collator is None:    data_collator = DataCollatorForPreference(pad_token_id=self.padding_value)train_dataset = self._prepare_dataset(train_dataset, processing_class, args, \"train\")\n\n其中 processing_class 就是 tokenizer。DPOTrainer._prepare_dataset 里经过了3个变化\ndataset = dataset.map(maybe_extract_prompt, **map_kwargs)dataset = dataset.map(    maybe_apply_chat_template, fn_kwargs={\"tokenizer\": processing_class, \"tools\": args.tools}, **map_kwargs)dataset = dataset.map(    self.tokenize_row if not self.is_vision_model else self.process_row,    remove_columns=[\"chosen\", \"rejected\"],    fn_kwargs={        \"processing_class\": processing_class,        \"max_prompt_length\": args.max_prompt_length,        \"max_completion_length\": args.max_completion_length,        # for enc-dec, we add the special tokens ([bos_token] + prompt + [eos_token]; completion + [eos_token])        \"add_special_tokens\": False,    },    **map_kwargs,)\n\nmaybe_extract_prompt 和 maybe_apply_chat_template 都是嵌套函数，内部简单判断下是否需要执行 extract_prompt 和 apply_chat_template。extract_prompt直接把对话 messages 拆分为共同的 messages 前缀 prompt 和之后各自的回答 chosen 与 rejected。\nfor idx in range(min(len(example[\"chosen\"]), len(example[\"rejected\"]))):    if example[\"chosen\"][idx] != example[\"rejected\"][idx]:        if example[\"chosen\"][idx - 1] == \" \":  # remove space before the prompt            idx -= 1        breakreturn {    \"prompt\": example[\"chosen\"][:idx],    \"chosen\": example[\"chosen\"][idx:],    \"rejected\": example[\"rejected\"][idx:],}\n\napply_chat_template 就是分别为 prompt, chosen, rejected 转换为文本形式。\n# Apply the chat template to the whole conversationif \"messages\" in example:    messages = tokenizer.apply_chat_template(example[\"messages\"], tools=tools, tokenize=False)# Apply the chat template to the prompt, adding the generation promptif \"prompt\" in example:    last_role = example[\"prompt\"][-1][\"role\"]    if last_role == \"user\":        add_generation_prompt = True        continue_final_message = False    elif last_role == \"assistant\":        add_generation_prompt = False        continue_final_message = True    else:        raise ValueError(f\"Invalid role in the last message: {last_role}\")    prompt = tokenizer.apply_chat_template(        example[\"prompt\"],        tools=tools,        continue_final_message=continue_final_message,        tokenize=False,        add_generation_prompt=add_generation_prompt,    )if \"prompt\" in example:  # explicit prompt and prompt-completion case    if \"chosen\" in example:        prompt_chosen = tokenizer.apply_chat_template(            example[\"prompt\"] + example[\"chosen\"], tools=tools, tokenize=False        )        # DeepSeek-R1 inserts a &lt;think&gt; token when using `add_generation_prompt`, which can cause discrepancies        # between the prompt alone and the combined prompt+completion. To ensure consistency, we extract the        # common prefix between the two. In most cases, this is a no-op.        prompt = \"\".join(x for x, _ in takewhile(lambda x: x[0] == x[1], zip(prompt, prompt_chosen)))        chosen = prompt_chosen[len(prompt) :]    if \"rejected\" in example and \"prompt\" in example:  # explicit prompt        prompt_rejected = tokenizer.apply_chat_template(            example[\"prompt\"] + example[\"rejected\"], tools=tools, tokenize=False        )        # Handle DeepSeek-R1 &lt;think&gt; token, see the above comment for details        prompt = \"\".join(x for x, _ in takewhile(lambda x: x[0] == x[1], zip(prompt, prompt_rejected)))        rejected = prompt_rejected[len(prompt) :]    if \"completion\" in example:        prompt_completion = tokenizer.apply_chat_template(            example[\"prompt\"] + example[\"completion\"], tools=tools, tokenize=False        )        # Handle DeepSeek-R1 &lt;think&gt; token, see the above comment for details        prompt = \"\".join(x for x, _ in takewhile(lambda x: x[0] == x[1], zip(prompt, prompt_completion)))        completion = prompt_completion[len(prompt) :]else:  # implicit prompt case    if \"chosen\" in example:        chosen = tokenizer.apply_chat_template(example[\"chosen\"], tools=tools, tokenize=False)    if \"rejected\" in example:        rejected = tokenizer.apply_chat_template(example[\"rejected\"], tools=tools, tokenize=False)# Extract the completion by removing the prompt part from the prompt-completion stringoutput = {}if \"messages\" in example:    output[\"text\"] = messagesif \"prompt\" in example:    output[\"prompt\"] = promptif \"chosen\" in example:    output[\"chosen\"] = chosenif \"rejected\" in example:    output[\"rejected\"] = rejectedif \"completion\" in example:    output[\"completion\"] = completionif \"label\" in example:    output[\"label\"] = example[\"label\"]return output\n\n最后一个函数就是正常的 tokenize 为 ids.\n此外，它还单独设计了一个 data collater, DataCollatorForPreference，分别对 prompt, chosen, rejected 做 padding。\nforwardforward 函数是 DPOTrainer.concatenated_forward, 它内部太长了，只抽取部分内容简单讲下, 内容直接写在代码段里。\n# 把 prompt x 2, 同时 chosen, rejected 拼接，分别作为 prompt_input_ids, completion_input_idsconcatenated_batch = self.concatenated_inputs(batch, padding_value=self.padding_value)# Concatenate the prompt and completion inputsinput_ids = torch.cat((prompt_input_ids, completion_input_ids), dim=1)# loss mask 和 input_ids 形状一样，但对应 prompt 的内容是 0 ，对应 completion 的内容是 1。目的也是只mask prompt部分。# Mask the prompt but not the completion for the lossloss_mask = torch.cat(    (torch.zeros_like(prompt_attention_mask), completion_attention_mask),    dim=1,)# .... 中间一段变化，把一堆变量都塞入 model_kwargsoutputs = model(input_ids, **model_kwargs)logits = outputs.logits# 之后就要开始计算 log prob# 先 shift label， label直接基于 input_ids 变换过来# Offset the logits by one to align with the labelslabels = torch.roll(input_ids, shifts=-1, dims=1)loss_mask = torch.roll(loss_mask, shifts=-1, dims=1).bool()# label 经过 loss_mask 处理，把mask对象变为 0，不能为 -100，因为 -100 不是合法的索引# Compute the log probabilities of the labelslabels[~loss_mask] = 0  # dummy token; we'll ignore the losses on these tokens laterper_token_logps = selective_log_softmax(logits, labels)per_token_logps[~loss_mask] = 0per_token_logps = torch.roll(per_token_logps, shifts=1, dims=1)# 在 seq 维度取sum，因为都是 logall_logps = per_token_logps[:, 1:].sum(-1)# 因为数据是前部分是 chosen, 后部分是 rejected, 直接分开就行output[\"chosen_logps\"] = all_logps[:num_examples]output[\"rejected_logps\"] = all_logps[num_examples:]\n\nDPO lossloss 函数是 DPOTrainer.dpo_loss，传参分别是策略函数和参考函数的上计算 chosen 和 rejected 的logprob\n# 这里就是 log(\\pi/\\pi_ref)，因为传入的就是 logprob，所以这里直接相减就行logratios = chosen_logps - rejected_logpsref_logratios = ref_chosen_logps - ref_rejected_logpslogits = logratios - ref_logratios# 之后乘以beta，再 log sigmoid 处理下就变成 lossif self.loss_type == \"sigmoid\":    losses = (        -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)        - F.logsigmoid(-self.beta * logits) * self.label_smoothing    )# 返回 chosen 和 rejected 的奖励平均值，和 losschosen_rewards = self.beta * (chosen_logps.to(device) - ref_chosen_logps.to(device)).detach()rejected_rewards = self.beta * (rejected_logps.to(device) - ref_rejected_logps.to(device)).detach()return losses, chosen_rewards, rejected_rewards\n\n\n\n","categories":["强化学习"]},{"title":"Langchain 入门教程 - 2.提示词模板","url":"/2025/05/04/langchain-tutorial-series/02Prompt/","content":"Prompt TemplatePrompt 模板对于生成动态且灵活的提示至关重要，可用于各种场景，例如会话历史记录、结构化输出和特定查询。  \n在本教程中，我们将探讨创建 PromptTemplate 对象的方法，应用部分变量，通过 YAML 文件管理模板，并利用 ChatPromptTemplate 和 MessagePlaceholder 等高级工具来增强功能。\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)\n\n创建 PromptTemplate 对象有两种方法可以创建 PromptTemplate 对象：  \n\n1. 使用 from_template() 方法。  \n2. 直接创建 PromptTemplate 对象并同时生成提示词。\n\n方法 1. 使用 from_template() 方法\n使用 {variable} 语法定义模板，其中 variable 代表可替换的变量。\n\nfrom langchain_core.prompts import PromptTemplate# {}内部是变量template = \"What is the capital of {country}?\"# 使用`from_template`函数来创建模板prompt = PromptTemplate.from_template(template)prompt\n\n\n\n\nPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')\n\nPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')\n类已经解析出country这个变量，可以通过为变量 country 赋值来完成提示词。\n# 类似str的`format`方法来创建实例prompt.format(country=\"United States of America\")\n\n\n\n\n'What is the capital of United States of America?'\n\n进一步用chain来简化流程\ntemplate = \"What is the capital of {country}?\"prompt = PromptTemplate.from_template(template)chain = prompt | llmchain.invoke(\"United States of America\").content\n\n\n\n\n'The capital of the United States of America is Washington, D.C.'\n\n方法 2. 直接创建 PromptTemplate 对象并同时生成提示\n明确指定 input_variables 以进行额外的验证。  \n否则，如果 input_variables 与模板字符串中的变量不匹配，实例化时可能会引发异常。\n\nfrom langchain_core.prompts import PromptTemplate# Define templatetemplate = \"What is the capital of {country}?\"# Create a prompt template with `PromptTemplate` objectprompt = PromptTemplate(    template=template,    input_variables=[\"country\"],)prompt\n\n\n\n\nPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')\n\npartial variables可临时固定的可变参数, 是特殊的 input_variables, 是对应 input_variables 在缺失时的默认值。使用 partial_variables，您可以部分应用函数。这在需要共享 通用变量 时特别有用。  \n常见示例：  \n\n日期或时间（date or time） 是典型的应用场景。\n\n例如，假设您希望在提示中指定当前日期：  \n\n直接硬编码日期 或 每次手动传递日期变量 可能不太灵活。  \n更好的方法 是使用一个返回当前日期的函数，将其部分应用于提示模板，从而动态填充日期变量，使提示更具适应性。\n\nfrom langchain_core.prompts import PromptTemplate# Define templatetemplate = \"What are the capitals of {country1} and {country2}, respectively?\"# Create a prompt template with `PromptTemplate` objectprompt = PromptTemplate(    template=template,    input_variables=[\"country1\"],    partial_variables={        \"country2\": \"United States of America\"  # Pass `partial_variables` in dictionary form    },)prompt\n\n\n\n\nPromptTemplate(input_variables=['country1'], input_types={}, partial_variables={'country2': 'United States of America'}, template='What are the capitals of {country1} and {country2}, respectively?')\n\nprompt.format(country1=\"South Korea\")\n\n\n\n\n'What are the capitals of South Korea and United States of America, respectively?'\n\n通过partial()函数修改或者增加临时变量, 或者直接修改 PromptTemplate.partial_variables\n\nprompt_partial = prompt.partial(country2=”India”), 可创建新实例的同时保留原实例\nprompt.partial_variables = {‘country2’:’china’}, 直接修改原实例\n\nprompt_partial = prompt.partial(country2=\"India\")prompt_partial.format(country1=\"South Korea\")\n\n\n\n\n'What are the capitals of South Korea and India, respectively?'\n\nprompt.partial_variables = {'country2':'china'}prompt.format(country1=\"South Korea\")\n\n\n\n\n'What are the capitals of South Korea and china, respectively?'\n\npartial variables 可以临时用新值, 不会影响缺失时的默认值\nprint(prompt_partial.format(country1=\"South Korea\", country2=\"Canada\"))print(prompt_partial.format(country1=\"South Korea\"))\n\nWhat are the capitals of South Korea and Canada, respectively?\nWhat are the capitals of South Korea and India, respectively?\n\npartial variables 可用函数传递, 不需要手动设置新值\nfrom datetime import datetimedef get_today():    return datetime.now().strftime(\"%B %d\")prompt = PromptTemplate(    template=\"Today's date is {today}. Please list {n} celebrities whose birthday is today. Please specify their date of birth.\",    input_variables=[\"n\"],    partial_variables={        \"today\": get_today  # Pass `partial_variables` in dictionary form    },)prompt.format(n=3)\n\n\n\n\n\"Today's date is January 30. Please list 3 celebrities whose birthday is today. Please specify their date of birth.\"\n\n从 YAML 文件加载 Prompt 模板您可以将 Prompt 模板 存储在单独的 YAML 文件 中，并使用 load_prompt 进行加载和管理。\n以下是一个yaml示例: \n\n_type: \"prompt\"template: \"What is the color of {fruit}?\"input_variables: [\"fruit\"]\n\nfrom langchain_core.prompts import load_promptprompt = load_prompt(\"prompts/fruit_color.yaml\", encoding=\"utf-8\")prompt\n\nChatPromptTemplateChatPromptTemplate 可用于将会话历史记录包含到提示词中，以提供上下文信息。消息以 (role, message) 元组的形式组织，并存储在 列表 中。\n角色（role）:\n\n\"system\" ：系统设置信息，通常用于全局指令或设定 AI 的行为。  \n\"human\" ：用户输入的消息。  \n\"ai\" ：AI 生成的响应消息。\n\nfrom langchain_core.prompts import ChatPromptTemplatechat_prompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")chat_prompt\n\n\n\n\nChatPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?'), additional_kwargs={})])\n\nChatPromptTemplate(input_variables=[‘country’], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[‘country’], input_types={}, partial_variables={}, template=’What is the capital of {country}?’), additional_kwargs={})])\n注意这个prompt被 HumanMessagePromptTemplate包装了，而且位于一个list中\nchat_prompt.format(country=\"United States of America\")\n\n\n\n\n'Human: What is the capital of United States of America?'\n\n多角色使用 ChatPromptTemplate.from_messages来定义模板, 内部是 chat list, 每个 chat 都是以 (role, message) 元组的形式组织\nfrom langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages(    [        # role, message        (\"system\", \"You are a friendly AI assistant. Your name is {name}.\"),        (\"human\", \"Nice to meet you!\"),        (\"ai\", \"Hello! How can I assist you?\"),        (\"human\", \"{user_input}\"),    ])# Create chat messagesmessages = chat_template.format_messages(name=\"Teddy\", user_input=\"What is your name?\")messages\n\n\n\n\n[SystemMessage(content='You are a friendly AI assistant. Your name is Teddy.', additional_kwargs={}, response_metadata={}),\n HumanMessage(content='Nice to meet you!', additional_kwargs={}, response_metadata={}),\n AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}),\n HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n\n可直接用上面的 Message list 的形式调用大模型\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)llm.invoke(messages).content\n\n\n\n\n\"My name is Teddy. It's nice to meet you! How can I help you today?\"\n\nMessagePlaceholderLangChain 提供了 MessagePlaceholder，用途包括:\n\n当不确定使用哪些角色 作为消息提示模板的一部分时，它可以提供灵活性。  \n在格式化时插入一组消息列表，适用于动态会话历史记录的场景。\n\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderchat_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.\",        ),        MessagesPlaceholder(variable_name=\"conversation\"),        (\"human\", \"Summarize the conversation so far in {word_count} words.\"),    ])chat_prompt\n\n\n\n\nChatPromptTemplate(input_variables=['conversation', 'word_count'], input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=&lt;function _get_type at 0x7ff1a966cfe0&gt;, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='Summarize the conversation so far in {word_count} words.'), additional_kwargs={})])\n\nformatted_chat_prompt = chat_prompt.format(    word_count=5,    conversation=[        (\"human\", \"Hello! I’m Teddy. Nice to meet you.\"),        (\"ai\", \"Nice to meet you! I look forward to working with you.\"),    ],)print(formatted_chat_prompt)\n\nSystem: You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.\nHuman: Hello! I’m Teddy. Nice to meet you.\nAI: Nice to meet you! I look forward to working with you.\nHuman: Summarize the conversation so far in 5 words.\n\nFew-Shot PromptingLangChain 的 Few-Shot Prompting 提供了一种强大的框架，通过提供精心挑选的示例，引导语言模型生成高质量的输出。此技术减少了大量模型微调的需求，同时确保在各种应用场景中提供精准且符合上下文的结果。  \n\nFew-Shot Prompt 模板：  \n\n通过嵌入示例定义提示的结构和格式，指导模型生成一致的输出。\n\n\n示例选择策略（Example Selection Strategies）：  \n\n动态选择最相关的示例 以匹配特定查询，增强模型的上下文理解能力，提高响应准确性。\n\n\nChroma 向量存储（Chroma Vector Store）：  \n\n用于存储和检索基于语义相似度的示例，提供可扩展且高效的 Prompt 结构构建。\n\n\n\nFewShotPromptTemplateFew-shot prompting 是一种强大的技术，它通过提供少量精心设计的示例，引导语言模型生成准确且符合上下文的输出。LangChain 的 FewShotPromptTemplate 简化了这一过程，使用户能够构建灵活且可复用的提示，适用于问答、摘要、文本校正等任务。  \n1. 设计 Few-Shot 提示（Designing Few-Shot Prompts）  \n\n定义示例，展示所需的输出结构和风格。  \n确保示例覆盖边界情况，以增强模型的理解能力和性能。\n\n2. 动态示例选择（Dynamic Example Selection）  \n\n利用语义相似性或向量搜索，选择最相关的示例，以匹配输入查询。\n\n3. 集成 Few-Shot 提示（Integrating Few-Shot Prompts）  \n\n结合 Prompt 模板与语言模型，构建强大的链式调用，以生成高质量的响应。\n\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(\tbase_url='http://localhost:5551/v1',\tapi_key='EMPTY',\tmodel_name='Qwen2.5-7B-Instruct',\ttemperature=0.2,)# User queryquestion = \"What is the capital of United States of America?\"# Query the modelresponse = llm.invoke(question)# Print the responseprint(response.content)\n\nThe capital of the United States of America is Washington, D.C.\n\n以下是一个 CoT 的示例prompt\nfrom langchain_core.prompts import PromptTemplate, FewShotPromptTemplate# Define examples for the few-shot promptexamples = [    {        \"question\": \"Who lived longer, Steve Jobs or Einstein?\",        \"answer\": \"\"\"Does this question require additional questions: Yes.Additional Question: At what age did Steve Jobs die?Intermediate Answer: Steve Jobs died at the age of 56.Additional Question: At what age did Einstein die?Intermediate Answer: Einstein died at the age of 76.The final answer is: Einstein\"\"\",    },    {        \"question\": \"When was the founder of Naver born?\",        \"answer\": \"\"\"Does this question require additional questions: Yes.Additional Question: Who is the founder of Naver?Intermediate Answer: Naver was founded by Lee Hae-jin.Additional Question: When was Lee Hae-jin born?Intermediate Answer: Lee Hae-jin was born on June 22, 1967.The final answer is: June 22, 1967\"\"\",    },    {        \"question\": \"Who was the reigning king when Yulgok Yi's mother was born?\",        \"answer\": \"\"\"Does this question require additional questions: Yes.Additional Question: Who is Yulgok Yi's mother?Intermediate Answer: Yulgok Yi's mother is Shin Saimdang.Additional Question: When was Shin Saimdang born?Intermediate Answer: Shin Saimdang was born in 1504.Additional Question: Who was the king of Joseon in 1504?Intermediate Answer: The king of Joseon in 1504 was Yeonsangun.The final answer is: Yeonsangun\"\"\",    },    {        \"question\": \"Are the directors of Oldboy and Parasite from the same country?\",        \"answer\": \"\"\"Does this question require additional questions: Yes.Additional Question: Who is the director of Oldboy?Intermediate Answer: The director of Oldboy is Park Chan-wook.Additional Question: Which country is Park Chan-wook from?Intermediate Answer: Park Chan-wook is from South Korea.Additional Question: Who is the director of Parasite?Intermediate Answer: The director of Parasite is Bong Joon-ho.Additional Question: Which country is Bong Joon-ho from?Intermediate Answer: Bong Joon-ho is from South Korea.The final answer is: Yes\"\"\",    },]example_prompt = PromptTemplate.from_template(    \"Question:\\n{question}\\nAnswer:\\n{answer}\")# Print the first formatted exampleprint(example_prompt.format(**examples[0]))\n\nQuestion:\nWho lived longer, Steve Jobs or Einstein?\nAnswer:\nDoes this question require additional questions: Yes.\nAdditional Question: At what age did Steve Jobs die?\nIntermediate Answer: Steve Jobs died at the age of 56.\nAdditional Question: At what age did Einstein die?\nIntermediate Answer: Einstein died at the age of 76.\nThe final answer is: Einstein\n\n以下这个 FewShotPromptTemplate 将 examples 以 example_prompt 格式添加到真正 QA 的前面。真正的 QA 按照 suffix 格式展示\n# Initialize the FewShotPromptTemplatefew_shot_prompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question:\\n{question}\\nAnswer:\",    input_variables=[\"question\"],)# Example questionquestion = \"How old was Bill Gates when Google was founded?\"# Generate the final promptfinal_prompt = few_shot_prompt.format(question=question)print(final_prompt)\n\nQuestion:\nWho lived longer, Steve Jobs or Einstein?\nAnswer:\nDoes this question require additional questions: Yes.\nAdditional Question: At what age did Steve Jobs die?\nIntermediate Answer: Steve Jobs died at the age of 56.\nAdditional Question: At what age did Einstein die?\nIntermediate Answer: Einstein died at the age of 76.\nThe final answer is: Einstein\n\n​    Question:    When was the founder of Naver born?    Answer:    Does this question require additional questions: Yes.    Additional Question: Who is the founder of Naver?    Intermediate Answer: Naver was founded by Lee Hae-jin.    Additional Question: When was Lee Hae-jin born?    Intermediate Answer: Lee Hae-jin was born on June 22, 1967.    The final answer is: June 22, 1967\n​    Question:    Who was the reigning king when Yulgok Yi’s mother was born?    Answer:    Does this question require additional questions: Yes.    Additional Question: Who is Yulgok Yi’s mother?    Intermediate Answer: Yulgok Yi’s mother is Shin Saimdang.    Additional Question: When was Shin Saimdang born?    Intermediate Answer: Shin Saimdang was born in 1504.    Additional Question: Who was the king of Joseon in 1504?    Intermediate Answer: The king of Joseon in 1504 was Yeonsangun.    The final answer is: Yeonsangun\n​    Question:    Are the directors of Oldboy and Parasite from the same country?    Answer:    Does this question require additional questions: Yes.    Additional Question: Who is the director of Oldboy?    Intermediate Answer: The director of Oldboy is Park Chan-wook.    Additional Question: Which country is Park Chan-wook from?    Intermediate Answer: Park Chan-wook is from South Korea.    Additional Question: Who is the director of Parasite?    Intermediate Answer: The director of Parasite is Bong Joon-ho.    Additional Question: Which country is Bong Joon-ho from?    Intermediate Answer: Bong Joon-ho is from South Korea.    The final answer is: Yes\n​    Question:    How old was Bill Gates when Google was founded?    Answer:\nresponse = llm.invoke(final_prompt)print(response.content)\n\nDoes this question require additional questions: Yes.\nAdditional Question: When was Google founded?\nIntermediate Answer: Google was founded in 1998.\nAdditional Question: When was Bill Gates born?\nIntermediate Answer: Bill Gates was born on October 28, 1955.\nThe final answer is: Bill Gates was 43 years old when Google was founded.\n\n特殊 promptRAG 文档分析基于检索到的文档上下文处理并回答问题，确保高准确性和高相关性。\nfrom langchain.prompts import ChatPromptTemplatesystem = \"\"\"You are a precise and helpful AI assistant specializing in question-answering tasks based on provided context.Your primary task is to:1. Analyze the provided context thoroughly2. Answer questions using ONLY the information from the context3. Preserve technical terms and proper nouns exactly as they appear4. If the answer cannot be found in the context, respond with: 'The provided context does not contain information to answer this question.'5. Format responses in clear, readable paragraphs with relevant examples when available6. Focus on accuracy and clarity in your responses\"\"\"human = \"\"\"#Question:{question}#Context:{context}#Answer:Please provide a focused, accurate response that directly addresses the question using only the information from the provided context.\"\"\"prompt = ChatPromptTemplate.from_messages(\t[\t\t(\"system\", system), \t\t(\"human\", human)\t])prompt\n\n\n\n\nChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are a precise and helpful AI assistant specializing in question-answering tasks based on provided context.\\nYour primary task is to:\\n1. Analyze the provided context thoroughly\\n2. Answer questions using ONLY the information from the context\\n3. Preserve technical terms and proper nouns exactly as they appear\\n4. If the answer cannot be found in the context, respond with: 'The provided context does not contain information to answer this question.'\\n5. Format responses in clear, readable paragraphs with relevant examples when available\\n6. Focus on accuracy and clarity in your responses\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='#Question:\\n{question}\\n\\n#Context:\\n{context}\\n\\n#Answer:\\nPlease provide a focused, accurate response that directly addresses the question using only the information from the provided context.'), additional_kwargs={})])\n\n具有来源归因的 RAG（RAG with Source Attribution）增强型 RAG 实现，支持详细的来源追踪和引用，以提高可追溯性和验证可靠性。\nfrom langchain.prompts import ChatPromptTemplatesystem = \"\"\"You are a precise and thorough AI assistant that provides well-documented answers with source attribution.Your responsibilities include:1. Analyzing provided context thoroughly2. Generating accurate answers based solely on the given context3. Including specific source references for each key point4. Preserving technical terminology exactly as presented5. Maintaining clear citation format [source: page/document]6. If information is not found in the context, state: 'The provided context does not contain information to answer this question.'Format your response as:1. Main Answer2. Sources Used (with specific locations)3. Confidence Level (High/Medium/Low)\"\"\"human = \"\"\"#Question:{question}#Context:{context}#Answer:Please provide a detailed response with source citations using only information from the provided context.\"\"\"prompt = ChatPromptTemplate.from_messages(\t[\t\t(\"system\", system), \t\t(\"human\", human)\t])PROMPT_OWNER = \"eun\"hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, new_repo_is_public=True)\n\n其实在回答要求里加入了源引用的要求\nLLM 响应评估（LLM Response Evaluation）基于多项质量指标对 LLM 响应进行全面评估，并提供详细的评分方法。\nfrom langchain.prompts import PromptTemplateevaluation_prompt = \"\"\"Evaluate the LLM's response based on the following criteria:INPUT:Question: {question}Context: {context}LLM Response: {answer}EVALUATION CRITERIA:1. Accuracy (0-10)- Perfect (10): Completely accurate, perfectly aligned with context- Good (7-9): Minor inaccuracies- Fair (4-6): Some significant inaccuracies- Poor (0-3): Major inaccuracies or misalignment2. Completeness (0-10)- Perfect (10): Comprehensive coverage of all relevant points- Good (7-9): Covers most important points- Fair (4-6): Missing several key points- Poor (0-3): Critically incomplete3. Context Relevance (0-10)- Perfect (10): Optimal use of context- Good (7-9): Good use with minor omissions- Fair (4-6): Partial use of relevant context- Poor (0-3): Poor context utilization4. Clarity (0-10)- Perfect (10): Exceptionally clear and well-structured- Good (7-9): Clear with minor issues- Fair (4-6): Somewhat unclear- Poor (0-3): Confusing or poorly structuredSCORING METHOD:1. Calculate individual scores2. Compute weighted average:   - Accuracy: 40%   - Completeness: 25%   - Context Relevance: 25%   - Clarity: 10%3. Normalize to 0-1 scaleOUTPUT FORMAT:{    \"individual_scores\": {        \"accuracy\": float,        \"completeness\": float,        \"context_relevance\": float,        \"clarity\": float    },    \"weighted_score\": float,    \"normalized_score\": float,    \"evaluation_notes\": string}Return ONLY the normalized_score as a decimal between 0 and 1.\"\"\"prompt = PromptTemplate.from_template(evaluation_prompt)","categories":["Langchain 入门教程"],"tags":["Langchain"]},{"title":"LLM 推理 & speculative sampling","url":"/2025/06/30/LLM-basic-series/decode+speculative_sampling/","content":"推理基本方式贪心解码类似于分类器通常会选择概率最大的标签，对于文本生成任务，最直接的方法就是每次取概率最大的token_id，接下来我以贪心搜索为例介绍文本生成的流程。\n# Encode initial inputinput_text = \"What is star war?\"input_ids = tokenizer.encode(input_text, return_tensors='pt').to(DEVICE)  # Shape: [1, 4] # Set the number of tokens to generatenum_tokens_to_generate = 100 # Iteratively generate tokens# for _ in tqdm(range(num_tokens_to_generate), mininterval=1):for _ in range(num_tokens_to_generate):     # Get model output logits    outputs = model(input_ids)  # Shape: [1, current_length, 50257] or [batch_size, token length, vocab size]    logits = outputs.logits     '''    Predict the next token based on the last position    i.e., the i-th position logits is for predicting the i+1-th token    In this case, we want to predict the next token based on previous tokens, so we use the logits of the final token.    If you see the source code of forward function, you can notice the shifting of labels and logits for aligning.    '''    next_token_logits = logits[:, -1, :]  # Shape: [1, 50257], corresponding to each vocab     '''    Greedy decoding: select the token with the highest probability    Supposily you can try top-k and beam search    '''    greedy_token_id = torch.argmax(next_token_logits, dim=-1)  # Shape: [1]     # Append the predicted token to the input_ids    input_ids = torch.cat([input_ids, greedy_token_id.unsqueeze(-1)], dim=-1).to(DEVICE)  # Shape: [1, current_length + 1]     # print(tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)) # Decode the entire sequence of tokensgenerated_text = tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)print(\"Generated Text:\\n\", generated_text)\n\n# encode context the generation is conditioned onmodel_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(DEVICE) pprint(model_inputs, width=100)# generate 40 new tokens# the output of generate is a `GenerateDecoderOnlyOutput` object, we only need the first attribute.greedy_output = model.generate(**model_inputs,     max_new_tokens=40,     # max_length=50,     ) token_ids = torch.squeeze(greedy_output[0])print(tokenizer.decode(token_ids, skip_special_tokens=True))\n\n\n\nLLM的 logit 是 batch size * seq length * token num，在训练时，模型通常是用第 i-1 位的 logit 去预测第 i 位的 token id，所以预测时用最后一位的 logit 去预测新的 token id，结果softrmax后，我们得到新的 token id，然后按照 auto regressive 的风格，把新 token id加入到 之前的 token id 序列中， 再用相同的方法得到下一个 token id。这就是预测的整体流程， 贪心搜索只不过决定了新的 token id 的选择。\n\n以图中的概率发布，解码结果位 ‘The cat is’。\n在给定上下文生成的词语是合理的，但模型很快开始重复自己！这是语言生成中一个非常常见的问题，尤其在贪婪搜索和波束搜索中更为明显。然而，贪婪搜索的主要缺点是它会错过那些被低概率词遮挡的高概率词。其实是因为它的视野窗口只有1，因此无法做出偏长远的判断。\ntop-k采样在 top-k采样中，模型会筛选出 k 个最可能的下一 token，并将概率质量重新分配到这 K 个token，然后在他们之中随机采样。\n\n\n计算token概率：在模型处理输入文本后，它会预测可能的下一个 token 的概率分布。\n\n筛选 top-k：与考虑所有可能的 token 不同，top-k 采样将选择范围缩小到概率最高的 k 个 token 。这种“剪枝”减少了潜在输出空间，专注于最可能的下一 token ，而忽略了不太可能的选项。\n\n随机采样：从 top-k  token 中，重新分配他们的概率（一般再除以概率和，使得整体概率和仍对于1），根据它们的概率随机采样一个 token ，而不是总是选择最高概率的词元。这种方式引入了多样性，使生成的文本更加丰富多样。\n\n\ntopk_output = model.generate(**model_inputs,     max_new_tokens=40,    do_sample=True,     top_k=50    ) token_ids = torch.squeeze(topk_output[0])print(tokenizer.decode(token_ids, skip_special_tokens=True))\n\n通过调整 k 的值，高 k 值（例如 50 或 100）允许更多的选择，增加了多样性和创造性，但可能降低连贯性。低 k 值（例如 5 或 10）限制了选项，通常使文本更具确定性和集中性，但有时也会导致内容过于重复或保守。\n\n然而，top-k采样的一个问题是，它不会动态调整从下一个 token 的概率分布中过滤的 token 。这可能会导致问题，因为某些 token 可能来自非常陡峭的分布（即集中于少数词的分布，这时更容易加入很低概率的 token ，保留他们的意义不大），而其他词则来自更平坦的分（这种情况 token 的选择更正常）。例如图中左侧发布相对平坦，概率更均匀， top-6 随机采样时合理的，右侧发布主要集中在  top-3 中，如果取 top-6，后三个词的概率太小了，往往不可能选中，因此这样的概率在一开始就不需要成为候选。\n因此，将采样池限制为固定大小 k 可能会导致模型在陡峭分布中产生无意义的内容，而在平坦分布中限制模型的创造力。\ntop-p采样与仅从最可能的 k 个 token 中采样不同，top-p 采样选择的是累计概率超过阈值 p 的最小 token 集合。\n它与 top-k 的区别仅在于筛选方式。top-k 采样选择个体概率最高的前 k 个 token ，而 top-p 采样则考虑累计概率至少为 p 的最小 token 集合，即概率较大的那些 token 。\ntopp_output = model.generate(**model_inputs,     max_new_tokens=40,    do_sample=True,     top_p=0.92    ) token_ids = torch.squeeze(topp_output[0])print(tokenizer.decode(token_ids, skip_special_tokens=True))\n\n在开放式语言生成中，top-p 和 top-k 采样似乎比传统的贪心搜索和 beam search 生成更流畅的文本。但有人证明，贪心搜索和 beam search 的明显缺陷（主要是生成重复词序列）是由模型本身（尤其是模型的训练方式）引起的，而不是生成方法的问题。\ntemperature采样方法过程中都涉及到对部分数据采样，但通常都会把他们原始的概率进行调整。概率调整时的重要参数就是 temperature，它在文本生成中用于控制生成内容的随机性或创造性。通过调整可能生成的下一个 token 的概率分布，temperature 参数可以让模型生成的文本更保守或更具有创意。\ntemperature 会对每个可能生成的下一个 token 的概率分布进行缩放。temperature 越高，分布越“平”，即模型对每个候选 token 的选择倾向性降低，更可能从更大范围的 token 中随机选取下一个 token 。temperature 越低，分布越“尖”，模型会更偏向选择高概率的候选 token ，这样生成的文本就会更可预测和连贯。\nspeculative samplingspeculative sampling 基本思想时利用小模型先自回归生成，然后大模型批量评估，如果接受则继续生成，如果不接受，利用大模型评估的结果重新计算对应位置的token。\n\n步骤如下:\n\ndraft model  基于 prefix 逐个生成 ，其中  是小模型生成的长度，同时也要返回对于位置的分布概率 \n用 target model  基于  为 input_ids 生成对应位置的发布概率 \n对于每个位置，以 target model 的 token 概率与 draft model 的 token 概率为接受率 ，取一个随机数，如果随机了大于此概率则接受，这个位置的 token 就是 ，否则就是不接受，那这个位置的 token 就直接基于  来推理，之后的 token 就没法推理了，因为此后 target model 的发布概率是基于不被接受的 token 产生的。\n如果所有 token 都被接受，那可以利用 target model 产生的最后一位的发布概况，直接推理出新的token\n\n\n原论文有半页内容解释为什么这样子推理的概率发布等同于 target model 的概率发布，很简单，不解释了。\n至于为什么快了，因为推理的时间成本在于自回归的解码，只有先生成上一个位置的 token，才能把 token 加入然后生成新的 token。模型越大，生成每一个 token 的时间越长。speculative sampling 就是让小模型先生成，再大模型验证，而且大模型的发布概率是不需要一次一次计算的，是基于一段新的token 批量计算的，因此会快。\n但 speculative sampling 不能与 top k 和 top p 共同设置，因为接受率  需要保证  非零，而 top p 和 top k 是会将大部分概率赋值为 0，因此就会冲突。而 temperature 是缩放，因此不受影响，可以共同使用。\n代码实现import osos.environ['CUDA_VISIBLE_DEVICES'] = '0'import torch, timefrom tqdm import tqdm, trangeimport torch.nn.functional as Ffrom transformers import AutoTokenizer, AutoModelForCausalLMmodel_id = '../../DC/qwen2-1.5b-ins'tokenizer = AutoTokenizer.from_pretrained(model_id)model = AutoModelForCausalLM.from_pretrained(\tmodel_id, \ttorch_dtype=torch.bfloat16,\tdevice_map='auto',)\n\n\n\nhuggingface model generate 函数正常推理可以直接用 model.generate 函数\ntext = 'Do not go gentle into that good night,\\nOld age should burn and rave at close of day;'input_ids = tokenizer(text, return_tensors='pt').to(model.device)start = time.time()with torch.no_grad():\toutputs = model.generate(\t\t**input_ids,\t\tmax_new_tokens=20,\t\tdo_sample=True,\t\ttemperature=0.7,\t\ttop_p=0.9,\t)\tprint(tokenizer.decode(outputs[0], skip_special_tokens=True))print(f'Inference time: {time.time() - start:.2f} seconds')\n\nDo not go gentle into that good night,Old age should burn and rave at close of day; The wind is still, the rain is falling, The stars are all bright and bright. But IInference time: 0.64 seconds\n\nnaive 贪心解码 （带kv cache）在调用 model.forward(input_ids=input_ids, past_key_values=past_kv) 时：input_ids 中应该只包含“新输入的 token”，也就是**还未被缓存（未进入 past_key_values）**的 token。\ndef autoregressive_generate_by_greedy_search(\t\tmodel, \t\ttokenizer, \t\ttext, \t\tmax_new_tokens: int = 10, \t\tuse_past_key_values: bool = True\t):\tdevice = model.device\tinput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)  # 初始输入 [B=1,S]\twith torch.no_grad():\t\tstart = time.time()\t\tpast_key_values = None\t\tfor _ in trange(max_new_tokens):\t\t\tif use_past_key_values and past_key_values is not None:\t\t\t\t# 仅输入最近生成的 token\t\t\t\tinput_ids_step = input_ids[:, -1:]  # shape: [B, 1]\t\t\telse:\t\t\t\t# 第一步或不使用缓存：输入当前全部输入（会随着 input_ids 增长）\t\t\t\tinput_ids_step = input_ids\t\t\toutputs = model(input_ids=input_ids_step, past_key_values=past_key_values if use_past_key_values else None)\t\t\tlogits = outputs.logits  # shape: [B, S, V]\t\t\tpast_key_values = outputs.past_key_values if use_past_key_values else None\t\t\tlogits_last_token = logits[:, -1, :]  # shape: [B, V]\t\t\tnext_token_id = logits_last_token.argmax(dim=-1, keepdim=True)  # shape: [B, 1]\t\t\t# 更新 input_ids 以便下一轮使用\t\t\tinput_ids = torch.cat([input_ids, next_token_id], dim=-1)\t\tprint(f'Inference time: {time.time() - start:.2f} seconds')\tgenerated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\treturn generated_texttext = 'Do not go gentle into that good night,\\nOld age should burn and rave at close of day;'generated_text = autoregressive_generate_by_greedy_search(model, tokenizer, text, max_new_tokens=20, use_past_key_values=True)print(generated_text)generated_text = autoregressive_generate_by_greedy_search(model, tokenizer, text, max_new_tokens=20, use_past_key_values=False)print(generated_text)\n\n100%|██████████| 20/20 [00:00&lt;00:00, 26.17it/s]Inference time: 0.77 secondsDo not go gentle into that good night,Old age should burn and rave at close of day; The night is not set for us between wind and falling leaves.A good night is not enough.100%|██████████| 20/20 [00:00&lt;00:00, 33.57it/s]Inference time: 0.60 secondsDo not go gentle into that good night,Old age should burn and rave at close of day; The night is not set for us between wind and falling leaves.A good night is not enough.\n\nnaive  top k &amp; top p  &amp; temperature 解码 （带 kv cache）def apply_top_k_filter(probs: torch.Tensor, top_k: int = 0):\t\"\"\"\t将 probs 中非 top k 的值置为 -inf\t\"\"\"\tif top_k &lt;= 0:\t\treturn probs\tvocab_size = probs.size(-1)\ttop_k = min(top_k, vocab_size)\t# 获得 top k， 将非 top k 的 probs 置为 0\ttop_k_values, top_k_indices = torch.topk(probs, top_k)\tmin_top_k_values = top_k_values[:, -1].unsqueeze(-1)\tindices_to_remove = probs &lt; min_top_k_values\tprobs = probs.masked_fill(indices_to_remove, 0)\treturn probs\tdef apply_top_p_filter(probs: torch.Tensor, top_p: float = 0.0):\t\"\"\"\t将 probs top 累积概率超过 top_p 的值置保留，其他置为 0\t\"\"\"\tif top_p &lt;= 0.0:\t\treturn probs\t\t# 排序 probs\tsorted_probs, sorted_indices = torch.sort(probs, descending=True)\t# 计算累计概率\tcumulative_probs = torch.cumsum(sorted_probs, dim=-1)\t# 计算累计概率 &gt; top p 的掩码，需要前移一位，最大概率位始终要保留\tremove_mask = cumulative_probs &gt; top_p\tremove_mask[..., 1:] = remove_mask[..., :-1].clone()\tremove_mask[..., 0] = 0\tindices_to_remove = remove_mask.scatter(1, sorted_indices, remove_mask)\tprobs = probs.masked_fill(indices_to_remove, 0)\treturn probsdef apply_temperature(logits: torch.Tensor, temperature: float = 1.0):\t\"\"\"\t将 logits 除以 temperature, temperature 越大越平滑，越小越尖锐\t\"\"\"\tif temperature &lt;= 0.0:\t\traise ValueError(\"Temperature must be greater than 0.\")\treturn logits / temperaturedef re_normalize_probs(probs: torch.Tensor):\t\"\"\"\t将 top p 和 top k 过滤后的概率重新归一化为概率分布\t\"\"\"\tprob_sums = torch.sum(probs, dim=-1, keepdim=True)\tprobs = probs / prob_sums\treturn probs\n\n\ndef autoregressive_generate_by_sampling(\t\tmodel, \t\ttokenizer, \t\ttext, \t\tmax_new_tokens:int=10, \t\ttop_k:int=10, \t\ttop_p:float=0.9, \t\ttemperature:float=1.0, \t\tuse_past_key_values:bool=True\t):\t\"\"\"\t使用采样方法生成文本\t\"\"\"\tdevice = model.device\tinput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device) # [B=1,S]\twith torch.no_grad():\t\tstart = time.time()\t\tpast_key_values = None\t\tfor _ in trange(max_new_tokens):\t\t\t\t\t\tif use_past_key_values and past_key_values is not None:\t\t\t\t# 仅输入最近生成的 token\t\t\t\tinput_ids_step = input_ids[:, -1:]  # shape: [B, 1]\t\t\telse:\t\t\t\t# 第一步或不使用缓存：输入所有 tokens\t\t\t\tinput_ids_step = input_ids\t\t\toutputs = model(input_ids=input_ids_step, past_key_values=past_key_values if use_past_key_values else None)\t\t\tlogits = outputs.logits  # shape: [B, S, V]\t\t\tpast_key_values = outputs.past_key_values if use_past_key_values else None\t\t\tlogits_last_token = logits[:, -1, :]\t\t\tlogits_last_token = apply_temperature(logits_last_token, temperature)\t\t\tprobs = F.softmax(logits_last_token, dim=-1)  # [B,S,V]\t\t\tprobs = apply_top_k_filter(probs, top_k)\t\t\tprobs = apply_top_p_filter(probs, top_p)\t\t\tprobs = re_normalize_probs(probs)\t\t\tnext_token_id = torch.multinomial(probs, num_samples=1)  # [B,S,V]\t\t\t\t\t\tinput_ids = torch.cat([input_ids, next_token_id], dim=-1)\t\tprint(f'Inference time: {time.time() - start:.2f} seconds')\t\tgenerated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\treturn generated_texttext = 'Do not go gentle into that good night,\\nOld age should burn and rave at close of day;'generated_text = autoregressive_generate_by_sampling(model, tokenizer, text, max_new_tokens=20, use_past_key_values=True)print(generated_text)generated_text = autoregressive_generate_by_sampling(model, tokenizer, text, max_new_tokens=20, use_past_key_values=False)print(generated_text)\n\n100%|██████████| 20/20 [00:00&lt;00:00, 30.06it/s]Inference time: 0.67 secondsDo not go gentle into that good night,Old age should burn and rave at close of day; Old age might’s well ask what’s in store.For we are not simply dead, we are100%|██████████| 20/20 [00:00&lt;00:00, 32.77it/s]Inference time: 0.61 secondsDo not go gentle into that good night,Old age should burn and rave at close of day;The summer bugle grew and the day but faltered;The roosting-dove peeps\n\nspeculative samplingtarget_model = AutoModelForCausalLM.from_pretrained(\t'../../DC/opt-1.3b', \ttorch_dtype=torch.bfloat16,\tdevice_map='auto',)tokenizer = AutoTokenizer.from_pretrained('../../DC/opt-1.3b')draft_model = AutoModelForCausalLM.from_pretrained(\t'../../DC/opt-6.7b', \ttorch_dtype=torch.bfloat16,\tdevice_map='auto',)\n\n\ndef truncate_kv_cache(past_key_values, new_length):    new_past = []    for layer_past in past_key_values:        k, v = layer_past        k = k[:, :, :new_length, :]        v = v[:, :, :new_length, :]        new_past.append((k, v))    return new_pastdef sample_from_draft_model(model, input_ids, past_key_values, max_new_tokens=4, temperature=1.0):\t\"\"\"\t从模型中采样生成文本\t对于 draft 模型，需要返回 output 和 prob，前者用于计算 target model 的 prob，后者用于判断是否保留\t\"\"\"\tdevice = model.device\tinput_ids = input_ids.to(device)  # [B=1,S]\t\tcollected_probs = []\twith torch.no_grad():\t\tfor _ in range(max_new_tokens):\t\t\toutputs = model(input_ids, past_key_values=past_key_values)\t\t\t\t\t\tpast_key_values = outputs.past_key_values\t\t\tnext_token_logits = outputs.logits[:, -1, :]\t\t\tnext_token_logits = apply_temperature(next_token_logits, temperature)\t\t\tnext_token_probs = torch.softmax(next_token_logits, dim=-1)\t\t\tnext_token_id = torch.multinomial(next_token_probs, num_samples=1)\t\t\tinput_ids = torch.cat([input_ids, next_token_id], dim=-1)\t\t\tcollected_probs.append(next_token_probs)\tdraft_new_token_probs = torch.cat(collected_probs, dim=0)\treturn input_ids, draft_new_token_probs, past_key_valuesdef get_target_model_probs(model, input_ids, num_new_tokens=4, past_key_values=None):\t\"\"\"\t获取 target 模型的 logits\t\"\"\"\tdevice = model.device\tinput_ids = input_ids.to(device)  # [B=1,S]\t\twith torch.no_grad():\t\toutputs = model(input_ids, past_key_values=past_key_values)\t\tlogits = outputs.logits\t\tnew_token_logits = logits[0, -num_new_tokens-1:, :]\t\tnew_token_logits = apply_temperature(new_token_logits, temperature=1.0)\t\tnew_token_probs = torch.softmax(new_token_logits, dim=-1)\t\treturn new_token_probs[:num_new_tokens], new_token_probs[-1], outputs.past_key_valuestext = 'Do not go gentle into that good night,\\nOld age should burn and rave at close of day;'input_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(draft_model.device)max_new_tokens = 20lookahead = 10temperature = 1.0epsilon = 1e-9draft_past_key_values = Nonetarget_past_key_values = Nonetarget_length = input_ids.shape[1] + max_new_tokensgenerated_ids = input_ids.clone()while generated_ids.shape[1] &lt; len(input_ids[0]) + max_new_tokens:\tprompt_len = generated_ids.shape[1]\tdraft_input_ids, draft_new_token_probs, draft_past_key_values = sample_from_draft_model(\t\tdraft_model, generated_ids, draft_past_key_values, max_new_tokens=lookahead, temperature=temperature\t)\ttarget_new_token_probs, target_fianl_token_probs, target_past_key_values = get_target_model_probs(\t\ttarget_model, draft_input_ids, num_new_tokens=lookahead, past_key_values=target_past_key_values\t)\tnum_accepted = 0\tall_accepted = True\tfor i in range(lookahead):\t\tdrafted_token_id = draft_input_ids[0, prompt_len + i]\t\tp_val = target_new_token_probs[i][drafted_token_id.item()]\t\tq_val = draft_new_token_probs[i][drafted_token_id.item()]\t\tratio = p_val / (q_val + epsilon)  # Add epsilon to avoid division by zero\t\trandom_num = torch.rand(1)\t\t## Acceptance\t\tif random_num.item() &lt; ratio.item():\t\t\tgenerated_ids = torch.cat([generated_ids, drafted_token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\t\t\tnum_accepted += 1\t\t\t\t## Rejection\t\telse:\t\t\tall_accepted = False\t\t\tnew_dist =  target_new_token_probs[i] - draft_new_token_probs[i]\t\t\tnew_dist = torch.max(torch.zeros_like(new_dist), new_dist)\t\t\tnew_dist = re_normalize_probs(new_dist)\t\t\tresampled_token_id = torch.multinomial(new_dist, num_samples=1)\t\t\tgenerated_ids = torch.cat([generated_ids, resampled_token_id.unsqueeze(0)], dim=-1)\t\t\tbreak\tif all_accepted:\t\tfinal_token_id = torch.multinomial(target_fianl_token_probs, num_samples=1)\t\tgenerated_ids = torch.cat([generated_ids, final_token_id.unsqueeze(0)], dim=-1)\t\tnum_accepted += 1\taccepted_input_id_len = prompt_len + num_accepted\t\t# target_past_key_values = truncate_kv_cache(target_past_key_values, new_length=accepted_input_id_len)\t# draft_past_key_values = truncate_kv_cache(draft_past_key_values, new_length=accepted_input_id_len)\ttarget_past_key_values = truncate_kv_cache(target_past_key_values, new_length=accepted_input_id_len-1)\tdraft_past_key_values = truncate_kv_cache(draft_past_key_values, new_length=accepted_input_id_len-1)\t\tprint(f\"Accepted {num_accepted} tokens. Total length: {accepted_input_id_len}\")\t\n\nAccepted 1 tokens. Total length: 23Accepted 11 tokens. Total length: 35Accepted 9 tokens. Total length: 44","categories":["LLM 基础"]},{"title":"Qwen 2.5 VL 微调处理","url":"/2025/07/12/Multi-modal-series/qwen-vl-SFT/","content":"LVLM 在 transformer 里好像没有统一的训练方法，大概率是因为对模块内部处理方式的不同，不像 LLM 一样都是类似的 GPT 结构，forward 没有 multi-modal，single-modal 的处理比较单调。因此这个 blog 里我参考 Qwen 2.5 VL 中 qwen-vl-finetune 中的处理方式复现其流程，主要是数据处理流程，之后就是简单的 model forward (简单介绍)。\n单条数据的处理这是单挑数据格式，官方支持对话格式是sharegpt的格式，角色和对话字段和名称都和正常用的openai格式不一样，实际代码里还是用openai格式进行判断\n{\t\"image\": [\"10149.png\", \"COCO_train2014_000000580957.jpg\"],\t\"conversations\": [\t\t{\t\t\t\"from\": \"human\",\t\t\t\"value\": \"&lt;image&gt;\\nIn which year the value was 51?\\n&lt;image&gt;\"\t\t},\t\t{\t\t\t\"from\": \"gpt\",\t\t\t\"value\": \"2014\"\t\t}\t],\t\"data_path\": \"demo/images\"}\n为了方便，我接下来直接用 openai 格式的对话模板。\nimport os, torch, copy, jsonfrom PIL import Imagefrom typing import List, Dictfrom transformers import AutoProcessor, AutoTokenizerfrom transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessormodel_id = '../../../DC/qwen2.5vl-3b-ins'processor = AutoProcessor.from_pretrained(model_id, use_fast=True)image_processor = processor.image_processortokenizer = processor.tokenizer\n\n\nsource  = {\t\"image\": [\"10149.png\", \"COCO_train2014_000000580957.jpg\"],\t\"conversations\": [\t\t{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\t\t{\"role\": \"user\", \"content\": \"&lt;image&gt;\\nIn which year the value was 51?\\n&lt;image&gt;\"},\t\t{\"role\": \"assistant\", \"content\": \"2014\"}\t],\t\"data_path\": \"demo/images\"}\n\n官方处理韩式是被封装在 qwen2.5-vl/qwen-vl-finetune/qwenvl/data/data_qwen 中的 LazySupervisedDataset 里。lazy dataset 只在需要时读取单个样本或 batch，适合大规模数据（如图片、视频、大文本）。通常具体处理方式放在 __getitem__ 函数内部。正常 dataset 在 init 时就会一次性加载到内存或完成预处理，不适合特别大的数据集。\n因此接下里处理方式里都是处理单条数据，实现的 __getitem__的处理方式.\nimage 处理这里是将 image 做预处理\n\n图片调整大小\npixel的归一化 (作为 vit 的输入)\n图片的维度计算 (作为文本处理的参数之一，需要为 image token 预留位置)\n\ndef process_single_image(processor, image_file:str):\t# 读取 Image 为 PIL 对象\timage = Image.open(image_file).convert(\"RGB\")\t# 处理单个图片为tensor\tvisual_processed = processor.preprocess(image, return_tensors=\"pt\")\timage_tensor = visual_processed[\"pixel_values\"]\tif isinstance(image_tensor, List):\t\timage_tensor = image_tensor[0]\tgrid_thw = visual_processed[\"image_grid_thw\"][0]\treturn image_tensor, grid_thwif \"image\" in source:\timage_folder = source[\"data_path\"]\timage_file = source[\"image\"]\tif not isinstance(image_file, List):\t\timage_file = [image_file]\t\timage_file = [\t\tos.path.join(image_folder, file) for file in image_file\t]\tresults = [\t\tprocess_single_image(image_processor, file) \t\tfor file in image_file\t]\t# grid_thw 是 time height width 的缩写\timage_list, grid_thw_list = zip(*results)\t# 计算 grid_thw 的点乘 / merge 数量的2次方，即 image 投影后的占用 token 数量\t# 由于 qwen 2.5vl 是将相邻的 patch 合并成一个 token，所以需要除以 merge_size 的平方，才得到实际的 token 数量\tgrid_thw_merged = copy.deepcopy(grid_thw_list)\tgrid_thw_merged = [\t\tgrid_thw.prod() // image_processor.merge_size ** 2\t\tfor grid_thw in grid_thw_merged\t]print(\"image_list:\", *[item.shape for item in image_list])print(\"grid_thw_list:\", *[item.tolist() for item in grid_thw_list])print(\"grid_thw_merged:\", grid_thw_merged)\n\n预处理结果如下，当然部分值下面需要用到\nimage_list: torch.Size([440, 1176]) torch.Size([1380, 1176])grid_thw_list: [1, 20, 22] [1, 30, 46]grid_thw_merged: [tensor(110), tensor(345)]\nimage_list 是已经处理为 patch 的数据，vit 是以 patch 为输入的，第一维度则是该图片转换为 patch 的数量，但这样却失去了图片的比例信息，而 grid_thw 参数弥补了这个损失。而 grid_thw_merged 作为图片 token 数量，用于为文本处理预留相关的 pad token。\ntext 处理# tokenizer 重新指定, 不清楚的看 https://huggingface.co/blog/chat-templatestokenizer_alter = copy.deepcopy(tokenizer)# 这个提示chat_template = \"{% for message in messages %}{{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|im_start|&gt;assistant\\n' }}{% endif %}\"tokenizer_alter.chat_template = chat_template\n\n这个格式就是 qwen 语言模型的 chat template格式\n{% for message in messages %}\t{{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}{% endfor %}{% if add_generation_prompt %}\t{{ '&lt;|im_start|&gt;assistant\\n' }}{% endif %}\n\n原来 qwenvl 的 chat template 格式比较复杂\n{# 初始化图像和视频计数器 #}{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{# 遍历所有消息 #}{% for message in messages %}    {# 在第一个消息不是 system 时插入默认 system prompt #}    {% if loop.first and message['role'] != 'system' %}&lt;|im_start|&gt;systemYou are a helpful assistant.&lt;|im_end|&gt;    {% endif %}&lt;|im_start|&gt;{{ message['role'] }}        {# 如果消息内容是字符串 #}    {% if message['content'] is string %}{{ message['content'] }}&lt;|im_end|&gt;    {# 如果消息内容是一个包含多段内容的列表 #}    {% else %}        {% for content in message['content'] %}            {# 处理图片类型内容 #}            {% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}                {% set image_count.value = image_count.value + 1 %}                {% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;            {# 处理视频类型内容 #}            {% elif content['type'] == 'video' or 'video' in content %}                {% set video_count.value = video_count.value + 1 %}                {% if add_vision_id %}Video {{ video_count.value }}: {% endif %}&lt;|vision_start|&gt;&lt;|video_pad|&gt;&lt;|vision_end|&gt;            {# 处理文本类型内容 #}            {% elif 'text' in content %}{{ content['text'] }}            {% endif %}        {% endfor %}&lt;|im_end|&gt;    {% endif %}{% endfor %}{# 结束时可选性地加入 assistant 的生成提示 #}{% if add_generation_prompt %}&lt;|im_start|&gt;assistant{% endif %}\n\n接下来处理对话。首先是确保对话合法：\n\n用户消息和大模型消息交叉产生，即配对\n用户消息必须是第一个，除非有 system 消息。\n\ndefault_system_message = \"You are a helpful assistant.\"chat_sources = copy.deepcopy(source[\"conversations\"])r\"\"\"源码处理，只保证第一个是 human 消息，system消息是没法设置，实际第一个是 system 消息，第二个是 human 消息的情况也行 try:\tif roles[source[0][\"from\"]] != roles[\"human\"]:\t\tsource = source[1:]except:\tprint(chat_sources)\"\"\"# 确保第一个是 human 消息while not (\t(chat_sources[0]['role'] == 'user') or \t(len(chat_sources) &gt;=2 and chat_sources[0]['role'] == 'system' and chat_sources[1]['role'] == 'user')):\tchat_sources = chat_sources[1:]# 确保用户消息和大模型消息交叉产生，即配对if chat_sources[0]['role'] == 'system':\tassert len(chat_sources) % 2 == 1, \"user messages and assistant messages must be paired.\"\tfor i in range(1, len(chat_sources), 2):\t\tassert (chat_sources[i]['role'] == 'user' and chat_sources[i + 1]['role'] == 'assistant'), \"user messages and assistant messages must be paired.\"else:\tassert len(chat_sources) % 2 == 0, \"user messages and assistant messages must be paired.\"\tfor i in range(0, len(chat_sources), 2):\t\tassert (chat_sources[i]['role'] == 'user' and chat_sources[i + 1]['role'] == 'assistant'), \"user messages and assistant messages must be paired.\"\tchat_sources = [{\"role\": \"system\", \"content\": default_system_message}] + chat_sources # 添加默认 system 消息print(json.dumps(chat_sources, indent=2, ensure_ascii=False))\n\n[  {    \"role\": \"system\",    \"content\": \"You are a helpful assistant.\"  },  {    \"role\": \"user\",    \"content\": \"&lt;image&gt;\\nIn which year the value was 51?\\n&lt;image&gt;\"  },  {    \"role\": \"assistant\",    \"content\": \"2014\"  }]\n这就是一个合法的消息列表。\n接下来依次处理每条消息。根据对应消息的角色，来处理对应的 label。注意只有 assistant 角色的 content 需要设置对应 label 为原 token id，而其他都是 -100。\n相比于纯文本的处理，主要区别在于需要为 vision 内容预留对应 token 位置，上面 image 处理时已经计算到 每个 image 的 patch 数量，这里就依靠这些数量创建 pad token。\n# 这两个变量是用于记录现在处理的图片的index，每个图片的处理就是塞入图片大小相关的 '&lt;|image_pad|&gt;' token。visual_replicate_index_image = 0IGNORE_INDEX = -100input_id, target = [], []for conv in chat_sources:\trole  = conv['role']\tcontent = conv['content']\t# content 内容只有需要对 user 特殊处理，因为可能有图片和视频相关的内容。\tif role == 'user':\t\tif '&lt;image&gt;' in content:\t\t\tparts = content.split('&lt;image&gt;')\t\t\tnum_parts = len(parts)\t\t\tnew_parts = []\t\t\tfor i in range(num_parts):\t\t\t\t# 加入被 &lt;image&gt; 分割的文本部分\t\t\t\tnew_parts.append(parts[i])\t\t\t\t# 加入图片相关的 token，但最后一次不需要\t\t\t\tif i != num_parts - 1:\t\t\t\t\timage_tokens = (\t\t\t\t\t\t\"&lt;|vision_start|&gt;\"\t\t\t\t\t\t+ \"&lt;|image_pad|&gt;\" * grid_thw_merged[visual_replicate_index_image]\t\t\t\t\t\t+ \"&lt;|vision_end|&gt;\"\t\t\t\t\t)\t\t\t\t\tvisual_replicate_index_image += 1\t\t\t\t\tnew_parts.append(image_tokens)\t\t\tcontent = \"\".join(new_parts)\t\telif '&lt;video&gt;' in content:\t\t\t# 不做视频相关任务，不写了\t\t\tpass\t\telse:\t\t\tpass\ttext_conv = [{\"role\": role, \"content\": content}]\tencode_id = tokenizer_alter.apply_chat_template(text_conv) # 一定要用 tokenizer_alter，默认 tokenizer 会加入默认系统消息\tinput_id += encode_id\tif role in [\"user\", \"system\"]:\t\t# user 和 system 消息不需要计算损失\t\ttarget += [IGNORE_INDEX] * len(encode_id) \telse:\t\t# assistant 消息需要计算损失，但前缀特殊 token 不需要计算损失\t\ttarget_mask = encode_id.copy()\t\ttarget_mask[:3] = [IGNORE_INDEX] * 3 # 忽略开头的 '&lt;|im_start|&gt;system\\n'\t\ttarget += target_maskassert len(input_id) == len(target), \\\tf\"input_id length ({len(input_id)}) != target length ({len(target)})\"assert visual_replicate_index_image == len(grid_thw_merged), \\\tf\"visual_replicate_index_image ({visual_replicate_index_image}) != len(grid_thw_merged) ({len(grid_thw_merged)})\"input_ids = torch.tensor([input_id], dtype=torch.long)labels = torch.tensor([target], dtype=torch.long)print(\ttokenizer.decode(input_ids[0]),\ttokenizer.decode([i for i in labels[0] if i != IGNORE_INDEX]),\tsep='\\n'+'-'*10+'\\n')\n\n以下是 input_ids\n|im_start|&gt;systemYou are a helpful assistant.&lt;|im_end|&gt;&lt;|im_start|&gt;user&lt;|vision_start|&gt;&lt;|image_pad|&gt;...(共110个&lt;|image_pad|&gt;)&lt;|vision_end|&gt;In which year the value was 51?&lt;|vision_start|&gt;&lt;|image_pad|&gt;...(共345个&lt;|image_pad|&gt;)&lt;|vision_end|&gt;&lt;|im_end|&gt;&lt;|im_start|&gt;assistant2014&lt;|im_end|&gt;----------2014&lt;|im_end|&gt;----------\n以下是非 -100 的 labels 内容，只有 assistant 的消息和对应的结束 token: ‘2014&lt;|im_end|&gt;’\n位置编码qwen 2.5 vl 的 3d rope 建议还是找专门的博客看下，我这里直接用源码中的 rope 算法，其相比正常 rope 的区别在于为图片和视频创建新的序列维度，所以它的第一维度是 3，图片位置的文本序号会不变，而图片序列会正常增长。\n以下 rope2d 来自于 qwen 2.5 vl 的 Qwen2.5-VL/qwen-vl-finetune/qwenvl/data/rope2d.py 。\nfrom rope2d import get_rope_index_25position_ids, _ = get_rope_index_25(\timage_processor.merge_size,\tinput_ids,\timage_grid_thw=torch.stack(grid_thw_list, dim=0) if grid_thw_list else None,\tvideo_grid_thw=None,)\n\n以下的 data_dict 包含以下元素，作为 model.forward 的 inputs\n\ninput_ids: torch.Size([1, 495])\nlabels: torch.Size([1, 495])\nposition_ids: torch.Size([3, 1, 495])\nattention_mask: torch.Size([1, 495])\npixel_values: torch.Size([1820, 1176])\nimage_grid_thw: torch.Size([2, 3])\n\n[item.shape for item in image_list]\n\n\ndata_dict = {\t\"input_ids\": input_ids,\t\"labels\": labels,\t\"position_ids\": position_ids,\t\"attention_mask\": torch.ones_like(input_ids, dtype=torch.int),}if \"image\" in source:\t# image_list 即大小为 [torch.Size([440, 1176]), torch.Size([1380, 1176])] 的 patch 化图片\t# 因为都是 patch level 的数据，所以需要将其拼接成一个大的 tensor\tdata_dict[\"pixel_values\"] = torch.cat(image_list, dim=0)\t# 每个图片的 grid_thw 都是一个 tensor 为 [time, height, width]，大小一样时 [1,3]，将其拼接成一个大的 tensor\tdata_dict[\"image_grid_thw\"] = torch.cat(\t\t[thw.unsqueeze(0) for thw in grid_thw_list], dim=0\t)for k, v in data_dict.items():\tprint(f\"{k}: {v.shape if isinstance(v, torch.Tensor) else v}\")\n\nVL 模型前向传播Qwen2_5_VLForConditionalGeneration 包含两部分:\n\nbackbone: Qwen2_5_VLModel\nvisual: Qwen2_5_VisionTransformerPretrainedModel\nlanguage_model: Qwen2_5_VLTextModel\n\n\nhead: nn.Linear\n\nQwen2_5_VLModel 的 forward 流程如下:\n\ntext token embedding\n\n# 出自 Qwen2_5_VLTextModeldef get_input_embeddings(self):\treturn self.embed_tokens# 出自 Qwen2_5_VLModeldef get_input_embeddings(self):\treturn self.language_model.get_input_embeddings()inputs_embeds = self.get_input_embeddings()(input_ids)\n\nvision token 填充\n\n# 出自 Qwen2_5_VisionTransformerPretrainedModeldef forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -&gt; torch.Tensor:\t# hidden_states = pixel_values \thidden_states = self.patch_embed(hidden_states)\trotary_pos_emb = self.rot_pos_emb(grid_thw)\twindow_index, cu_window_seqlens = self.get_window_index(grid_thw)\tcu_window_seqlens = torch.tensor(\t\tcu_window_seqlens,\t\tdevice=hidden_states.device,\t\tdtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\t)\tcu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)\tseq_len, _ = hidden_states.size()\thidden_states = hidden_states.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\thidden_states = hidden_states[window_index, :, :]\thidden_states = hidden_states.reshape(seq_len, -1)\trotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\trotary_pos_emb = rotary_pos_emb[window_index, :, :]\trotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\temb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\tposition_embeddings = (emb.cos(), emb.sin())\tcu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\t\tdim=0,\t\t# Select dtype based on the following factors:\t\t#  - FA2 requires that cu_seqlens_q must have dtype int32\t\t#  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\t\t# See https://github.com/huggingface/transformers/pull/34852 for more information\t\tdtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\t)\tcu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\tfor layer_num, blk in enumerate(self.blocks):\t\tif layer_num in self.fullatt_block_indexes:\t\t\tcu_seqlens_now = cu_seqlens\t\telse:\t\t\tcu_seqlens_now = cu_window_seqlens\t\thidden_states = blk(\t\t\thidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings, **kwargs\t\t)\t# 2*2 的 patch hidden states 合并\thidden_states = self.merger(hidden_states)\treverse_indices = torch.argsort(window_index)\thidden_states = hidden_states[reverse_indices, :]\treturn hidden_states# 出自 Qwen2_5_VLModeldef get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\t# 统一数据类型\tpixel_values = pixel_values.type(self.visual.dtype)\t# 获取 patch 的 hidden states，并合并为 image token\timage_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\t# 根据 image_grid_thw 计算每个图片的 token 数量\tsplit_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\t# 根据 image_grid_thw 将 image token 切分为每个图片的 image token\timage_embeds = torch.split(image_embeds, split_sizes)\treturn image_embeds# 获取 image token (按图片切分)image_embeds = self.get_image_features(pixel_values, image_grid_thw)# 不知道啥又合并回去了image_embeds = torch.cat(image_embeds, dim=0)# self.config.image_token_id 就是 &lt;|image_pad|&gt;，即获取 image padding token 数量n_image_tokens = (input_ids == self.config.image_token_id).sum()# 获取 image token 的数据n_image_features = image_embeds.shape[0]# 检查实际计算的 image token 与文本中的 padding token 数量是否一致if n_image_tokens != n_image_features:\traise ValueError(\t\tf\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\t)\n\ntoken embedding 合并\n\n# 获取 image padding token 位置的掩码mask = input_ids == self.config.image_token_id# 扩展维度，获得 text token embedding 同大小的掩码矩阵mask_unsqueezed = mask.unsqueeze(-1)mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)image_mask = mask_expanded.to(inputs_embeds.device)# 将 image_embeds 基于掩码 image_mask 映射到 token embedding 上，这样 vision 和 text 的 token embedding 汇合image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n\n\nlanguage model 的 forward\n\n# 内部 forward 和 LLM 几乎一致了outputs = self.language_model(\tinput_ids=None,\tposition_ids=position_ids,\tattention_mask=attention_mask,\tpast_key_values=past_key_values,\tinputs_embeds=inputs_embeds,\tuse_cache=use_cache,\toutput_attentions=output_attentions,\toutput_hidden_states=output_hidden_states,\treturn_dict=True,\tcache_position=cache_position,\t**kwargs,)output = Qwen2_5_VLModelOutputWithPast(\tlast_hidden_state=outputs.last_hidden_state,\tpast_key_values=outputs.past_key_values,\thidden_states=outputs.hidden_states,\tattentions=outputs.attentions,\trope_deltas=self.rope_deltas,)\n\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(    model_id,     torch_dtype=\"auto\", )\n\n\noutput = model(**data_dict)for k, v in output.items():\tif isinstance(v, torch.Tensor):\t\tprint(f\"{k}: {v.shape}\")\telse:\t\tprint(f\"{k}: {v}\")\n\nloss: torch.Size([])logits: torch.Size([1, 495, 151936])past_key_values: &lt;transformers.cache_utils.DynamicCache object at 0x7f668976e1d0&gt;\n\nLazy Dataset (整合上述处理方式)数据集预处理支持多个数据集合并和采样\nimport re, random, os, torch, copy, jsonfrom PIL import Imagefrom typing import Listfrom torch.utils.data import Datasetfrom torch.nn.utils.rnn import pad_sequencefrom transformers import AutoProcessor, PreTrainedTokenizerfrom rope2d import get_rope_index_25from dataclasses import dataclassfrom typing import Dict, Sequence, Listdemo = {\t\"annotation_path\": \"./demo/single_images.json\",\t\"data_path\": \"./demo/images\",}data_dict = {\t\"demo\": demo,}def parse_sampling_rate(dataset_name):\t\"\"\"\t解析数据集名称中的采样率。\t该函数从数据集名称字符串中提取以百分号（%）结尾的数字，并将其转换为采样率（小数形式）。\t如果数据集名称不包含采样率，则默认返回1.0。\t\"\"\"\tmatch = re.search(r\"%(\\d+)$\", dataset_name)\tif match:\t\treturn int(match.group(1)) / 100.0\treturn 1.0def get_dataset_config_list(dataset_names):\t\"\"\"\t根据提供的数据集名称列表，生成对应的数据集配置字典列表。\t测试用例 1\tdataset_names = [\"demo\"]\t输出结果为\t[{'annotation_path': './demo/single_images.json', 'data_path': './demo/images', 'sampling_rate': 1.0}]\t\"\"\"\tconfig_list = []\tfor dataset_name in dataset_names:\t\tsampling_rate = parse_sampling_rate(dataset_name)\t\tdataset_name = re.sub(r\"%(\\d+)$\", \"\", dataset_name)\t\tif dataset_name in data_dict.keys():\t\t\tconfig = data_dict[dataset_name].copy()\t\t\tconfig[\"sampling_rate\"] = sampling_rate\t\t\tconfig_list.append(config)\t\telse:\t\t\traise ValueError(f\"do not find {dataset_name}\")\treturn config_listdef read_jsonl(path):\twith open(path, \"r\") as f:\t\treturn [json.loads(line) for line in f]\tdef load_datasets(dataset_config_list):\tlist_data_dict = []\tfor config in dataset_config_list:\t\t\t\tannotation_path = config['annotation_path']\t\tdata_path = config['data_path']\t\tsampling_rate = config.get('sampling_rate', 1.0)\t\tfile_format = annotation_path.split(\".\")[-1]\t\tif file_format == \"jsonl\":\t\t\tannotations = read_jsonl(annotation_path)\t\telse:\t\t\tannotations = json.load(open(annotation_path, \"r\"))\t\tif sampling_rate &lt; 1.0:\t\t\tannotations = random.sample(\t\t\t\tannotations, int(len(annotations) * sampling_rate)\t\t\t)\t\t\tprint(f\"sampling {len(annotations)} examples from dataset {config}\")\t\telse:\t\t\tprint(f\"full loading dataset: {config}\")\t\tfor ann in annotations:\t\t\tann[\"data_path\"] = data_path\t\tlist_data_dict += annotations\t\tprint(f\"Total training samples: {len(list_data_dict)}\")\trandom.shuffle(list_data_dict)\treturn list_data_dictdataset_names = [\"demo\", \"demo%80\", \"demo%50\"]configs = get_dataset_config_list(dataset_names)print(configs)\n\n[{'annotation_path': './demo/single_images.json', 'data_path': './demo/images', 'sampling_rate': 1.0}, {'annotation_path': './demo/single_images.json', 'data_path': './demo/images', 'sampling_rate': 0.8}, {'annotation_path': './demo/single_images.json', 'data_path': './demo/images', 'sampling_rate': 0.5}]\n\ndataset 和 data collator 定义class LazySupervisedDataset(Dataset):\t\"\"\"Dataset for supervised fine-tuning.\"\"\"\tdef __init__(self, dataset_use, image_processor, tokenizer, chat_template, default_system_message):\t\tsuper(LazySupervisedDataset, self).__init__()\t\tdataset_names = dataset_use.split(\",\")\t\tdataset_config_list = get_dataset_config_list(dataset_names)\t\tlist_data_dict = load_datasets(dataset_config_list)\t\tself.list_data_dict = list_data_dict\t\tself.image_processor = image_processor\t\tself.tokenizer = copy.deepcopy(tokenizer)\t\tself.tokenizer.chat_template = chat_template\t\tself.default_system_message = default_system_message\t\tself.get_rope_index = get_rope_index_25\tdef __len__(self):\t\treturn len(self.list_data_dict)\t# 包装的 __getitem__ 方法\tdef __getitem__(self, idx):\t\tnum_base_retries = 3\t\t# 首先尝试获取指定索引的样本，如果失败则重试\t\tfor attempt_idx in range(num_base_retries):\t\t\ttry:\t\t\t\tsample = self._get_item(idx)\t\t\t\treturn sample\t\t\texcept Exception as e:\t\t\t\t# sleep 1s in case it is a cloud disk issue\t\t\t\tprint(f\"[Try #{attempt_idx}] Failed to fetch sample {idx}. Exception:\", e)\t\t# 如果获取指定索引的样本失败，则尝试获取下一个样本\t\tfor attempt_idx in range(num_base_retries):\t\t\ttry:\t\t\t\tnext_index = (idx + 1) % len(self.list_data_dict)\t\t\t\t# sample_idx = random.choice(range(len(self)))\t\t\t\tsample = self._get_item(next_index)\t\t\t\treturn sample\t\t\texcept Exception as e:\t\t\t\tprint(f\"[Try other #{attempt_idx}] Failed to fetch sample {next_index}. Exception:\", e,)\t\t# 最后尝试获取第一个样本，不行就抛出异常\t\ttry:\t\t\tsample = self._get_item(idx)\t\t\treturn sample\t\texcept Exception as e:\t\t\traise e\t\t\t\tdef process_single_image(self, image_file:str):\t\t# 读取 Image 为 PIL 对象\t\timage = Image.open(image_file).convert(\"RGB\")\t\t# 处理单个图片为tensor\t\tvisual_processed = self.image_processor.preprocess(image, return_tensors=\"pt\")\t\timage_tensor = visual_processed[\"pixel_values\"]\t\tif isinstance(image_tensor, List):\t\t\timage_tensor = image_tensor[0]\t\tgrid_thw = visual_processed[\"image_grid_thw\"][0]\t\treturn image_tensor, grid_thw\tdef preprocess_qwen_2_visual(self, chat_sources, image_grid_thw_merged:List = [], video_grid_thw_merged:List = []):\t\t# 确保第一个是 human 消息\t\twhile not (\t\t\t(chat_sources[0]['role'] == 'user') or \t\t\t(len(chat_sources) &gt;=2 and chat_sources[0]['role'] == 'system' and chat_sources[1]['role'] == 'user')\t\t):\t\t\tchat_sources = chat_sources[1:]\t\t\t\t# 确保用户消息和大模型消息交叉产生，即配对\t\tif chat_sources[0]['role'] == 'system':\t\t\tassert len(chat_sources) % 2 == 1, \"user messages and assistant messages must be paired.\"\t\t\tfor i in range(1, len(chat_sources), 2):\t\t\t\tassert (chat_sources[i]['role'] == 'user' and chat_sources[i + 1]['role'] == 'assistant'), \"user messages and assistant messages must be paired.\"\t\telse:\t\t\tassert len(chat_sources) % 2 == 0, \"user messages and assistant messages must be paired.\"\t\t\tfor i in range(0, len(chat_sources), 2):\t\t\t\tassert (chat_sources[i]['role'] == 'user' and chat_sources[i + 1]['role'] == 'assistant'), \"user messages and assistant messages must be paired.\"\t\t\tchat_sources = [{\"role\": \"system\", \"content\": self.default_system_message}] + chat_sources # 添加默认 system 消息\t\t\t\t# 这两个变量是用于记录现在处理的图片的index，每个图片的处理就是塞入图片大小相关的 '&lt;|image_pad|&gt;' token。\t\tvisual_replicate_index_image = 0\t\tIGNORE_INDEX = -100\t\tinput_id, target = [], []\t\tfor conv in chat_sources:\t\t\trole  = conv['role']\t\t\tcontent = conv['content']\t\t\t# content 内容只有需要对 user 特殊处理，因为可能有图片和视频相关的内容。\t\t\tif role == 'user':\t\t\t\tif '&lt;image&gt;' in content:\t\t\t\t\tparts = content.split('&lt;image&gt;')\t\t\t\t\tnum_parts = len(parts)\t\t\t\t\tnew_parts = []\t\t\t\t\tfor i in range(num_parts):\t\t\t\t\t\t# 加入被 &lt;image&gt; 分割的文本部分\t\t\t\t\t\tnew_parts.append(parts[i])\t\t\t\t\t\t# 加入图片相关的 token，但最后一次不需要\t\t\t\t\t\tif i != num_parts - 1:\t\t\t\t\t\t\timage_tokens = (\t\t\t\t\t\t\t\t\"&lt;|vision_start|&gt;\"\t\t\t\t\t\t\t\t+ \"&lt;|image_pad|&gt;\" * image_grid_thw_merged[visual_replicate_index_image]\t\t\t\t\t\t\t\t+ \"&lt;|vision_end|&gt;\"\t\t\t\t\t\t\t)\t\t\t\t\t\t\tvisual_replicate_index_image += 1\t\t\t\t\t\t\tnew_parts.append(image_tokens)\t\t\t\t\tcontent = \"\".join(new_parts)\t\t\t\telif '&lt;video&gt;' in content:\t\t\t\t\t# 不做视频相关任务，不写了\t\t\t\t\tpass\t\t\t\telse:\t\t\t\t\tpass\t\t\ttext_conv = [{\"role\": role, \"content\": content}]\t\t\tencode_id = self.tokenizer.apply_chat_template(text_conv)\t\t\tinput_id += encode_id\t\t\tif role in [\"user\", \"system\"]:\t\t\t\t# user 和 system 消息不需要计算损失\t\t\t\ttarget += [IGNORE_INDEX] * len(encode_id) \t\t\telse:\t\t\t\t# assistant 消息需要计算损失，但前缀特殊 token 不需要计算损失\t\t\t\ttarget_mask = encode_id.copy()\t\t\t\ttarget_mask[:3] = [IGNORE_INDEX] * 3 # 忽略开头的 '&lt;|im_start|&gt;system\\n'\t\t\t\ttarget += target_mask\t\tassert len(input_id) == len(target), \\\t\t\tf\"input_id length ({len(input_id)}) != target length ({len(target)})\"\t\tassert visual_replicate_index_image == len(image_grid_thw_merged), \\\t\t\tf\"visual_replicate_index_image ({visual_replicate_index_image}) != len(image_grid_thw_merged) ({len(image_grid_thw_merged)})\"\t\tinput_ids = torch.tensor([input_id], dtype=torch.long)\t\tlabels = torch.tensor([target], dtype=torch.long)\t\treturn dict(\t\t\tinput_ids=input_ids,\t\t\tlabels=labels,\t\t)\tdef _get_item(self, idx):\t\tsource = self.list_data_dict[idx]\t\timage_grid_thw_merged = []\t\timage_grid_thw_list = []\t\tvideo_grid_thw_merged = []\t\tvideo_grid_thw_list = []\t\tif \"image\" in source:\t\t\timage_folder = source[\"data_path\"]\t\t\timage_file = source[\"image\"]\t\t\tif not isinstance(image_file, List):\t\t\t\timage_file = [image_file]\t\t\t\t\t\timage_file = [\t\t\t\tos.path.join(image_folder, file) for file in image_file\t\t\t]\t\t\tresults = [\t\t\t\tself.process_single_image(file) \t\t\t\tfor file in image_file\t\t\t]\t\t\t# grid_thw 是 time height width 的缩写\t\t\timage_list, image_grid_thw_list = zip(*results)\t\t\t# 计算 grid_thw 的点乘 / merge 数量的2次方，即 image 投影后的占用 token 数量\t\t\t# 由于 qwen 2.5vl 是将相邻的 patch 合并成一个 token，所以需要除以 merge_size 的平方，才得到实际的 token 数量\t\t\timage_grid_thw_merged = copy.deepcopy(image_grid_thw_list)\t\t\timage_grid_thw_merged = [\t\t\t\tgrid_thw.prod() // image_processor.merge_size ** 2\t\t\t\tfor grid_thw in image_grid_thw_merged\t\t\t]\t\tif \"video\" in source:\t\t\traise NotImplementedError(\"video is not supported yet\")\t\t\t\tchat_sources = copy.deepcopy(source[\"conversations\"])\t\tif \"image\" not in source and \"video\" not in source:\t\t\tdata_dict = self.preprocess_qwen_2_visual(\t\t\t\tchat_sources, image_grid_thw_merged=[]\t\t\t)\t\t\tposition_ids = (\t\t\t\ttorch.arange(0, data_dict[\"input_ids\"].size(1))\t\t\t\t.view(1, -1)\t\t\t\t.unsqueeze(0)\t\t\t\t.expand(3, -1, -1)\t\t\t)\t\telse:\t\t\t\t\t\tdata_dict = self.preprocess_qwen_2_visual(\t\t\t\tchat_sources, \t\t\t\timage_grid_thw_merged=image_grid_thw_merged if \"image\" in source else [],\t\t\t\tvideo_grid_thw_merged=video_grid_thw_merged if \"video\" in source else []\t\t\t)\t\t\tposition_ids, _ = self.get_rope_index(\t\t\t\timage_processor.merge_size,\t\t\t\tdata_dict[\"input_ids\"],\t\t\t\timage_grid_thw=torch.stack(image_grid_thw_list, dim=0) if image_grid_thw_list else None,\t\t\t\tvideo_grid_thw=None,\t\t\t)\t\tdata_dict[\"position_ids\"] = position_ids\t\tdata_dict[\"attention_mask\"] = torch.ones_like(data_dict[\"input_ids\"], dtype=torch.int)\t\tif \"image\" in source:\t\t\t# image_list 即大小为 [torch.Size([440, 1176]), torch.Size([1380, 1176])] 的 patch 化图片\t\t\t# 因为都是 patch level 的数据，所以需要将其拼接成一个大的 tensor\t\t\tdata_dict[\"pixel_values\"] = torch.cat(image_list, dim=0)\t\t\t# 每个图片的 grid_thw 都是一个 tensor 为 [time, height, width]，大小一样时 [1,3]，将其拼接成一个大的 tensor\t\t\tdata_dict[\"image_grid_thw\"] = torch.cat(\t\t\t\t[thw.unsqueeze(0) for thw in image_grid_thw_list], dim=0\t\t\t)\t\telif \"video\" in self.list_data_dict[i]:\t\t\traise NotImplementedError(\"video is not supported yet\")\t\treturn data_dictdef pad_and_cat(tensor_list):\tmax_length = max(tensor.shape[2] for tensor in tensor_list)\tpadded_tensors = []\tfor tensor in tensor_list:\t\tpad_length = max_length - tensor.shape[2]\t\t# 在原 tensor 后面填充 pad_length 个 1\t\tpadded_tensor = torch.nn.functional.pad(tensor, (0, pad_length), \"constant\", 1)\t\tpadded_tensors.append(padded_tensor)\tstacked_tensor = torch.cat(padded_tensors, dim=1)\treturn stacked_tensor@dataclassclass DataCollatorForSupervisedDataset(object):\ttokenizer: PreTrainedTokenizer\tdef __call__(self, instances: Sequence[Dict]) -&gt; Dict[str, torch.Tensor]:\t\t# input_ids, labels, position_ids, attention_masks 的 padding 处理\t\tinput_ids, labels, position_ids, attention_masks = tuple(\t\t\t[instance[key] for instance in instances]\t\t\tfor key in (\"input_ids\", \"labels\", \"position_ids\", \"attention_mask\")\t\t)\t\tinput_ids = [ids.squeeze(0) for ids in input_ids]\t\tlabels = [ids.squeeze(0) for ids in labels]\t\tattention_masks = [ids.squeeze(0) for ids in attention_masks]\t\t# input_ids 和 labels 分别用 padding token 和 ignore index 填充\t\tinput_ids = pad_sequence(\t\t\tinput_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\t\t)\t\tlabels = pad_sequence(\t\t\tlabels, batch_first=True, padding_value=-100\t\t)\t\tattention_masks = pad_sequence(\t\t\tattention_masks, batch_first=True, padding_value=0\t\t)\t\tposition_ids = pad_and_cat(position_ids)\t\t# 长度截断\t\tinput_ids = input_ids[:, : self.tokenizer.model_max_length]\t\tlabels = labels[:, : self.tokenizer.model_max_length]\t\tposition_ids = position_ids[:, : self.tokenizer.model_max_length]\t\tattention_masks = attention_masks[:, : self.tokenizer.model_max_length]\t\tbatch = dict(\t\t\tinput_ids=input_ids,\t\t\tlabels=labels,\t\t\tposition_ids=position_ids,\t\t\tattention_mask=attention_masks,\t\t)\t\t# images 不存在长度上的不一致，只是 batch of patch 上会不同，所以在 batch 维度上拼接\t\timages = list(\t\t\tinstance[\"pixel_values\"]\t\t\tfor instance in instances\t\t\tif \"pixel_values\" in instance\t\t)\t\tif len(images) != 0:\t\t\tconcat_images = torch.cat([image for image in images], dim=0)\t\t\tgrid_thw = [\t\t\t\tinstance[\"image_grid_thw\"]\t\t\t\tfor instance in instances\t\t\t\tif \"image_grid_thw\" in instance\t\t\t]\t\t\tgrid_thw = torch.cat(grid_thw, dim=0)\t\telse:\t\t\tconcat_images = None\t\t\tgrid_thw = None\t\tvideos = list(\t\t\tinstance[\"pixel_values_videos\"]\t\t\tfor instance in instances\t\t\tif \"pixel_values_videos\" in instance\t\t)\t\t\t\tif len(videos) != 0:\t\t\traise NotImplementedError(\"video is not supported yet\")\t\telse:\t\t\tconcat_videos = None\t\t\tvideo_grid_thw = None\t\tbatch[\"pixel_values\"] = concat_images\t\tbatch[\"image_grid_thw\"] = grid_thw\t\tbatch[\"pixel_values_videos\"] = concat_videos\t\tbatch[\"video_grid_thw\"] = video_grid_thw\t\treturn batch\n\ndummy data 测试model_id = '../../../DC/qwen2.5vl-3b-ins'processor = AutoProcessor.from_pretrained(model_id, use_fast=True)image_processor = processor.image_processortokenizer = processor.tokenizerconfig = dict(\tdataset_use = \"demo\",\timage_processor = image_processor,\ttokenizer = tokenizer,\tchat_template = \"{% for message in messages %}{{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|im_start|&gt;assistant\\n' }}{% endif %}\",\tdefault_system_message = \"You are a helpful assistant.\")dataset = LazySupervisedDataset(**config)data_collator = DataCollatorForSupervisedDataset(\ttokenizer=tokenizer)\n\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n\n\nfull loading dataset: {'annotation_path': './demo/single_images.json', 'data_path': './demo/images', 'sampling_rate': 1.0}\nTotal training samples: 2\n\ndef print_data_dict(data_dict):\tfor k, v in data_dict.items():\t\tprint(f\"{k}: {v.shape if isinstance(v, torch.Tensor) else v}\")item_1 = dataset[0]item_2 = dataset[1]print_data_dict(item_1)print(20*'=')item_batch = data_collator([item_1, item_2])print_data_dict(item_batch)\n\ninput_ids: torch.Size([1, 495])\nlabels: torch.Size([1, 495])\nposition_ids: torch.Size([3, 1, 495])\nattention_mask: torch.Size([1, 495])\npixel_values: torch.Size([1820, 1176])\nimage_grid_thw: torch.Size([2, 3])\n====================\ninput_ids: torch.Size([2, 495])\nlabels: torch.Size([2, 495])\nposition_ids: torch.Size([3, 2, 495])\nattention_mask: torch.Size([2, 495])\npixel_values: torch.Size([2600, 1176])\nimage_grid_thw: torch.Size([3, 3])\npixel_values_videos: None\nvideo_grid_thw: None\n\nmain train loopVLM 微调和 LLM 微调的另外一个不同之处在于 VLM 是散装的由各个模块组合而成的，因此每个模块都可以独立或者联合训练。一般按照\n\nmerger: Cross-modal Fusion\nmerger + vit: vision \nmerger + vit + LLM / just LLM: LM\n\nfrom transformers import Trainer, TrainingArguments# 设置模型的参数，冻结/解冻视觉模型、merger 和 LLM 模型def set_model(model_args, model):    if model_args.tune_mm_vision:        for n, p in model.visual.named_parameters():            p.requires_grad = True    else:        for n, p in model.visual.named_parameters():            p.requires_grad = False    if model_args.tune_mm_mlp:        for n, p in model.visual.merger.named_parameters():            p.requires_grad = True    else:        for n, p in model.visual.merger.named_parameters():            p.requires_grad = False\t# head 是和 LM 搭配的，需要同时训练/冻结    if model_args.tune_mm_llm:        for n, p in model.model.named_parameters():            p.requires_grad = True        model.lm_head.requires_grad = True    else:        for n, p in model.model.named_parameters():            p.requires_grad = False        model.lm_head.requires_grad = Falsemodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\tmodel_id,\tcache_dir=\"qwenvl-train-test\",\ttorch_dtype=torch.bfloat16,)data_module = dict(\ttrain_dataset = dataset,\tdata_collator = data_collator,)model_args = dict(\t# 分别时训练 语言模型、VL merger、视觉模型 的 flag\ttune_mm_llm = True,\ttune_mm_mlp = True,\ttune_mm_vision = True,)training_args = TrainingArguments(    learning_rate = 5e-5,)trainer = Trainer(\tmodel=model, processing_class=tokenizer, args=training_args, **data_module)\n\n数据集样例这是我测试时用的 demo 的文件夹\n├── images│   ├── 10095.png│   ├── 10149.png│   └── COCO_train2014_000000580957.jpg└── single_images.json\n分别是图片文件夹和文本文件。single_images.json 内部信息如下\n[    {      \"image\": \"10095.png\",      \"conversations\": [        {          \"role\": \"user\",          \"content\": \"Is the value of Favorable 38 in 2015?\\n&lt;image&gt;\"        },        {          \"role\": \"assistant\",          \"content\": \"Yes\"        }      ]    },    {      \"image\": [\"10149.png\", \"COCO_train2014_000000580957.jpg\"],      \"conversations\": [        {          \"role\": \"user\",          \"content\": \"&lt;image&gt;\\nIn which year the value was 51?\\n&lt;image&gt;\"        },        {          \"role\": \"assistant\",          \"content\": \"2014\"        }      ]    }]\n\n","categories":["多模态"],"tags":["多模态"]},{"title":"cuda 显存占用优化","url":"/2025/07/13/LLM-basic-series/memory_optimization/","content":"记录下 transformers 模型简单的减少显存的方式\n\n半精度加载\n混合精度训练\n激活 checkpoint\nlora\nflash attention\n\n但不包含例如 FSDP 和 deepspeed 之类的相关代码，因为我是在 jupyter notebook 里实验代码的。\n显存中分为两部分:\n\nallocated: 当前正在使用中的显存（张量等活跃对象），例如模型参数、梯度、优化器状态、活跃变量等\nreserved: 分配给 PyTorch CUDA 内存池，但暂时未使用的空间，例如中间变量释放后留下的内存；未来训练可能会复用，除非用 torch.cuda.empty_cache() 主动释放。\n\n非训练相关代码import json, torchfrom copy import deepcopyfrom datasets import load_datasetfrom trl import SFTConfig, SFTTrainerfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq, Trainer, TrainingArgumentsmodel_name_or_path = \"Qwen/Qwen2-0.5B-Instruct\"        def get_memory_usage():    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    if torch.cuda.is_available():        allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)        reserved = torch.cuda.memory_reserved(device) / (1024 ** 3)        max_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 3)        max_reserved = torch.cuda.max_memory_reserved(device) / (1024 ** 3)        print(f\"[Current] Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")        print(f\"[Peak]    Max Allocated: {max_allocated:.2f} GB, Max Reserved: {max_reserved:.2f} GB\")    else:        print(\"CUDA not available\")        get_memory_usage()\n\n\ndataset = load_dataset(\"trl-lib/ultrafeedback-gpt-3.5-turbo-helpfulness\", split=\"train[:100]\")# print(json.dumps(dataset[0], indent=2, ensure_ascii=False))tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)if tokenizer.pad_token is None:\ttokenizer.pad_token = tokenizer.eos_token# 默认模板会为每个对话加上 system prompt，但我们处理数据时是分别对每个 message 进行处理的，所以这里不需要再加上 system prompt。tokenizer.chat_template = \"{% for message in messages %}{{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|im_start|&gt;assistant\\n' }}{% endif %}\"\n\n\ndef preprocess(example):\tinput_ids = []\tlabels = []\tsystem_prompt = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\tprompt = example[\"prompt\"]\tcompletion = example[\"completion\"]\ttext = tokenizer.apply_chat_template(system_prompt, tokenize=False)\tencode_ids = tokenizer.encode(text)\tinput_ids.extend(encode_ids)\tlabels.extend([-100] * len(encode_ids))\ttext = tokenizer.apply_chat_template(prompt, tokenize=False)\tencode_ids = tokenizer.encode(text)\tinput_ids.extend(encode_ids)\tlabels.extend([-100] * len(encode_ids))\ttext = tokenizer.apply_chat_template(completion, tokenize=False)\tencode_ids = tokenizer.encode(text)\tinput_ids.extend(encode_ids)\ttarget_encode_ids = deepcopy(encode_ids)\ttarget_encode_ids[:3] = [-100] * 3  # 前三个 token, '&lt;|im_start|&gt;user\\n', 不需要计算 loss\tlabels.extend(target_encode_ids)\tattention_mask = [1] * len(input_ids)\treturn dict(\t\tinput_ids=input_ids,\t\tlabels=labels,\t\tattention_mask=attention_mask,\t)tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)data_collator = DataCollatorForSeq2Seq(\ttokenizer=tokenizer,\tpadding='longest',\tmax_length=512,\treturn_tensors=\"pt\",)\n\nvanilla以下是很举出的训练代码，几乎没有指定参数。\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path)training_args = TrainingArguments(    output_dir=\"./train_output\",    per_device_train_batch_size=2,    gradient_accumulation_steps=6,    num_train_epochs=1,    logging_steps=1,    save_strategy=\"no\",    # bf16=True,    # 启用 float16 混合精度    deepspeed=None,    report_to=[],)trainer = Trainer(    model=model,    args=training_args,    train_dataset=tokenized_dataset,    tokenizer=tokenizer,    data_collator=data_collator,)print(\"Before training:\")get_memory_usage()trainer.train()print(\"After training:\")get_memory_usage()\n\nBefore training:[Current] Allocated: 1.84 GB, Reserved: 1.95 GB[Peak]    Max Allocated: 1.84 GB, Max Reserved: 1.95 GB\nAfter training:[Current] Allocated: 5.61 GB, Reserved: 20.54 GB[Peak]    Max Allocated: 19.26 GB, Max Reserved: 20.54 GB\n当没有指定模型数据类型时，以 float32 加载的，所以 allocated = 0.5 billion * 4 B (float32是四字节) = 2GB。训练后，优化器的一二阶参数 + 模型本体 = 3 * 0.5 billion * 4 B = 6GB, 训练完后梯度已经被清除了。\nhalf-precisionmodel = AutoModelForCausalLM.from_pretrained(\tmodel_name_or_path,    torch_dtype=torch.bfloat16,)training_args = TrainingArguments(    output_dir=\"./train_output\",    per_device_train_batch_size=2,    gradient_accumulation_steps=6,    num_train_epochs=1,    logging_steps=1,    save_strategy=\"no\",    # bf16=True,    # 启用 float16 混合精度    deepspeed=None,    report_to=[],)trainer = Trainer(    model=model,    args=training_args,    train_dataset=tokenized_dataset,    tokenizer=tokenizer,    data_collator=data_collator,)print(trainer.args.bf16)print(\"Before training:\")get_memory_usage()trainer.train()print(\"After training:\")get_memory_usage()\n\nBefore training:[Current] Allocated: 0.93 GB, Reserved: 0.97 GB[Peak]    Max Allocated: 0.93 GB, Max Reserved: 0.97 GB\nAfter training:[Current] Allocated: 2.80 GB, Reserved: 16.16 GB[Peak]    Max Allocated: 12.05 GB, Max Reserved: 16.16 GB\n用半精度 bf16 加载后，看显存占用，整体都是用 bf16。\nmixed precision trainingmodel = AutoModelForCausalLM.from_pretrained(\tmodel_name_or_path,    torch_dtype=torch.bfloat16,)training_args = TrainingArguments(    output_dir=\"./train_output\",    per_device_train_batch_size=2,    gradient_accumulation_steps=6,    num_train_epochs=1,    logging_steps=1,    save_strategy=\"no\",    bf16=True,    # 启用 float16 混合精度    deepspeed=None,    report_to=[],)trainer = Trainer(    model=model,    args=training_args,    train_dataset=tokenized_dataset,    tokenizer=tokenizer,    data_collator=data_collator,)print(trainer.args.bf16)print(\"Before training:\")get_memory_usage()trainer.train()print(\"After training:\")get_memory_usage()\n\nBefore training:[Current] Allocated: 0.93 GB, Reserved: 0.97 GB[Peak]    Max Allocated: 0.93 GB, Max Reserved: 0.97 GB\nAfter training:[Current] Allocated: 2.80 GB, Reserved: 16.16 GB[Peak]    Max Allocated: 12.05 GB, Max Reserved: 16.16 GB\ngradient checkpointinggradient_checkpointing=True：是一种训练节省显存的技术，通过丢弃激活、在反向传播时重新计算来减少显存。原先中间激活都被保留，启动 gradient_checkpointing 后，仅会保留部分中间激活。use_cache=True：在模型前向传播中缓存 past_key_values，用于加速生成（如推理时的 decoder caching）。gradient_checkpointing 和 use_cache 并不兼容可通过以下代码看到哪些层被 checkpint。\nfor name, module in model.named_modules():    if hasattr(module, \"gradient_checkpointing\") or \"Block\" in name:        print(name, type(module))\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path)model.config.use_cache = False # 显式禁用缓存model.gradient_checkpointing_enable()  # 启用梯度检查点training_args = TrainingArguments(    output_dir=\"./train_output\",    per_device_train_batch_size=2,    gradient_accumulation_steps=6,    num_train_epochs=1,    logging_steps=1,    save_strategy=\"no\",    bf16=True,    # 启用 float16 混合精度    deepspeed=None,    report_to=[],)trainer = Trainer(    model=model,    args=training_args,    train_dataset=tokenized_dataset,    tokenizer=tokenizer,    data_collator=data_collator,)print(\"Before training:\")get_memory_usage()trainer.train()print(\"After training:\")get_memory_usage()\n\nBefore training:[Current] Allocated: 1.84 GB, Reserved: 1.95 GB[Peak]    Max Allocated: 1.84 GB, Max Reserved: 1.95 GB\nAfter training:[Current] Allocated: 5.55 GB, Reserved: 15.49 GB[Peak]    Max Allocated: 12.17 GB, Max Reserved: 15.49 GB\n用了 checkpoint 后，激活显存占用少了近 1/4。\nloramodel = AutoModelForCausalLM.from_pretrained(model_name_or_path)from peft import get_peft_model, LoraConfig, TaskTypepeft_config = LoraConfig(    task_type=TaskType.CAUSAL_LM,    inference_mode=False,    r=8,    lora_alpha=32,    lora_dropout=0.05,    target_modules=[\"q_proj\", \"v_proj\"],  # Qwen 使用q_proj/v_proj)model = get_peft_model(model, peft_config)training_args = TrainingArguments(    output_dir=\"./train_output\",    per_device_train_batch_size=2,    gradient_accumulation_steps=6,    num_train_epochs=1,    logging_steps=1,    save_strategy=\"no\",    bf16=True,    # 启用 float16 混合精度    deepspeed=None,    report_to=[],)trainer = Trainer(    model=model,    args=training_args,    train_dataset=tokenized_dataset,    tokenizer=tokenizer,    data_collator=data_collator,)print(\"Before training:\")get_memory_usage()trainer.train()print(\"After training:\")get_memory_usage()\n\nBefore training:[Current] Allocated: 1.84 GB, Reserved: 1.95 GB[Peak]    Max Allocated: 1.84 GB, Max Reserved: 1.95 GB\nAfter training:[Current] Allocated: 1.86 GB, Reserved: 17.88 GB[Peak]    Max Allocated: 13.29 GB, Max Reserved: 17.88 GB\n训练前后 allocated 都没有大的变化，因为 lora 训练参数量非常少，因此对应的优化器参数量也少，但对激活也稍微有点影响，减了一点。\nflahs attentionmodel = AutoModelForCausalLM.from_pretrained(\tmodel_name_or_path,    attn_implementation=\"flash_attention_2\",    torch_dtype=torch.bfloat16,)training_args = TrainingArguments(    output_dir=\"./train_output\",    per_device_train_batch_size=2,    gradient_accumulation_steps=6,    num_train_epochs=1,    logging_steps=1,    save_strategy=\"no\",    # bf16=True,    # 启用 float16 混合精度    deepspeed=None,    report_to=[],)trainer = Trainer(    model=model,    args=training_args,    train_dataset=tokenized_dataset,    tokenizer=tokenizer,    data_collator=data_collator,)print(\"Before training:\")get_memory_usage()trainer.train()print(\"After training:\")get_memory_usage()\n\nBefore training:[Current] Allocated: 0.93 GB, Reserved: 0.97 GB[Peak]    Max Allocated: 0.93 GB, Max Reserved: 0.97 GB\nAfter training:[Current] Allocated: 2.80 GB, Reserved: 15.93 GB[Peak]    Max Allocated: 11.62 GB, Max Reserved: 15.93 GB\nflash attention 也减少了一点激活的显存占用。\nall togethermodel = AutoModelForCausalLM.from_pretrained(\tmodel_name_or_path,    attn_implementation=\"flash_attention_2\",    torch_dtype=torch.bfloat16,)model.config.use_cache = False # 显式禁用缓存model.gradient_checkpointing_enable()  # 启用梯度检查点from peft import get_peft_model, LoraConfig, TaskTypepeft_config = LoraConfig(    task_type=TaskType.CAUSAL_LM,    inference_mode=False,    r=8,    lora_alpha=32,    lora_dropout=0.05,    target_modules=[\"q_proj\", \"v_proj\"],  # Qwen 使用q_proj/v_proj)model = get_peft_model(model, peft_config)training_args = TrainingArguments(    output_dir=\"./train_output\",    per_device_train_batch_size=2,    gradient_accumulation_steps=6,    num_train_epochs=1,    logging_steps=1,    save_strategy=\"no\",    # bf16=True,    # 启用 float16 混合精度    deepspeed=None,    report_to=[],)trainer = Trainer(    model=model,    args=training_args,    train_dataset=tokenized_dataset,    tokenizer=tokenizer,    data_collator=data_collator,)print(\"Before training:\")get_memory_usage()trainer.train()print(\"After training:\")get_memory_usage()\n\nBefore training:[Current] Allocated: 0.93 GB, Reserved: 0.97 GB[Peak]    Max Allocated: 0.93 GB, Max Reserved: 0.97 GB\nAfter training:[Current] Allocated: 0.95 GB, Reserved: 13.53 GB[Peak]    Max Allocated: 6.02 GB, Max Reserved: 13.53 GB\n将上述方法结合起来，peak 显存占用在模型方面减少 2/3，激活减少 3/10。\n","categories":["LLM 基础"]},{"title":"DPO trainer - by trl","url":"/2025/07/16/Reinforcement_learning/DPO_trainer_exp/","content":"简单记录下用 trl 库的 DPO trainer 的训练效果，源码出自于 trl 官方 https://huggingface.co/docs/trl/en/dpo_trainer 给的示例代码。DPO trainer 的底层代码在之前的 DPO 复现的 blog 中提到过，这次就直接用 DPO trainer，不涉及任何底层代码。\n偏好数据集 preference datasettrl 中关于数据集有一个专门的介绍 https://huggingface.co/docs/trl/main/en/dataset_formats。DPO 用的是 preference dataset，主要有两个字段 chosen 和 rejected。\n{  \"chosen\": [\t{\t  \"content\": \"Host a virtual panel discussion on the ethical implications of genetic engineering.\",\t  \"role\": \"user\"\t},\t{\t  \"content\": \"Hello! I'm here to help you host a virtual panel discussion on the ethical implications of genetic engineering. This is a very important and timely topic, as genetic engineering has the potential to revolutionize many areas of our lives, from healthcare to agriculture. However, it also raises important ethical considerations, such as privacy concerns, potential discrimination, and the possibility of unintended consequences.\\n\\nTo begin, I'll provide a brief overview of genetic engineering. Genetic engineering involves the deliberate modification of an organism's genetic material to produce a desired trait or characteristic. This can be done using a variety of techniques, including gene editing and gene therapy. The field of genetic engineering is rapidly advancing, and it has the potential to treat diseases, improve crop yields, and even extend human lifespan.\\n\\nHowever, as with any powerful technology, there are potential risks and benefits to consider. The ethical implications of genetic engineering are complex and multifaceted, and they require careful consideration from experts in the field, as well as from the general public.\\n\\nTo explore these ethical implications, I've assembled a panel of experts who will share their insights and perspectives on this topic. Our panel includes:\\n\\n* Dr. Jane Smith, a leading geneticist who has made significant contributions to the field of gene editing.\\n* Dr. John Doe, a bioethicist who has written extensively on the ethical implications of genetic engineering.\\n* Ms. Susan Johnson, a patient advocate who has firsthand experience with genetic testing and gene therapy.\\n* Mr. David Kim, a lawyer who specializes in intellectual property and privacy law as it relates to genetic information.\\n\\nI will now turn the floor over to our panelists, who will each provide their perspective on the ethical implications of genetic engineering. Please feel free to ask questions and engage in a respectful and constructive dialogue.\\n\\nDr. Jane Smith, could you begin by discussing the potential benefits of genetic engineering?\\n\\n(Please note that any responses should be socially unbiased and positive in nature, and should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.)\",\t  \"role\": \"assistant\"\t}  ],  \"rejected\": [\t{\t  \"content\": \"Host a virtual panel discussion on the ethical implications of genetic engineering.\",\t  \"role\": \"user\"\t},\t{\t  \"content\": \"Sure, I can definitely help you host a virtual panel discussion on the ethical implications of genetic engineering. Here's a suggested format for the discussion:\\n\\n1. Introduction:\\nBegin by introducing the topic and the discussants who will participate in the panel discussion. They could include experts in genetic engineering, bioethics, philosophy, law, sociology, and science policy.\\n\\n2. Understanding Genetic Engineering:\\nBefore delving into ethical implications, provide a brief overview of what genetic engineering is and its different types, such as gene editing, gene therapy, and gene drives. Also, discuss some of the current advancements and applications in genetic engineering.\\n\\n3. Ethical Implications:\\nThe discussion can begin by exploring the ethical implications of genetic engineering. Here are some topics you can consider:\\n\\n   a. Privacy and Informed Consent: Explore the dangers of confidentiality breaches in genetic testing and the transparency issues surrounding the use of genetic information. Also, discuss the importance of informed consent.\\n\\n   b. Discrimination: Discuss the concern that genetic information could be used for discrimination in areas like health insurance or employment.\\n\\n   c. Inequality: Discuss the worry that access to genetic engineering will only be available to the rich, leading to social inequalities.\\n\\n   d. Designer Babies: Examine the ethical implications of genetic engineering for selecting physical, intellectual, or moral traits in children, often referred to as \\\"designer babies\\\".\\n\\n   e. Genetic Exploitation: Discuss how genetic engineering could be exploited for extreme human enhancement beyond normal human capacity or for non-therapeutic purposes.\\n\\n   f. Environmental Impact: Look at the potential environmental consequences of genetic engineering, including the risk of unintended spreading of modifications into non-target species, particularly through gene drives.\\n\\n   g. Responsibility: Discuss the ethical responsibility of both scientific researchers and society as a whole in the field of genetic engineering.\\n\\n4. Legal Implications:\\nEach nation or jurisdiction has its own legal rules governing genetic engineering. Overview the current legal landscape of genetic engineering and discuss the potential need for new legislation to regulate the technology.\\n\\n5. Philosophical Implications:\\nDiscuss philosophical questions like the nature of human identity, the meaning of human life, and whether genetic engineering might conflict or enhance the flourishing of human life.\\n\\n6. Q&amp;A:\\nOpen the floor for questions from the audience. This is an excellent opportunity for the panel to address any pressing concerns or issues that were not discussed during the main part of the discussion.\\n\\nThis panel discussion should spark important conversations about the ethical implications of genetic engineering. The goal is to inform and engage the audience on these complex issues, stimulating further thought and debate outside of the panel discussion.\",\t  \"role\": \"assistant\"\t}  ],  \"score_chosen\": 8.0,  \"score_rejected\": 8.0}\n\nchosen 和 rejected 是有公共 messages 前缀的，底层会区分出公共部分，loss只会算到非公共部分 completion 上。\n代码和脚本以下是官网的代码\n# train_dpo.pyfrom datasets import load_datasetfrom trl import DPOConfig, DPOTrainerfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")train_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")training_args = DPOConfig(output_dir=\"Qwen2-0.5B-DPO\")trainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)trainer.train()\n\n执行脚本\naccelerate launch train_dpo.py\n\n\n\n我自己用了4张4090显卡(24GB显存)，增加了一些减少显存的处理，PS: flash_attention_2 和 lora 与 accelerate 有点冲突，我在网上没有找到对应的解决方法，所以还是用的全量微调。\n以下是我的代码\n# train_dpo.pyimport osos.environ['WANDB_MODE'] = 'offline'  # Disable Weights &amp; Biases loggingimport torchfrom datasets import load_datasetfrom trl import DPOConfig, DPOTrainerfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained(\t\"Qwen/Qwen2-0.5B-Instruct\",\t# attn_implementation=\"flash_attention_2\",\ttorch_dtype=torch.bfloat16,)model.config.use_cache = False  # Disable cache for DPO trainingmodel.gradient_checkpointing_enable()# ====== peft 和 acclerate 同时使用会有问题，暂时注释掉 ======# from peft import get_peft_model, LoraConfig, TaskType# peft_config = LoraConfig(#     task_type=TaskType.CAUSAL_LM,#     inference_mode=False,#     r=8,#     lora_alpha=32,#     lora_dropout=0.05,#     target_modules=[\"q_proj\", \"v_proj\"],  # Qwen 使用q_proj/v_proj# )# model = get_peft_model(model, peft_config)tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train[:10000]\")dataset = dataset.train_test_split(test_size=0.1, seed=42)train_dataset = dataset['train']test_dataset = dataset['test']training_args = DPOConfig(\toutput_dir=\"Qwen2-0.5B-DPO\",\tper_device_train_batch_size=2,\tper_device_eval_batch_size=4,\tmax_length=1024,\tgradient_accumulation_steps=8,\tnum_train_epochs=3,\treport_to=['wandb'],\tbf16=True,\tlearning_rate=5e-5,\tlr_scheduler_type=\"cosine\",\tlogging_steps=1,\teval_strategy=\"steps\",\teval_steps=100,)trainer = DPOTrainer(\tmodel=model, \targs=training_args, \tprocessing_class=tokenizer, \ttrain_dataset=train_dataset,\teval_dataset=test_dataset,)trainer.train()\n\n以下是acclerate config\ncompute_environment: LOCAL_MACHINEdebug: falsedistributed_type: MULTI_GPUdowncast_bf16: 'no'enable_cpu_affinity: truegpu_ids: 4,5,6,7machine_rank: 0main_training_function: mainmixed_precision: bf16num_machines: 1num_processes: 4rdzv_backend: staticsame_network: truetpu_env: []tpu_use_cluster: falsetpu_use_sudo: falseuse_cpu: false\n\n建议除非服务器配置类似，还是自己执行 accelerate config 创建配置\n执行命令\naccelerate launch --config_file config.yaml train_dpo.py\n\n需要自己调整下 max_length, 我用以下代码检视合理的长度限制\nchosen_lengths = [len(tokenizer.apply_chat_template(item['chosen'])) for item in train_dataset]rejected_lengths = [len(tokenizer.apply_chat_template(item['rejected'])) for item in train_dataset]print(f\"chosen 平均长度: {sum(chosen_lengths)/len(chosen_lengths):.2f}\")print(f\"rejected 平均长度: {sum(rejected_lengths)/len(rejected_lengths):.2f}\")import matplotlib.pyplot as pltplt.plot(chosen_lengths, label='Chosen Lengths', alpha=0.5)import numpy as npfor q in [25, 50, 75, 80, 85, 90, 95]:\tprint(f\"{q}th percentile:\", np.percentile(chosen_lengths, q))\n\n\n这个图里是完整数据集的长度，结果可能看起来虚高，平均1500，实际没那么高，最好用百分位数看下\n25th percentile: 220.050th percentile: 399.075th percentile: 609.080th percentile: 671.085th percentile: 748.090th percentile: 854.095th percentile: 1009.0\n\n大部分数据都是 1000 以下，太长的就截断算了。\nplt.hist(chosen_lengths, bins=100, alpha=0.7)plt.xlabel('Length')plt.ylabel('Frequency')plt.title('Histogram of Chosen Lengths')plt.show()\n\n用统计柱状图显示更合理点。\n\n训练结果10k数据\n\n\n\n\n效果在第1个epoch结束时变得很明显，之后reward acc就一直在0.95以上，可惜的是 chosen 的 reward 没有比一开始的值大，rejected 的 reward 明显降了，感觉 loss 对 rejected 的作用挺大的，但对 chosen 的提升不明显。我只用了部分数据, 10k/62k，在四卡4090上跑了20min。\n虽然训练集上 acc 很高，但实际测试集不是很高。\n\n62k数据\n\n\n\n\n评估结果上看和10k数据没有多大差别\n\n\n\n\n\n可能0.5B能力有限吧，reward acc 都没有破 70%\n","categories":["强化学习"]},{"title":"GRPO-trainer-HF 长度奖励的文本压缩任务","url":"/2025/07/31/Reinforcement_learning/GRPO_length_reward/","content":"以下是出于 huggingface trl GRPO trainer 的教程的代码\n数据和代码使用的是 TLDR dataset ， TLDR 指的是 Too Long Didn’t Read，代表太长了不想看，这个数据就是提供压缩前后的数据。以下是一组示例数据。\n# promptSUBREDDIT: r/relationshipsTITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insultingPOST: Not sure if this belongs here but it's worth a try.Backstory:When I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact.Now: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down.His friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid.So I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.TL;DR:  # completionI still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?\n\n代码如下:\n# train_grpo.pyfrom datasets import load_datasetfrom trl import GRPOConfig, GRPOTrainerdataset = load_dataset(\"trl-lib/tldr\", split=\"train\")# Define the reward function, which rewards completions that are close to 20 charactersdef reward_len(completions, **kwargs):    return [-abs(20 - len(completion)) for completion in completions]training_args = GRPOConfig(output_dir=\"Qwen2-0.5B-GRPO\")trainer = GRPOTrainer(    model=\"Qwen/Qwen2-0.5B-Instruct\",    reward_funcs=reward_len,    args=training_args,    train_dataset=dataset,)trainer.train()\n\n其中 reward 可以根据想要的压缩长度去调整，这个函数是计算输出长度与20间的差距的负值，reward最好的情况就是当输出长度为20，reward为0。这个reward是(-inf, 0]，没有下限。\n脚本如下:\nexport WANDB_MODE=offline  # Disable Weights &amp; Biases loggingexport CUDA_VISIBLE_DEVICES=6  # Set the GPUs to useexport WANDB_PROJECT=trl-grpo-length-reward  # Set the Weights &amp; Biases project nameexport NCCL_P2P_DISABLE=1export NCCL_IB_DISABLE=1python grpo.py \n\n我是单卡4090训练的，4-5行是4090不支持某些行为而额外设置的，WANDB_MODE根据服务器是否能联网设置。\n我实际调正了压缩长度的效果会很差，接下来看实验。\n实验target length=20\n\n从图中可以看出来completion平均长度是在20附近的，大概多一点。\ntarget length=40\n\n平均长度最后在10左右，解释是这个 completions 只指的 tokenized 的长度，而不是reward中untokenized的长度。\n此外这个数据集的提示词也有点问题\n这是某次生成结果，整体上有太多杂乱的内容。所以不改了。\nOk, so I appreciate you Persona has everyone on the planet! I appreciate you trying to empathise and understand this situation, but it's just too damn frustrating!! Please help me work through this??LINK: https://subreddits.stackoverflow.com/subredirects  0  1—Hey M31 with ninfolake. At 28, mate, the recent crisis can no longer be withstood.  0  1—^_^^Sometime away.If you're committed to turning in your original_username and the arguably useful @M31withNinfeyes, please do so! Thanks!PS: Let me know if there's a way for me to communicate my frustration through redirect or subs sub.Edit: As requested! Thanks for spreading the love. This is the second part of a series where some people on the AoPS just took on some of my issues and mediation with them. This one is a mix tutorial redux and what might happen when you split up to group a team in Minecraft vs Minecraft. I'm happy to learn more about the Minecraft dungeon wording.  0  1—^_^^What your using them struggle on: the minecraft\n\n","categories":["强化学习"]}]
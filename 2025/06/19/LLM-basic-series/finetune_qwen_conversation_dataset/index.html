<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Peng Xia">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2025/06/19/llm-basic-series/finetune_qwen_conversation_dataset/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="多轮对话数据微调 qwen">
<meta property="og:url" content="http://example.com/2025/06/19/LLM-basic-series/finetune_qwen_conversation_dataset/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/redefine-og.webp">
<meta property="article:published_time" content="2025-06-19T14:42:53.788Z">
<meta property="article:modified_time" content="2025-06-19T14:45:13.713Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/redefine-og.webp">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/github-color-svgrepo-com.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/github-color-svgrepo-com.svg">
    <meta name="theme-color" content="#FFD700">
    <link rel="shortcut icon" href="/images/github-color-svgrepo-com.svg">
    <!--- Page Info-->
    
    <title>
        
            多轮对话数据微调 qwen | Sharpen&#39;s Blogs
        
    </title>

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/tailwind.css">

    

    
<link rel="stylesheet" href="/fonts/GeistMono/geist-mono.css">

    
<link rel="stylesheet" href="/fonts/Geist/geist.css">

    <!--- Font Part-->
    
        <link href="https://fonts.googleapis.com/css2?family=Lora" rel="stylesheet">
    
    
    
    
    
    

    <script id="hexo-configurations">
    window.config = {"hostname":"example.com","root":"/","language":"en","path":"search.json"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":false,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.4rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":false,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":{"enable":false,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#FFD700","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":false,"custom_message":null},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/dune.jpg","dark":"/images/dune.jpg"},"title":"Sharpen's Blogs","subtitle":{"text":["Just regularly appending some blogs here, to keep my memory fresh and mind straight.","Here I am. ","Do not go gentle into that good night. "],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":true,"family":"Lora","url":"https://fonts.googleapis.com/css2?family=Lora"},"social_links":{"enable":false,"style":"default","links":{"github":"https://github.com/shar-pen","instagram":null,"zhihu":null,"twitter":null,"email":"xiapeng21011@mail.ustc.edu.cn"},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.2","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Categories":{"path":"/categories","icon":"fa-solid fa-folder"},"About":{"path":"/about","icon":"fa-regular fa-user"},"Links":{"icon":"fa-regular fa-link","submenus":{"Github":"https://github.com/shar-pen","Blog":"https://github.com/shar-pen.github.io","CSDN":"https://blog.csdn.net/the_3rd_bomb"}}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":":)","show_on_mobile":true,"links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"}}},"article_date_format":"YYYY-MM-DD","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":null};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>



<body>
	<div class="progress-bar-container">
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
                <a class="logo-image h-8 w-8 sm:w-10 sm:h-10 mr-3" href="/">
                    <img src="/images/github-color-svgrepo-com.svg" class="w-full h-full rounded-sm">
                </a>
            
            <a class="logo-title" href="/">
                
                Sharpen&#39;s Blogs
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    ARCHIVES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/categories"
                                        >
                                    <i class="fa-solid fa-folder fa-fw"></i>
                                    CATEGORIES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/about"
                                        >
                                    <i class="fa-regular fa-user fa-fw"></i>
                                    ABOUT
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown"
                                   href="#"
                                        onClick=&#34;return false;&#34;>
                                    <i class="fa-regular fa-link fa-fw"></i>
                                    LINKS
                                    <i class="fa-solid fa-chevron-down fa-fw"></i>
                                </a>

                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://github.com/shar-pen">
                                                    GITHUB
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://github.com/shar-pen.github.io">
                                                    BLOG
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://blog.csdn.net/the_3rd_bomb">
                                                    CSDN
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i>
                    </div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                ARCHIVES
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/categories"
                        >
                            <span>
                                CATEGORIES
                            </span>
                            
                                <i class="fa-solid fa-folder fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/about"
                        >
                            <span>
                                ABOUT
                            </span>
                            
                                <i class="fa-regular fa-user fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full">
                        
                        <div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                             navbar-data-toggle="submenu-Links"
                        >
                            <span>
                                LINKS
                            </span>
                            
                                <i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i>
                            
                        </div>
                        

                        
                            <div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-Links">
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://github.com/shar-pen">GITHUB</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://github.com/shar-pen.github.io">BLOG</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://blog.csdn.net/the_3rd_bomb">CSDN</a>
                                    </div>
                                
                            </div>
                        
                    </li>
            

            
            
                
                    
                    
                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full active"
                           href="/tags"
                        >
                            <span>Tags</span>
                            <i class="fa-regular fa-tags fa-sm fa-fw"></i>
                        </a>
                    </li>
                
                    
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">9</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">10</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">40</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			<div class="w-full flex items-center pt-6 justify-start">
				<h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">多轮对话数据微调 qwen</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/avatar.jpg">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">Peng Xia</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2025-06-19 22:42:53</span>
        <span class="mobile">2025-06-19 22:42:53</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2025-06-19 22:45:13</span>
            <span class="mobile">2025-06-19 22:45:13</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/LLM-%E5%9F%BA%E7%A1%80/">LLM 基础</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>3.1k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>16 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<p>多轮数据训练可以让模型学会在连续对话中理解上下文、保持对话连贯性和角色一致性。相比直接生成（单轮问答）, 多轮训练能让模型更好地处理复杂对话场景, 实现更自然的人机交互。直接生成只关注单次提问和回答, 无法捕捉对话历史信息。</p>
<p>以下以 qwen2.5-3b-instruct 在一个心理辅导数据集上以 messages 格式训练。</p>
<h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line">huggingface-cli download Qwen/Qwen2.5-3B-Instruct --local-dir DC/qwen2.5-3B-ins --resume-download</span><br></pre></td></tr></table></figure></div>


<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, json, torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">List</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1"</span></span><br></pre></td></tr></table></figure></div>


<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model_name_or_path = <span class="string">"DC/qwen2.5-3B-ins"</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">	model_name_or_path, </span><br><span class="line">	device_map=<span class="string">"auto"</span>, </span><br><span class="line">    torch_dtype=<span class="string">"auto"</span>,</span><br><span class="line">	trust_remote_code=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(model.dtype)</span><br></pre></td></tr></table></figure></div>

<p>最好指定下 torch_dtype=”auto”, 不然模型会以 fp32 精度加载。</p>
<h2 id="推理测试-直接生成和-chat-生成"><a href="#推理测试-直接生成和-chat-生成" class="headerlink" title="推理测试 - 直接生成和 chat 生成"></a>推理测试 - 直接生成和 chat 生成</h2><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接生成</span></span><br><span class="line"><span class="built_in">input</span> = <span class="string">"What is the capital of France?"</span></span><br><span class="line">inputs = tokenizer(<span class="built_in">input</span>, return_tensors=<span class="string">"pt"</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_new_tokens=<span class="number">50</span>, do_sample=<span class="literal">True</span>, temperature=<span class="number">0.7</span>, top_p=<span class="number">0.9</span>)</span><br><span class="line">output_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(output_text)</span><br></pre></td></tr></table></figure></div>


<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># chat 生成</span></span><br><span class="line">messages = [</span><br><span class="line">	{</span><br><span class="line">		<span class="string">"role"</span>: <span class="string">"system"</span>,</span><br><span class="line">		<span class="string">"content"</span>: <span class="string">"You are a helpful assistant."</span></span><br><span class="line">	},</span><br><span class="line">	{</span><br><span class="line">		<span class="string">"role"</span>: <span class="string">"user"</span>,</span><br><span class="line">		<span class="string">"content"</span>: <span class="string">"What is the capital of France?"</span></span><br><span class="line">	}</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 使用 tokenizer.apply_chat_templat e来处理对话消息 messages, 只对有 chat template 的模型有效</span></span><br><span class="line"><span class="comment"># 一般会将 messages 内容加上角色的标识, add_generation_prompt 是指加上属于模型的标识, 这个其实不需要模型生成, 直接加了会起到提示大模型该轮到你说了</span></span><br><span class="line">inputs = tokenizer.apply_chat_template(messages, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">inputs = tokenizer(inputs, return_tensors=<span class="string">"pt"</span>).to(model.device)</span><br><span class="line">outputs = model.generate(**inputs, max_new_tokens=<span class="number">100</span>, do_sample=<span class="literal">True</span>, temperature=<span class="number">0.7</span>, top_p=<span class="number">0.9</span>)</span><br><span class="line">output_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(output_text)</span><br></pre></td></tr></table></figure></div>



<p>直接生成:</p>
<p>What is the capital of France? The capital of France is Paris. Paris is a beautiful city known for its art, fashion, food, and culture. It is also home to many famous landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum</p>
<hr>
<p>chat 格式生成:</p>
<div class="code-container" data-rel="Markdown"><figure class="iseeu highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im<span class="emphasis">_start|&gt;system</span></span><br><span class="line"><span class="emphasis">You are a helpful assistant.&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;user</span></span><br><span class="line"><span class="emphasis">What is the capital of France?&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;assistant</span></span><br><span class="line"><span class="emphasis">The capital of France is Paris.&lt;|im_</span>end|&gt;</span><br></pre></td></tr></table></figure></div>

<p>区别在于特殊的角色标识符号, 可以更显式的区别人与模型的内容, 可以人与模型的内容交叉实现对话的形式。</p>
<h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">ds = load_dataset(<span class="string">"Amod/mental_health_counseling_conversations"</span>)</span><br><span class="line"><span class="built_in">print</span>(ds)</span><br><span class="line"><span class="built_in">print</span>(json.dumps(ds[<span class="string">"train"</span>][<span class="number">0</span>], indent=<span class="number">2</span>, ensure_ascii=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure></div>

<div class="code-container" data-rel="Markdown"><figure class="iseeu highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict({</span><br><span class="line"><span class="code">	train: Dataset({</span></span><br><span class="line"><span class="code">		features: ['Context', 'Response'],</span></span><br><span class="line"><span class="code">		num_rows: 3512</span></span><br><span class="line"><span class="code">	})</span></span><br><span class="line"><span class="code">})</span></span><br><span class="line"><span class="code">{</span></span><br><span class="line"><span class="code">	"Context": "I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\n   How can I change my feeling of being worthless to everyone?",</span></span><br><span class="line"><span class="code">	"Response": "If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media.  Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living.  They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible.   Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today."</span></span><br><span class="line"><span class="code">}</span></span><br></pre></td></tr></table></figure></div>


<h3 id="将数据格式化为-openai-格式的-messages"><a href="#将数据格式化为-openai-格式的-messages" class="headerlink" title="将数据格式化为 openai 格式的 messages"></a>将数据格式化为 openai 格式的 messages</h3><p>将原始的心理健康咨询对话数据集（<code>ds</code>）中的每条数据转换为对话格式（<code>message_format_ds</code>）, 每条数据包含一组 user-assistant 消息, 便于后续用于对话模型的训练或推理。</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_into_chat_format</span>(<span class="params">example</span>):</span><br><span class="line">	messages = []</span><br><span class="line">	messages.append({</span><br><span class="line">		<span class="string">"role"</span>: <span class="string">"system"</span>, </span><br><span class="line">		<span class="string">"content"</span>: <span class="string">"You are a mental health counselor to help those who are suffering from a number of disorders including anxiety or depression.."</span></span><br><span class="line">	})</span><br><span class="line">	messages.append({</span><br><span class="line">		<span class="string">"role"</span>: <span class="string">"user"</span>, </span><br><span class="line">		<span class="string">"content"</span>: example[<span class="string">"Context"</span>].replace(<span class="string">"\xa0"</span>, <span class="string">" "</span>)</span><br><span class="line">	})</span><br><span class="line">	messages.append({</span><br><span class="line">		<span class="string">"role"</span>: <span class="string">"assistant"</span>, </span><br><span class="line">		<span class="string">"content"</span>: example[<span class="string">"Response"</span>].replace(<span class="string">"\xa0"</span>, <span class="string">" "</span>)</span><br><span class="line">	})</span><br><span class="line">	<span class="keyword">return</span> {<span class="string">"conversations"</span>: messages}</span><br><span class="line"></span><br><span class="line">message_format_ds = ds.<span class="built_in">map</span>(</span><br><span class="line">	process_into_chat_format, </span><br><span class="line">	remove_columns=[<span class="string">"Context"</span>, <span class="string">"Response"</span>]</span><br><span class="line">)</span><br><span class="line">message_format_ds = message_format_ds[<span class="string">'train'</span>].train_test_split(test_size=<span class="number">0.1</span>, seed=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(message_format_ds)</span><br><span class="line"><span class="built_in">print</span>(json.dumps(message_format_ds[<span class="string">'train'</span>][<span class="number">0</span>], indent=<span class="number">2</span>, ensure_ascii=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure></div>


<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict({</span><br><span class="line">	train: Dataset({</span><br><span class="line">		features: ['conversations'],</span><br><span class="line">		num_rows: 3160</span><br><span class="line">	})</span><br><span class="line">	test: Dataset({</span><br><span class="line">		features: ['conversations'],</span><br><span class="line">		num_rows: 352</span><br><span class="line">	})</span><br><span class="line">})</span><br><span class="line">{</span><br><span class="line">	"conversations": [</span><br><span class="line">	{</span><br><span class="line">		"content": "You are a mental health counselor to help those who are suffering from a number of disorders including anxiety or depression..",</span><br><span class="line">		"role": "system"</span><br><span class="line">	},</span><br><span class="line">	{</span><br><span class="line">		"content": "I just took a job that requires me to travel far away from home. My family and I really need this job.\n   People keep telling me I have \"anxiety\" and I'm terrified of having an anxiety attack on the road. This is all new to me. What can I do?",</span><br><span class="line">		"role": "user"</span><br><span class="line">	},</span><br><span class="line">	{</span><br><span class="line">		"content": "It is ok to have anxiety.   Please don't be anxious about being anxious.If you feel anxiety coming over you, then pull off the road to a safe place.   Concentrate on centering yourself and to breath slowly.   Take some sips of water.  Sit still.     The anxiety should pass in about twenty minutes.If it does not pass, then continue calming yourself until you feel safe enough to drive to your hotel.     You can always explain to your supervisor that you were taking care of a medical problem, because anxiety is a medical problem.",</span><br><span class="line">		"role": "assistant"</span><br><span class="line">	}</span><br><span class="line">	]</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="tokenize-对话"><a href="#tokenize-对话" class="headerlink" title="tokenize 对话"></a>tokenize 对话</h3><p><code>tokenizer.apply_chat_template</code> 用在推理时时方便的, 但在转换训练数据时需要对不同角色的conntent和特殊符号分别处理, 以下的函数是基于 qwen template 设计的。</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_openai_messages_qwen_format</span>(<span class="params"></span></span><br><span class="line"><span class="params">	messages: <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">str</span>]],</span></span><br><span class="line"><span class="params">	tokenizer: AutoTokenizer,</span></span><br><span class="line"><span class="params">	max_length: <span class="built_in">int</span> = <span class="number">2048</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">	将对话数据集转换为适用于 Qwen 格式的输入特征, 包括 input_ids、labels 和 attention_mask, 便于后续微调模型。并提供解码函数, 方便检查预处理结果的正确性。</span></span><br><span class="line"><span class="string">	和非 chat 数据的区别在于需要注意 chat 格式和 label 仅为 assistant 内容。</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">	input_ids = []</span><br><span class="line">	labels = []</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> msg <span class="keyword">in</span> messages:</span><br><span class="line">		role = msg[<span class="string">"role"</span>]</span><br><span class="line">		content = msg[<span class="string">"content"</span>]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 1. &lt;|im_start|&gt;{role}\n → 不训练</span></span><br><span class="line">		prefix = <span class="string">f"&lt;|im_start|&gt;<span class="subst">{role}</span>\n"</span></span><br><span class="line">		prefix_ids = tokenizer(prefix, add_special_tokens=<span class="literal">False</span>)[<span class="string">"input_ids"</span>]</span><br><span class="line">		input_ids.extend(prefix_ids)</span><br><span class="line">		labels.extend([-<span class="number">100</span>] * <span class="built_in">len</span>(prefix_ids))</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 2. content → assistant 才训练</span></span><br><span class="line">		content_ids = tokenizer(content, add_special_tokens=<span class="literal">False</span>)[<span class="string">"input_ids"</span>]</span><br><span class="line">		input_ids.extend(content_ids)</span><br><span class="line">		<span class="keyword">if</span> role == <span class="string">"assistant"</span>:</span><br><span class="line">			labels.extend(content_ids)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			labels.extend([-<span class="number">100</span>] * <span class="built_in">len</span>(content_ids))</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 3. &lt;|im_end|&gt; → 仅 assistant 时参与训练</span></span><br><span class="line">		suffix = <span class="string">"&lt;|im_end|&gt;"</span></span><br><span class="line">		suffix_ids = tokenizer(suffix, add_special_tokens=<span class="literal">False</span>)[<span class="string">"input_ids"</span>]</span><br><span class="line">		input_ids.extend(suffix_ids)</span><br><span class="line">		<span class="keyword">if</span> role == <span class="string">"assistant"</span>:</span><br><span class="line">			labels.extend(suffix_ids)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			labels.extend([-<span class="number">100</span>] * <span class="built_in">len</span>(suffix_ids))</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 4. 添加换行符</span></span><br><span class="line">		input_ids.extend(tokenizer(<span class="string">'\n'</span>, add_special_tokens=<span class="literal">False</span>)[<span class="string">"input_ids"</span>])</span><br><span class="line">		labels.append(-<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">assert</span> <span class="built_in">len</span>(input_ids) == <span class="built_in">len</span>(labels), <span class="string">"Input IDs and labels must have the same length."</span></span><br><span class="line">	<span class="comment"># 截断</span></span><br><span class="line">	input_ids = input_ids[:max_length]</span><br><span class="line">	labels = labels[:max_length]</span><br><span class="line">	attention_mask = [<span class="number">1</span>] * <span class="built_in">len</span>(input_ids)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> {</span><br><span class="line">		<span class="string">"input_ids"</span>: input_ids,</span><br><span class="line">		<span class="string">"labels"</span>: labels,</span><br><span class="line">		<span class="string">"attention_mask"</span>: attention_mask</span><br><span class="line">	}</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decode_labels</span>(<span class="params">labels: <span class="type">List</span>[<span class="built_in">int</span>], tokenizer: AutoTokenizer</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">	<span class="comment"># 将 labels 中连续的非 -100 段分别 decode, 并用特殊分隔符拼接</span></span><br><span class="line">	segments = []</span><br><span class="line">	current = []</span><br><span class="line">	<span class="keyword">for</span> t <span class="keyword">in</span> labels:</span><br><span class="line">		<span class="keyword">if</span> t != -<span class="number">100</span>:</span><br><span class="line">			current.append(t)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			<span class="keyword">if</span> current:</span><br><span class="line">				segments.append(tokenizer.decode(current, skip_special_tokens=<span class="literal">False</span>))</span><br><span class="line">				current = []</span><br><span class="line">	<span class="keyword">if</span> current:</span><br><span class="line">		segments.append(tokenizer.decode(current, skip_special_tokens=<span class="literal">False</span>))</span><br><span class="line">	<span class="keyword">return</span> segments</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">	{<span class="string">"role"</span>: <span class="string">"system"</span>, <span class="string">"content"</span>: <span class="string">"You are a helpful assistant."</span>},</span><br><span class="line">	{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: <span class="string">"What is the capital of France?"</span>},</span><br><span class="line">	{<span class="string">"role"</span>: <span class="string">"assistant"</span>, <span class="string">"content"</span>: <span class="string">"The capital of France is Paris."</span>},</span><br><span class="line">	{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: <span class="string">"What is the capital of Germany?"</span>},</span><br><span class="line">	{<span class="string">"role"</span>: <span class="string">"assistant"</span>, <span class="string">"content"</span>: <span class="string">"The capital of Germany is Berlin."</span>}</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">sample = preprocess_openai_messages_qwen_format(messages, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokenizer.apply_chat_template(messages, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印 input_ids 解码后内容</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Decoded input:\n{}"</span>.<span class="built_in">format</span>(tokenizer.decode(sample[<span class="string">"input_ids"</span>], skip_special_tokens=<span class="literal">False</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印 labels 解码后内容（只显示参与训练的内容）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Decoded labels:\n{}"</span>.<span class="built_in">format</span>(decode_labels(sample[<span class="string">"labels"</span>], tokenizer)))</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<p>tokenizer.apply_chat_template:</p>
<div class="code-container" data-rel="Markdown"><figure class="iseeu highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im<span class="emphasis">_start|&gt;system</span></span><br><span class="line"><span class="emphasis">You are a helpful assistant.&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;user</span></span><br><span class="line"><span class="emphasis">What is the capital of France?&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;assistant</span></span><br><span class="line"><span class="emphasis">The capital of France is Paris.&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;user</span></span><br><span class="line"><span class="emphasis">What is the capital of Germany?&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;assistant</span></span><br><span class="line"><span class="emphasis">The capital of Germany is Berlin.&lt;|im_</span>end|&gt;</span><br></pre></td></tr></table></figure></div>

<p>Decoded input:</p>
<div class="code-container" data-rel="Markdown"><figure class="iseeu highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Decoded input:</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;system</span></span><br><span class="line"><span class="emphasis">You are a helpful assistant.&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;user</span></span><br><span class="line"><span class="emphasis">What is the capital of France?&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;assistant</span></span><br><span class="line"><span class="emphasis">The capital of France is Paris.&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;user</span></span><br><span class="line"><span class="emphasis">What is the capital of Germany?&lt;|im_</span>end|&gt;</span><br><span class="line">&lt;|im<span class="emphasis">_start|&gt;assistant</span></span><br><span class="line"><span class="emphasis">The capital of Germany is Berlin.&lt;|im_</span>end|&gt;</span><br><span class="line"></span><br><span class="line">Decoded labels:</span><br><span class="line">['The capital of France is Paris.&lt;|im<span class="emphasis">_end|&gt;', 'The capital of Germany is Berlin.&lt;|im_</span>end|&gt;']</span><br></pre></td></tr></table></figure></div>
<p>Decoded labels:<br>[‘The capital of France is Paris.&lt;|im_end|&gt;’, ‘The capital of Germany is Berlin.&lt;|im_end|&gt;’]</p>
<p>labels中非-100的只有</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">wrapped_preprocess</span>(<span class="params">example, tokenizer, max_length=<span class="number">2048</span></span>):</span><br><span class="line">	<span class="comment"># batched=True: example["conversations"] is a list of conversations</span></span><br><span class="line">	conversations_list = example[<span class="string">"conversations"</span>]</span><br><span class="line">	results = preprocess_openai_messages_qwen_format(example[<span class="string">"conversations"</span>], tokenizer, max_length)</span><br><span class="line">	<span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">input_ds = message_format_ds.<span class="built_in">map</span>(</span><br><span class="line">	wrapped_preprocess,</span><br><span class="line">	remove_columns=[<span class="string">"conversations"</span>],</span><br><span class="line">	desc=<span class="string">"Processing training dataset"</span>,</span><br><span class="line">	fn_kwargs={<span class="string">"tokenizer"</span>: tokenizer, <span class="string">"max_length"</span>: <span class="number">2048</span>} <span class="comment"># max_length=8192 时会OOM, 原因时有两个数据太长了, 一般数据都在1k以下</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>

<p>这个 cell 是可选的, 静态 bucketing, 在 batch 内排序以减小 padding, 下面的结果都是没有执行静态 bucketing 的结果。</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_ds = input_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: {<span class="string">"length"</span>: <span class="built_in">len</span>(x[<span class="string">"input_ids"</span>])}, desc=<span class="string">"Calculating input length"</span>)</span><br><span class="line">input_ds = input_ds.sort(<span class="string">"length"</span>)  <span class="comment"># 排序！</span></span><br></pre></td></tr></table></figure></div>


<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForSeq2Seq</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorForSeq2Seq(</span><br><span class="line">	tokenizer=tokenizer,</span><br><span class="line">	return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">	padding=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">samples = [input_ds[<span class="string">'train'</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)]</span><br><span class="line">batch = data_collator(samples)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> batch.items():</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f"<span class="subst">{key}</span>: <span class="subst">{value.shape}</span>"</span>)</span><br></pre></td></tr></table></figure></div>

<p>input_ids: torch.Size([3, 516])<br>attention_mask: torch.Size([3, 516])<br>labels: torch.Size([3, 516])</p>
<h3 id="检查-batch-长度"><a href="#检查-batch-长度" class="headerlink" title="检查 batch 长度"></a>检查 batch 长度</h3><p>防止出现过长的数据, 导致突然 OOM</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_batch_lengths</span>(<span class="params">dataset, data_collator, batch_size=<span class="number">1</span>, title=<span class="string">"Batch Token Lengths"</span></span>):</span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        collate_fn=data_collator</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    batch_lengths = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(dataloader, desc=<span class="string">"Analyzing batches"</span>):</span><br><span class="line">        input_ids = batch[<span class="string">"input_ids"</span>]</span><br><span class="line">        <span class="comment"># 如果是多条样本拼成的 batch, 取最长的那条（最大长度）</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(input_ids, torch.Tensor):</span><br><span class="line">            length = input_ids.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 防止出现 List[List[int]]</span></span><br><span class="line">            length = <span class="built_in">max</span>(<span class="built_in">len</span>(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> input_ids)</span><br><span class="line">        batch_lengths.append(length)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line">    plt.plot(batch_lengths, marker=<span class="string">'o'</span>, markersize=<span class="number">2</span>, linewidth=<span class="number">0.8</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Batch Index (Step)"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Token Length"</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> batch_lengths</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用方法</span></span><br><span class="line">batch_lengths = plot_batch_lengths(</span><br><span class="line">    dataset=input_ds[<span class="string">'train'</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    batch_size=<span class="number">1</span>,</span><br><span class="line">    title=<span class="string">"Token Length per Batch in Training Dataset"</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>



<p><img lazyload="" src="/images/loading.svg" data-src="/images/finetune_qwen_conversation_dataset/max_length_8192.png"></p>
<p>之前用 max_length=8192 还是过于看得起 4090 了, 有几个数据很长, 大约在 56 step 时就会遇到超长数据, 会突然 OOM。因此最终改 max_length=1024。</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/finetune_qwen_conversation_dataset/max_length_1024.png"></p>
<h2 id="peft"><a href="#peft" class="headerlink" title="peft"></a>peft</h2><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> (</span><br><span class="line">    LoraConfig,</span><br><span class="line">	TaskType,</span><br><span class="line">    get_peft_model,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">	task_type=TaskType.CAUSAL_LM, </span><br><span class="line">	target_modules=[<span class="string">'q_proj'</span>, <span class="string">'v_proj'</span>], </span><br><span class="line">	r=<span class="number">16</span>, </span><br><span class="line">	lora_alpha=<span class="number">16</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">peft_model = get_peft_model(model, lora_config)</span><br><span class="line">peft_model.print_trainable_parameters()</span><br></pre></td></tr></table></figure></div>

<p>trainable params: 3,686,400 || all params: 3,089,625,088 || trainable%: 0.1193</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">	output_dir=<span class="string">"./lora-conversation-2"</span>,</span><br><span class="line">	per_device_train_batch_size=<span class="number">1</span>,</span><br><span class="line">	gradient_accumulation_steps=<span class="number">32</span>,</span><br><span class="line">	per_device_eval_batch_size=<span class="number">4</span>,</span><br><span class="line">	num_train_epochs=<span class="number">2</span>,</span><br><span class="line">	learning_rate=<span class="number">2e-4</span>,</span><br><span class="line">	weight_decay=<span class="number">0.01</span>,</span><br><span class="line">	logging_steps=<span class="number">10</span>,</span><br><span class="line">	save_steps=<span class="number">100</span>,</span><br><span class="line">	eval_strategy=<span class="string">"steps"</span>,</span><br><span class="line">	eval_steps=<span class="number">10</span>,</span><br><span class="line">	save_total_limit=<span class="number">1</span>,</span><br><span class="line">	load_best_model_at_end=<span class="literal">False</span>,</span><br><span class="line">	report_to=<span class="string">'none'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">	model=peft_model,</span><br><span class="line">	args=training_args,</span><br><span class="line">	train_dataset=input_ds[<span class="string">'train'</span>],</span><br><span class="line">	eval_dataset=input_ds[<span class="string">'test'</span>],</span><br><span class="line">	data_collator=data_collator,</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>


<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure></div>



<p>{‘eval_loss’: 2.4432363510131836,<br>‘eval_runtime’: 9.3492,<br>‘eval_samples_per_second’: 37.65,<br>‘eval_steps_per_second’: 9.413,<br>‘epoch’: 1.9822784810126581}</p>
<h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_name_or_path = <span class="string">"DC/qwen2.5-3B-ins"</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">	model_name_or_path, </span><br><span class="line">	device_map=<span class="string">"auto"</span>, </span><br><span class="line">    torch_dtype=<span class="string">"auto"</span>,</span><br><span class="line">	trust_remote_code=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>



<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">example = message_format_ds[<span class="string">'test'</span>][<span class="number">0</span>][<span class="string">'conversations'</span>]</span><br><span class="line">example_i = example[:-<span class="number">1</span>]  <span class="comment"># 去掉最后一条 assistant</span></span><br><span class="line">example_o = example[-<span class="number">1</span>]  <span class="comment"># 最后一条是 assistant 的回复</span></span><br><span class="line">inputs = tokenizer.apply_chat_template(example_i, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">inputs = tokenizer(inputs, return_tensors=<span class="string">"pt"</span>).to(model.device)</span><br></pre></td></tr></table></figure></div>


<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">outputs = model.generate(**inputs, max_new_tokens=<span class="number">500</span>, do_sample=<span class="literal">True</span>, temperature=<span class="number">0.7</span>, top_p=<span class="number">0.9</span>)</span><br><span class="line">output_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Output:\n"</span>, output_text)</span><br></pre></td></tr></table></figure></div>


<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel, PeftConfig</span><br><span class="line">peft_model_id = <span class="string">"lora-conversation-2/checkpoint-196"</span>  <span class="comment"># LoRA adapter 保存路径</span></span><br><span class="line">config = PeftConfig.from_pretrained(peft_model_id)</span><br><span class="line"></span><br><span class="line">model_name_or_path = <span class="string">"DC/qwen2.5-3B-ins"</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">base_model  = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">	model_name_or_path, </span><br><span class="line">	device_map=<span class="string">"auto"</span>, </span><br><span class="line">    torch_dtype=<span class="string">"auto"</span>,</span><br><span class="line">	trust_remote_code=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">model = PeftModel.from_pretrained(base_model, peft_model_id)</span><br><span class="line"></span><br><span class="line">outputs = model.generate(**inputs, max_new_tokens=<span class="number">500</span>, do_sample=<span class="literal">True</span>, temperature=<span class="number">0.7</span>, top_p=<span class="number">0.9</span>)</span><br><span class="line">output_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Output:\n"</span>, output_text)</span><br></pre></td></tr></table></figure></div>


<h4 id="原输出"><a href="#原输出" class="headerlink" title="原输出"></a>原输出</h4><p>Instead of fighting about trust, is it possible for you and your wife to talk with other about areas which upset each of you?Whenever feelings are hurt, knowing what exactly is problematic and being heard and understood by the partner, goes a long way to building trust.These type of discussions are hard to have, and especially for the first time.  A lot of emotions arise and often people lose their conversation focus from this.If you and your wife have a tough time opening up to each other, consider scheduling time with a couples therapist.Just by each of you committing time to invest in the relationship will show faith in the relationship, and this may contribute to restoring trust.</p>
<hr>
<h4 id="微调前"><a href="#微调前" class="headerlink" title="微调前"></a>微调前</h4><p>It sounds like you’re experiencing significant distress related to feelings of betrayal and loss of trust, which can be deeply painful and challenging to navigate. It’s important to address these emotions and the situation with care and consideration for both your own emotional well-being and that of your wife.</p>
<p>Here are some steps you might consider:</p>
<ol>
<li><p><strong>Communication</strong>: Have an open, honest conversation with your wife about how you feel. It’s crucial to express your concerns without placing blame. Use “I” statements to avoid making her defensive. For example, say something like, “I’ve been feeling really hurt by the recent changes in our relationship dynamics.”</p>
</li>
<li><p><strong>Listening</strong>: Allow your wife to express her side of the story. Sometimes, people act out of fear or insecurity. Listening can provide you with insights into why she may have acted as she did and show her that you respect her perspective.</p>
</li>
<li><p><strong>Seek Support</strong>: Consider speaking with a therapist or counselor who can help facilitate communication between you and your wife. They can offer strategies for rebuilding trust and understanding.</p>
</li>
<li><p><strong>Establish Boundaries</strong>: Set clear boundaries regarding privacy and communication with friends and romantic partners. This can help prevent similar situations from arising in the future.</p>
</li>
<li><p><strong>Self-Care</strong>: Engage in activities that promote your mental and emotional health. This could include exercise, meditation, or hobbies that bring you joy and relaxation.</p>
</li>
<li><p><strong>Professional Help</strong>: If the situation feels overwhelming, professional guidance can be invaluable. A psychologist or psychiatrist can provide tools and techniques to help manage your anxiety and depression, which are common responses to betrayal and loss of trust.</p>
</li>
<li><p><strong>Time</strong>: Give yourself and your wife time to heal. Healing takes time, and it’s essential not to rush this process.</p>
</li>
</ol>
<p>Remember, the goal is to strengthen your relationship, not just to survive the current crisis. Trust can be rebuilt over time with patience, honesty, and commitment from both of you.</p>
<p>How does this resonate with you, and what specific areas do you need more assistance with?&lt;|im_end|&gt;</p>
<hr>
<h4 id="微调后"><a href="#微调后" class="headerlink" title="微调后"></a>微调后</h4><p>It sounds like you are in the middle of a “trust gap” between your spouse and yourself.  You both are in different places emotionally regarding the issue of trust.  It is a good idea for you to start by talking with your wife about what you have experienced and how it has affected you.  She may not be aware of your feelings and concerns.  You may also want to discuss your thoughts and feelings with someone else outside of your marriage, such as a trusted family member, friend, or therapist.  Having an objective listener can help you sort through your feelings and thoughts regarding this situation. &lt;|im_end|&gt;</p>
<p>回答效果不一定是正向提升, 但整体风格更接近数据集。</p>

		</div>

		

		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/2025/05/30/Multi-modal-series/qwen2.5_vl_infer/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item">Qwen 2.5 VL 推理</span>
						<span class="post-nav-item">Next posts</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
		<div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
			<div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        Comments
    </div>
    

        
            
    <div id="waline"></div>
    <script type="module" data-swup-reload-script>
      import { init } from '/js/libs/waline.mjs';

      function loadWaline() {
        init({
          el: '#waline',
          serverURL: 'https://example.example.com',
          lang: 'zh-CN',
          dark: 'body[class~="dark-mode"]',
          reaction: false,
          requiredMeta: ['nick', 'mail'],
          emoji: [],
          
          
        });
      }

      if (typeof swup !== 'undefined') {
        loadWaline();
      } else {
        window.addEventListener('DOMContentLoaded', loadWaline);
      }
    </script>



        
    
</div>

		</div>
		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">On this page</div>
		<div class="page-title">多轮对话数据微调 qwen</div>
		<ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-text">环境搭建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E6%B5%8B%E8%AF%95-%E7%9B%B4%E6%8E%A5%E7%94%9F%E6%88%90%E5%92%8C-chat-%E7%94%9F%E6%88%90"><span class="nav-text">推理测试 - 直接生成和 chat 生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">加载数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%86%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%B8%BA-openai-%E6%A0%BC%E5%BC%8F%E7%9A%84-messages"><span class="nav-text">将数据格式化为 openai 格式的 messages</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tokenize-%E5%AF%B9%E8%AF%9D"><span class="nav-text">tokenize 对话</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5-batch-%E9%95%BF%E5%BA%A6"><span class="nav-text">检查 batch 长度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#peft"><span class="nav-text">peft</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-text">模型训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#test"><span class="nav-text">test</span></a></li></ol>

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
            2025&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Peng Xia</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        40 posts in total
                    </span>
                    
                        <span>
                            130.7k words in total
                        </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.2</a></span>
        </div>
        
        
        
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
		<li class="go-comment">
			<i class="fa-regular fa-comments"></i>
		</li>
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog fa-spin"></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	
	<div class="search-pop-overlay">
	<div class="popup search-popup">
		<div class="search-header">
			<span class="search-input-field-pre">
				<i class="fa-solid fa-keyboard"></i>
			</span>
			<div class="search-input-container">
				<input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="Search..." spellcheck="false" type="search" class="search-input">
			</div>
			<span class="popup-btn-close">
				<i class="fa-solid fa-times"></i>
			</span>
		</div>
		<div id="search-result">
			<div id="no-result">
				<i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
			</div>
		</div>
	</div>
</div>
	

</main>



<script src="/js/build/libs/Swup.min.js"></script>

<script src="/js/build/libs/SwupSlideTheme.min.js"></script>

<script src="/js/build/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/build/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/build/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/build/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	
<script src="/js/build/tools/imageViewer.js" type="module"></script>

<script src="/js/build/utils.js" type="module"></script>

<script src="/js/build/main.js" type="module"></script>

<script src="/js/build/layouts/navbarShrink.js" type="module"></script>

<script src="/js/build/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/build/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/build/layouts/categoryList.js" type="module"></script>



    
<script src="/js/build/tools/localSearch.js" type="module"></script>




    
<script src="/js/build/tools/codeBlock.js" type="module"></script>




    
<script src="/js/build/layouts/lazyload.js" type="module"></script>






  
<script src="/js/build/libs/Typed.min.js"></script>

  
<script src="/js/build/plugins/typed.js" type="module"></script>








    
<script src="/js/build/libs/anime.min.js"></script>





    
<script src="/js/build/tools/tocToggle.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/layouts/toc.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/plugins/tabs.js" type="module" data-swup-reload-script=""></script>




<script src="/js/build/libs/moment-with-locales.min.js" data-swup-reload-script=""></script>


<script src="/js/build/layouts/essays.js" type="module" data-swup-reload-script=""></script>





	
</body>

</html>